{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab921c71",
   "metadata": {},
   "source": [
    "# Hyperparameters for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa54aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebe301",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "dir_p = Path().resolve() \n",
    "\n",
    "\n",
    "from agrid.grid import Grid\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "from scipy import stats, interpolate, spatial, io\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Arc \n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import pyproj as proj\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.metrics import make_scorer , r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbd194",
   "metadata": {},
   "source": [
    "define constants and dcitionaries to easy looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "\n",
    "#parent directory\n",
    "\n",
    "dir_p = Path().resolve() \n",
    "\n",
    "#constants\n",
    "km = 1000\n",
    "milli = 0.001\n",
    "micro = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "# create grid for each region\n",
    "# crs Coordinate reference system\n",
    "\n",
    "#EPSG is projection\n",
    "# 0.2 degrees equal roughly 20 km\n",
    "\n",
    "World = Grid(res=[0.2, 0.2], up=world_lat_max, down=world_lat_min)\n",
    "\n",
    "\n",
    "# africa grid low resolution 50 x 50 km\n",
    "\n",
    "Africa =    Grid(res=[0.5, 0.5],  left = afr_lon_min, right= afr_lon_max, up=afr_lat_max , down=afr_lat_min)\n",
    "\n",
    "\n",
    "#dictionary of all grids\n",
    "\n",
    "grids = {}\n",
    "\n",
    "grids['Afr'] = Africa\n",
    "grids['World'] = World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5077a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated\n"
     ]
    }
   ],
   "source": [
    "# to ease looping with dictionaries\n",
    "\n",
    "regions_a = [ 'Afr' ]\n",
    "\n",
    "\n",
    "regions_Total = ['World' ,'Afr']\n",
    "\n",
    "\n",
    "# raster exenets to adjust map\n",
    "raster_extent_Afr = [grids['Afr'].extent[0], grids['Afr'].extent[1], grids['Afr'].extent[3], grids['Afr'].extent[2]]\n",
    "raster_extent_World = [grids['World'].extent[0], grids['World'].extent[1], grids['World'].extent[3], grids['World'].extent[2]]\n",
    "\n",
    "# to correct plot maps\n",
    "raster_extents = {}\n",
    "\n",
    "raster_extents['Afr'] = raster_extent_Afr\n",
    "raster_extents['World'] = raster_extent_World\n",
    "\n",
    "\n",
    "# list of latitudes and longitudes\n",
    "lon_dict = {}\n",
    "lat_dict = {}\n",
    "\n",
    "lon_dict['Afr'] = [afr_lon_min, afr_lon_max]\n",
    "lon_dict['World'] = [world_lon_min, world_lon_max]\n",
    "\n",
    "lat_dict['Afr'] = [afr_lat_min, afr_lat_max]\n",
    "lat_dict['World'] = [world_lat_min, world_lat_max]\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1569405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        OBS_REF       LABELS        UNITS      V_RANGE    CMAPS\n",
      "0           CTD          CTD           km      (0, 50)   batlow\n",
      "1            SI  Shape index           si      (-1, 1)     broc\n",
      "2           LAB    LAB depth           km     (0, 300)   bamako\n",
      "3          MOHO   Moho depth           km     (15, 60)   batlow\n",
      "4            SV  S$_V$ 150km  $\\delta$...  (-0.075,...     roma\n",
      "5            PV  P$_V$ 150km  $\\delta$...  (-0.02, ...     roma\n",
      "6         GEOID        Geoid            m    (-45, 45)   bamako\n",
      "7            FA     Free air         mGal  (-100, 100)     broc\n",
      "8           DEM          DEM            m  (-2200, ...   bukavu\n",
      "9            BG      Bouguer         mGal  (-250, 100)     broc\n",
      "10  EMAG2_CLASS         Mag.        f(nT)  (-0.4, 0.4)   batlow\n",
      "11        RHO_L      Lith. ρ     kg/m$^3$  (3260, 3...   batlow\n",
      "12        RHO_C      Crust ρ     kg/m$^3$  (2650, 2...   batlow\n",
      "13  VOLC_DIST_W   Volcano d.           km       (0, 1)   bamako\n",
      "14          REG         GliM        class       (1, 6)  batlowS\n",
      "15         GLIM          REG        class      (1, 15)     topo\n"
     ]
    }
   ],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "''' \n",
    "obs['REF_n'] = [ 'MOHO','LAB', 'RHO_C', 'SV', 'PV', 'CTD',\n",
    "             'RHO_L', 'DEM', \n",
    "                'VOLC_DIST_W', 'A_MEDIAN_W', 'FA', 'SI','LITH_MANTLE', \n",
    "                'EMAG2_CLASS', 'GEOID', 'BG',\n",
    "              'GLIM']'''\n",
    "\n",
    "\n",
    "\n",
    "obs['OBS_REF'] = ['CTD' ,  'SI',\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            'GEOID','FA','DEM','BG', 'EMAG2_CLASS',\n",
    "                   'RHO_L', 'RHO_C', \n",
    "                  'VOLC_DIST_W', 'REG', 'GLIM']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "obs['LABELS'] = ['CTD',  'Shape index', 'LAB depth', 'Moho depth', \n",
    "                'S$_V$ 150km', 'P$_V$ 150km', \n",
    "                'Geoid', 'Free air', 'DEM', 'Bouguer', 'Mag.', \n",
    "                'Lith. ρ', 'Crust ρ',  \n",
    "                 'Volcano d.', 'GliM', 'REG', ]  \n",
    "    \n",
    "    \n",
    "# 'vp/vs'\n",
    "# Units to display in plots etc\n",
    "obs['UNITS'] = ['km',  'si', 'km', 'km',\n",
    "             '$\\delta$ v_s %','$\\delta$ v_p %', \n",
    "             'm', 'mGal', 'm', 'mGal',  'f(nT)', \n",
    "                 'kg/m$^3$', 'kg/m$^3$',\n",
    "                'km',  'class', 'class']\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs['V_RANGE'] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-250,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,15),]\n",
    "    \n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"topo\", ]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0,14,10,16,17,9, 2,1,5,13,12, 8,11,]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "#obs = obs.reindex(new_index)\n",
    "\n",
    "obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 12\n",
    "print(obs)\n",
    "\n",
    "n_obs = len(obs)\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe67da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'heat-flow (mW/m2)'\n",
    "coord = ['lon', 'lat']\n",
    "grid_index = ['grid_index']\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "features_ghf = []\n",
    "\n",
    "\n",
    "\n",
    "features = obs['OBS_REF'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "in_features = set(features)\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.extend(grid_index)\n",
    "\n",
    "features_ex.append(target)\n",
    "\n",
    "features_ghf = copy.deepcopy(features)\n",
    "features_ghf.append(target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c063f",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16412772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regressor_label = 'RFR'\n",
    "kfold = 10\n",
    "n_iter = 100\n",
    "\n",
    "\n",
    "\n",
    "file_labels =[ f'NOD_ra',  f'OD_ra', \n",
    "              f'NOD_rab',  f'OD_rab',]\n",
    "\n",
    "\n",
    "# param distribution\n",
    "n_estimators      = np.arange(100, 700,50)\n",
    "max_features = Categorical(['log2', 'sqrt'])\n",
    "#the maximum number of features Random Forest is allowed to try in individual tree\n",
    "\n",
    "bootstrap    = Categorical([True, False]) # deafult is false for extra trees True for randomforest\n",
    "criterion    = Categorical(['squared_error', 'absolute_error' ])\n",
    "\n",
    "# updated param distribution\n",
    "#n_estimators      = Integer(150, 500)#  BEST 100\n",
    "\n",
    "max_depth         =  Integer(5,50)  # best 50\n",
    "min_samples_split =  Integer(2,50) \n",
    "min_samples_leaf  =  Integer(1,30) \n",
    "\n",
    "\n",
    "\n",
    "hyperparameters = {}\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "print('terminated')\n",
    "\n",
    "\n",
    "for file_label in file_labels:\n",
    "    print(file_label)\n",
    "\n",
    "    tarin_f =  dir_p /'data'/'dataset'/'Preprocessed'/f'Training_W_{file_label}.csv'\n",
    "\n",
    "    train_df = pd.read_csv(tarin_f,  sep='\\t')\n",
    "\n",
    "\n",
    "    X = train_df[features]\n",
    "    y = train_df[target] \n",
    "    X['GLIM']  = X['GLIM'].astype('int').astype('category')\n",
    "    X['REG']  = X['REG'].astype('int').astype('category')\n",
    "\n",
    "    \n",
    "    # Update dict with Extra Trees\n",
    "    hyperparameters.update({\"RFR\": { \n",
    "             'regressor__n_estimators': n_estimators,  # # inc up to a point then overfit train reduce test\n",
    "\n",
    "             'regressor__bootstrap': bootstrap,\n",
    "             #'regressor__ccp_alpha':[ 0.0],\n",
    "             'regressor__criterion':criterion,\n",
    "\n",
    "             'regressor__max_depth': max_depth, # inc up to a point then overfit train reduce test\n",
    "\n",
    "             'regressor__max_features': max_features,  # # inc up to a point then overfit train reduce test\n",
    "\n",
    "            # if bootstrap True for RF\n",
    "             #'regressor__max_samples': [None],   # # inc up to a point then overfit train reduce test\n",
    "\n",
    "             #'regressor__max_leaf_nodes': [None],  # total numer of nodes should be dec\n",
    "\n",
    "             #'regressor__min_impurity_decrease': [0.0], \n",
    "\n",
    "             'regressor__min_samples_leaf': min_samples_leaf,  # should be inc to avoid pure nodes overfit \n",
    "\n",
    "             'regressor__min_samples_split': min_samples_split,  # should be inc to avoid pure nodes overfit \n",
    "             #'regressor__min_weight_fraction_leaf':[ 0.0],\n",
    "\n",
    "             #'regressor__oob_score': [True],\n",
    "             #'regressor__n_jobs': [1],\n",
    "             #'regressor__random_state': [random_state],\n",
    "             #'regressor__verbose':[ 0],\n",
    "             #'regressor__warm_start': [False]\n",
    "                             }})\n",
    "\n",
    "     #Create train and test set  \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, shuffle=True)\n",
    "\n",
    "    \n",
    "    # Create list of tuples with regressors label and regressors object\n",
    "    regressors = {}\n",
    "\n",
    "    regressors.update({\"RFR\": RandomForestRegressor()})\n",
    "\n",
    "\n",
    "    # Initialize dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "\n",
    "    cv = KFold(n_splits=kfold, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "    # Tune and evaluate regressors\n",
    "\n",
    "\n",
    "    # Print message to user\n",
    "    print('#'*60)\n",
    "    print(f\"Now tuning {regressor_label}.\")\n",
    "\n",
    "\n",
    "    #scoring = make_scorer(nrmse , greater_is_better=False )\n",
    "    scoring = make_scorer(mean_squared_error, squared=False, greater_is_better=False)\n",
    "    \n",
    "    regressor = regressors[regressor_label]\n",
    "\n",
    "\n",
    "    scaler = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "\n",
    "\n",
    "    numeric_transformer = scaler\n",
    "\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "       (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", regressor)]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline_bscv = Pipeline(steps = steps)\n",
    "\n",
    "\n",
    "    # Define parameter grid\n",
    "    search_space = hyperparameters[regressor_label]\n",
    "\n",
    "    # Initialize BaysSearch object                  \n",
    "    bscv = BayesSearchCV(\n",
    "        pipeline_bscv,\n",
    "        # (parameter space, # of evaluations)\n",
    "        search_space,\n",
    "        n_iter = n_iter, \n",
    "        cv = cv, \n",
    "        verbose = 3, \n",
    "        n_jobs= -1, \n",
    "        scoring= scoring,\n",
    "        return_train_score=True,\n",
    "        #refit=False,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Fit gscv\n",
    "    bscv.fit(X_train, y_train)  \n",
    "\n",
    "    # Get best parameters and score\n",
    "    best_params = bscv.best_params_\n",
    "    best_score = bscv.best_score_\n",
    "\n",
    "\n",
    "    bs_rfr_hyp_o =  dir_p /'Hyperparameters'/f'RFR_{file_label}.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bs_df = pd.DataFrame(best_params,  index=[0])\n",
    "\n",
    "\n",
    "    bs_df.to_csv(bs_rfr_hyp_o , index=False, sep='\\t')\n",
    "\n",
    "\n",
    "    print('Optimization has terminated')\n",
    "\n",
    "\n",
    "    #save results\n",
    "    bs_rfr_hyp =  dir_p/'Hyperparameters'/f'BS_hyper_{file_label}.csv'\n",
    "\n",
    "\n",
    "\n",
    "    dropped_columns = ['mean_fit_time', 'std_fit_time',  'params', 'mean_score_time', 'std_score_time']\n",
    "\n",
    "    # all hyp results\n",
    "    # gives traing and validation results\n",
    "    bs_df = pd.DataFrame(bscv.cv_results_)\n",
    "\n",
    "    bs_df = bs_df.sort_values(f'rank_test_score', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    bs_df['mean_test_score'] = bs_df['mean_test_score'] *-1\n",
    "    bs_df['mean_train_score'] = bs_df['mean_train_score'] *-1\n",
    "\n",
    "    split_test = [f'split{x}_test_score' for x in range(kfold)]\n",
    "    split_train = [f'split{x}_train_score' for x in range(kfold)]\n",
    "\n",
    "    dropped_columns.extend(split_test)\n",
    "    dropped_columns.extend(split_train)\n",
    "\n",
    "    bs_df = bs_df.drop(dropped_columns, axis=1)\n",
    "\n",
    "\n",
    "    bs_df.to_csv(bs_rfr_hyp , sep='\\t')\n",
    "\n",
    "\n",
    "    # evaluation of hyperparatmeters\n",
    "    \n",
    "\n",
    "    # Update dict with Extra Trees\n",
    "    hyperparameters.update({\"RFR\": { \n",
    "        'regressor__bootstrap': bs_df.loc[0:7, 'param_regressor__bootstrap'].mode()[0],\n",
    "        'regressor__criterion':bs_df.loc[0:7, 'param_regressor__criterion'].mode()[0],\n",
    "        'regressor__n_estimators': bs_df.loc[0:7, 'param_regressor__n_estimators'].min(), # should be dec to avoid pure nodes overfit \n",
    "        'regressor__max_features': bs_df.loc[0:7, 'param_regressor__max_features'].mode()[0],\n",
    "        'regressor__max_depth': bs_df.loc[0:7, 'param_regressor__max_depth'].min(), # should be dec to avoid pure nodes overfit \n",
    "        'regressor__min_samples_leaf': bs_df.loc[0:7, 'param_regressor__min_samples_leaf'].max(), # should be inc to avoid pure nodes overfit \n",
    "        'regressor__min_samples_split': bs_df.loc[0:7, 'param_regressor__min_samples_split'].max(), # should be inc to avoid pure nodes overfit \n",
    "    }})\n",
    "\n",
    "\n",
    "    # Print message to user\n",
    "    print('#'*60)\n",
    "    print(f\"Now training automatic {regressor_label}.\")\n",
    "\n",
    "\n",
    "    # Load hyper parameter \n",
    "    best_params = hyperparameters[regressor_label]\n",
    "    regressor = regressors[regressor_label]\n",
    "\n",
    "\n",
    "    tuned_params = {item[11:]: best_params[item] for item in best_params}\n",
    "    regressor.set_params(**tuned_params)\n",
    "    \n",
    "    # Scale features via Z-score normalization\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    numeric_transformer = scaler\n",
    "\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "       (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", regressor)]\n",
    "    \n",
    "    \n",
    "    model_pipeline = Pipeline(steps = steps)\n",
    "    \n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "    # Define a function to calculate negative RMSE (as a score)\n",
    "    def nrmse(y_true, y_pred):\n",
    "        cost  = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        return cost/(y_true.mean()) * -1\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred_nrmse = nrmse(y_test, y_pred ) * -1\n",
    "    y_pred_rmse = mean_squared_error(y_test, y_pred, squared=False )\n",
    "    y_pred_mae = mean_absolute_error(y_test, y_pred)\n",
    "    y_pred_mape = mean_absolute_percentage_error (y_test, y_pred )\n",
    "    y_pred_cd = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(f' Progress\\n')\n",
    "\n",
    "    print('Results of automatic tuning ')\n",
    "    print(f'RMse  : {y_pred_rmse:.2f} \\n')\n",
    "    print(f'NRMse : {y_pred_nrmse:.2f} \\n')\n",
    "    print(f'Mae   : {y_pred_mae:.2f} \\n')\n",
    "    print(f'Mape  : {y_pred_mape:.2f} \\n')\n",
    "    print(f'R2    : {y_pred_cd:.2f} \\n')\n",
    "\n",
    "\n",
    "    # Look at parameters used by our current forest\n",
    "    print('Parameters currently in use:\\n')\n",
    "    print(regressor.get_params())\n",
    "\n",
    "\n",
    "    ###### best hyperparameters from optimization\n",
    "\n",
    "\n",
    "    # Update dict with Extra Trees\n",
    "    hyperparameters.update({\"RFR\": { \n",
    "        'regressor__bootstrap': bs_df.loc[0, 'param_regressor__bootstrap'],\n",
    "        'regressor__criterion':bs_df.loc[0, 'param_regressor__criterion'],\n",
    "        'regressor__n_estimators': bs_df.loc[0, 'param_regressor__n_estimators'], # should be dec to avoid pure nodes overfit \n",
    "        'regressor__max_features': bs_df.loc[0, 'param_regressor__max_features'],\n",
    "        'regressor__max_depth': bs_df.loc[0, 'param_regressor__max_depth'], # should be dec to avoid pure nodes overfit \n",
    "        'regressor__min_samples_leaf': bs_df.loc[0, 'param_regressor__min_samples_leaf'], # should be inc to avoid pure nodes overfit \n",
    "        'regressor__min_samples_split': bs_df.loc[0, 'param_regressor__min_samples_split'], # should be inc to avoid pure nodes overfit \n",
    "\n",
    "    }})\n",
    "\n",
    "\n",
    "    # Print message to user\n",
    "    print('#'*60)\n",
    "    print(f\"Now training bt best results from optimization {regressor_label}.\")\n",
    "\n",
    "\n",
    "    # Load hyper parameter \n",
    "    best_params = hyperparameters[regressor_label]\n",
    "    regressor = regressors[regressor_label]\n",
    "\n",
    "\n",
    "    tuned_params = {item[11:]: best_params[item] for item in best_params}\n",
    "    regressor.set_params(**tuned_params)\n",
    "\n",
    "    # Scale features via Z-score normalization\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    # Define steps in pipeline\n",
    "\n",
    "    numeric_transformer = scaler\n",
    "\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "       (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", regressor)]\n",
    "    \n",
    "    \n",
    "    model_pipeline = Pipeline(steps = steps)\n",
    "    \n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred_nrmse = nrmse(y_test, y_pred ) * -1\n",
    "    y_pred_rmse = mean_squared_error(y_test, y_pred, squared=False )\n",
    "    y_pred_mae = mean_absolute_error(y_test, y_pred)\n",
    "    y_pred_mape = mean_absolute_percentage_error (y_test, y_pred )\n",
    "    y_pred_cd = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(f' Progress\\n')\n",
    "\n",
    "    print('Results of optimization tuning ')\n",
    "    print(f'RMse  : {y_pred_rmse:.2f} \\n')\n",
    "    print(f'NRMse : {y_pred_nrmse:.2f} \\n')\n",
    "    print(f'Mae   : {y_pred_mae:.2f} \\n')\n",
    "    print(f'Mape  : {y_pred_mape:.2f} \\n')\n",
    "    print(f'R2    : {y_pred_cd:.2f} \\n')\n",
    "\n",
    "\n",
    "    # Look at parameters used by our current forest\n",
    "    print('Parameters currently in use:\\n')\n",
    "    print(regressor.get_params())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb4253",
   "metadata": {},
   "source": [
    "# 4. Hyperparameters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23509eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameter_1 = 'param_regressor__n_estimators'\n",
    "parameter_2 = 'param_regressor__max_depth'\n",
    "parameter_3 = 'param_regressor__max_features'\n",
    "parameter_4 = 'param_regressor__min_samples_leaf'\n",
    "parameter_5 = 'param_regressor__min_samples_split'\n",
    "parameter_6 = 'param_regressor__criterion'\n",
    "\n",
    "file_labels =[ f'NOD_ra',  f'OD_ra', \n",
    "              f'NOD_rab',  f'OD_rab',]\n",
    "for file_label in file_labels:\n",
    "    \n",
    "    bs_df_i =  dir_p/'Hyperparameters'/f'BS_hyper_{file_label}.csv'\n",
    "    \n",
    "    bs_df = pd.read_csv(bs_df_i, sep='\\t')\n",
    "\n",
    "    lower_bound, upper_bound = 0.2,0.8\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, nrows=3)\n",
    "    sns.set(style=\"whitegrid\", color_codes=True, font_scale = 5)\n",
    "    fig.set_size_inches(40,30)\n",
    "    fig.suptitle(file_label)\n",
    "    \n",
    "    print('Pramater')\n",
    "    print('n_estimators, max_depth, max_features, min_samples_leaf, min_samples_split, criterion ')\n",
    "    print(bs_df.loc[0,[parameter_1, parameter_2, parameter_3,parameter_4,parameter_5, parameter_6]].values)\n",
    "    print('Pramater best top 10')\n",
    "    print( bs_df.loc[0:10, 'param_regressor__n_estimators'].min(), # should be dec to avoid pure nodes overfit \n",
    "           bs_df.loc[0:10, 'param_regressor__max_depth'].min(), # should be dec to avoid pure nodes overfit \n",
    "           bs_df.loc[0:10, 'param_regressor__max_features'].mode()[0],\n",
    "           bs_df.loc[0:10, 'param_regressor__min_samples_leaf'].max(), # should be inc to avoid pure nodes overfit \n",
    "           bs_df.loc[0:10, 'param_regressor__min_samples_split'].max(),\n",
    "           bs_df.loc[0:10, 'param_regressor__criterion'].mode()[0],)\n",
    "    \n",
    "    print('Pramater automatic')\n",
    "    print( bs_df.loc[0, 'param_regressor__n_estimators'], # should be dec to avoid pure nodes overfit \n",
    "           bs_df.loc[0, 'param_regressor__max_depth'], # should be dec to avoid pure nodes overfit \n",
    "           bs_df.loc[0, 'param_regressor__max_features'],\n",
    "           bs_df.loc[0, 'param_regressor__min_samples_leaf'], # should be inc to avoid pure nodes overfit \n",
    "           bs_df.loc[0, 'param_regressor__min_samples_split'],\n",
    "           bs_df.loc[0, 'param_regressor__criterion'])\n",
    "\n",
    "    ######\n",
    "\n",
    "    sns.lineplot(parameter_1, 'mean_test_score' , data=bs_df, ax=axs[0,0] ,  color='lightgrey', label ='Test',)\n",
    "    sns.lineplot(parameter_1, 'mean_train_score' , data=bs_df, ax=axs[0,0] , color='darkgrey', label ='Train',)\n",
    "    #axs[0,0].set(ylim=(lower_bound, upper_bound))\n",
    "    axs[0,0].set_title(label = f'{parameter_1[17:]}', weight='bold')\n",
    "    axs[0,0].legend(loc='lower right')\n",
    "    axs[0,0].set_ylabel('Misfit')\n",
    "    axs[0,0].set_xlabel('')\n",
    "\n",
    "\n",
    "    ####\n",
    "\n",
    "    sns.lineplot(x=parameter_2, y='mean_test_score', data=bs_df, ax=axs[0,1], color='lightpink', label ='Test',)\n",
    "    sns.lineplot(x=parameter_2, y='mean_train_score', data=bs_df, ax=axs[0,1], color='red' , label ='Train',)\n",
    "    #axs[0,1].set(ylim=(lower_bound, upper_bound))\n",
    "    axs[0,1].set_title(label = f'{parameter_2[17:]}', weight='bold')\n",
    "    axs[0,1].set_ylabel('')\n",
    "    axs[0,1].set_xlabel('')\n",
    "    axs[0,1].locator_params(axis='x', nbins=len(bs_df)/6)\n",
    "    axs[0,1].set(yticklabels=[])  \n",
    "\n",
    "    axs[0,1].legend(loc='lower right')\n",
    "\n",
    "    ######\n",
    "    \n",
    "        #######\n",
    "    sns.boxplot(parameter_3, 'mean_test_score' , data=bs_df , ax=axs[1,0], color='yellow', )\n",
    "    axs[1,0].set_title(label = f'{parameter_3[17:]}', weight='bold')\n",
    "    sns.boxplot(parameter_3, 'mean_train_score' , data=bs_df , ax=axs[1,0], color='orange', )\n",
    "    axs[1,0].set_ylabel('Misfit')\n",
    "    axs[1,0].set_xlabel('')\n",
    "    axs[1,0].set(yticklabels=[])  \n",
    "    axs[1,0].legend(axs[1,0].patches, ['Test'], loc='lower right')\n",
    "\n",
    "    #######\n",
    "\n",
    "\n",
    "    sns.lineplot(x=parameter_4, y='mean_test_score', data=bs_df, ax=axs[1,1], label ='Test', color='lightgreen')\n",
    "    sns.lineplot(x=parameter_4, y='mean_train_score', data=bs_df, ax=axs[1,1],label='Train', color='green')\n",
    "    #axs[1,1].set(ylim=(lower_bound, upper_bound))\n",
    "    axs[1,1].set_title(label = f'{parameter_4[17:]}', weight='bold')\n",
    "    axs[1,1].legend(loc='lower right')\n",
    "    axs[1,1].set_ylabel('')\n",
    "    axs[1,1].locator_params(axis='x', nbins=len(bs_df)/4)  # set divisor \n",
    "    axs[1,1].set_xlabel('')\n",
    "\n",
    "    ####\n",
    "\n",
    "    sns.lineplot(x=parameter_5, y='mean_test_score', data=bs_df, ax=axs[2,0], color='coral', label ='Test')\n",
    "    #axs[2,0].set(ylim=(lower_bound, upper_bound))\n",
    "    sns.lineplot(x=parameter_5, y='mean_train_score', data=bs_df, ax=axs[2,0], color='red', label='Train')\n",
    "    axs[2,0].set_title(label = f'{parameter_5[17:]}', weight='bold')\n",
    "    #axs[1,1].tick_params(labelsize=40) \n",
    "    axs[2,0].locator_params(axis='x', nbins=len(bs_df)/6)  # set divisor \n",
    "    axs[2,0].set(yticklabels=[])  \n",
    "\n",
    "\n",
    " \n",
    "    axs[2,0].legend(loc='lower right')\n",
    "    axs[2,0].set_ylabel('Misfit')\n",
    "    axs[2,0].set_xlabel('')\n",
    "    ###\n",
    "    \n",
    "        #######\n",
    "    sns.boxplot(parameter_6, 'mean_test_score' , data=bs_df , ax=axs[2,1], color='yellow',  )\n",
    "    axs[2,1].set_title(label = f'{parameter_6[17:]}', weight='bold')\n",
    "    sns.boxplot(parameter_6, 'mean_train_score' , data=bs_df , ax=axs[2,1], color='orange', )\n",
    "\n",
    "    axs[2,1].set_ylabel('')\n",
    "    axs[2,1].set_xlabel('')\n",
    "    axs[2,1].set(yticklabels=[])  \n",
    "    axs[2,1].legend(axs[2,1].patches, ['Test'], loc='lower right')\n",
    "\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
