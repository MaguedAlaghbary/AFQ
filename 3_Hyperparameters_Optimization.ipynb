{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab921c71",
   "metadata": {},
   "source": [
    "# Hyperparameters for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebe301",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import os, sys, pickle, copy, joblib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DIR = Path().resolve() \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import AFQ_utils as utils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, HalvingGridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer , r2_score, mean_absolute_error, explained_variance_score, \\\n",
    "mean_absolute_percentage_error, mean_squared_error, max_error, median_absolute_error\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "\n",
    "\n",
    "plt_params = {\n",
    "    \"axes.titlesize\" : 25, # main\n",
    "    \"axes.labelsize\" : 20,  # labels\n",
    "    \"axes.edgecolor\" : \"black\", \n",
    "    \"axes.linewidth\" : 1, \n",
    "    'xtick.labelsize': 17, # ticks\n",
    "    'ytick.labelsize': 17,\n",
    "    'legend.fontsize': 17,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for windows\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbd194",
   "metadata": {},
   "source": [
    "define constants and dcitionaries to easy looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "obs[\"OBS_REF\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2_CLASS\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST_W\", \"REG\", \"GLIM\"]\n",
    "\n",
    "obs[\"OBS_AFR\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV_SPEED\",\"PV_SPEED\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST\", \"REG\", \"GLIM\"]\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "\n",
    "\n",
    "# Labels for plots etc\n",
    "obs[\"LABELS_gmt\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"S@_v@ 150km\", \"P@_v@ 150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. rho\", \"Crust rho\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]  \n",
    "\n",
    "\n",
    "obs[\"LABELS\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"$S_v$ @150km\", \"$P_v$ @150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. ρ\", \"Crust ρ\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]\n",
    "    \n",
    "# \"vp/vs\"\n",
    "# Units to display in plots etc\n",
    "obs[\"UNITS\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"$\\delta$$S_v$ %\",\"$\\delta$$P_v$ %\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m$^3$\", \"kg/m$^3$\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "\n",
    "\n",
    "\n",
    "obs[\"UNITS_gmt\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"km/s\",\"km/s\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m@+3@+\", \"kg/m@+3@+\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs[\"V_RANGE\"] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,16),]\n",
    "\n",
    "\n",
    "    \n",
    "obs[\"V_RANGE_AFR\"] = [(0,50), (-1,1),(50,250),(20,50),\n",
    "          (-0.075,0.075), (-0.02,0.02), \n",
    "          (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-200, 200), \n",
    "               (3260, 3360), (2650, 2950),\n",
    "              (0,100), (1,6),(1,15),]\n",
    "\n",
    "\n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"categorical\", ]\n",
    "\n",
    "obs[\"CMAPS\"] = [\"SCM/bamako\",  \"SCM/broc\", \"SCM/bamako\", \"SCM/bamako\", \n",
    "             \"SCM/roma\",\"SCM/roma\", \n",
    "             \"SCM/bamako\", \"SCM/broc\", \"SCM/oleron\", \"SCM/broc\", \"SCM/bilbao\",            \n",
    "                \"SCM/batlow\", \"SCM/batlow\",\n",
    "               \"SCM/broc\",  \"gmt/categorical\",\"gmt/categorical\", ]\n",
    "\n",
    "new_index = [0,1,2,3,4,5,6,8,7,9,10,11,12,13,14,15]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "obs = obs.reindex(new_index)\n",
    "\n",
    "#obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n",
    "obs.set_index(['OBS_REF'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe67da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'GHF'\n",
    "coord = ['lon', 'lat']\n",
    "grid_index_world = 'grid_index_world'\n",
    "grid_index_afr = 'grid_index_afr'\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "features_ghf = []\n",
    "\n",
    "\n",
    "\n",
    "features = obs.index.to_list()\n",
    "\n",
    "\n",
    "\n",
    "in_features = set(features)\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.append(grid_index_world)\n",
    "features_ex.append(grid_index_afr)\n",
    "\n",
    "features_ex.append(target)\n",
    "\n",
    "features_ghf = copy.deepcopy(features)\n",
    "features_ghf.append(target)\n",
    "\n",
    "\n",
    "features_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ba3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"Concordance correlation coefficient.\"\"\"\n",
    "    \n",
    "    y_true = y_true.ravel().reshape(-1,)\n",
    "    y_pred = y_pred.ravel().reshape(-1,)\n",
    "    # Remove NaNs\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    df = df.dropna()\n",
    "    y_true = df['y_true']\n",
    "    y_pred = df['y_pred']\n",
    "    # Pearson product-moment correlation coefficients\n",
    "    cor = np.corrcoef(y_true, y_pred)[0][1]\n",
    "    # Mean\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    # Variance\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    # Standard deviation\n",
    "    sd_true = np.std(y_true)\n",
    "    sd_pred = np.std(y_pred)\n",
    "    # Calculate CCC\n",
    "    numerator = 2 * cor * sd_true * sd_pred\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n",
    "    return numerator / denominator\n",
    "\n",
    "def nrmse(y_true, y_pred):\n",
    "    cost  = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    return cost/(y_true.mean()) \n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    cost  = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return (100 - (cost * 100)) \n",
    "\n",
    "def min_e(y_true, y_pred):\n",
    "    cost  = abs(y_true - y_pred)\n",
    "    return cost.min()\n",
    "\n",
    "def mpe(y_true, y_pred):\n",
    "    return np.mean((y_true -y_pred)/y_true) \n",
    "\n",
    "\n",
    "\n",
    "scores_cv = {\n",
    "\n",
    "'RMSE'     :  make_scorer(mean_squared_error , squared=False),\n",
    "'NRMSE'    :  make_scorer(nrmse),\n",
    "'MAE'      :  make_scorer(mean_absolute_error),    \n",
    "'MAPE'     :  make_scorer(mean_absolute_percentage_error ),\n",
    "'ACC'      :  make_scorer(accuracy) ,\n",
    "'MPE'      :  make_scorer(mpe),\n",
    "'CD'       :  make_scorer(r2_score),\n",
    "'EV'       :  make_scorer(explained_variance_score),\n",
    "'MAX_E'    :  make_scorer( max_error),\n",
    "'MIN_E'    :  make_scorer(min_e),\n",
    "'MedAE'    :  make_scorer(median_absolute_error),\n",
    "'CCC'      :  make_scorer(concordance_correlation_coefficient),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c063f",
   "metadata": {},
   "source": [
    "# 3. Determining ranges for Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecaadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KFOLD=5\n",
    "N_ITER=30\n",
    "RANDOM_STATE=42\n",
    "N_JOBS=2\n",
    "VERBOSE=4\n",
    "\n",
    "grid_search_dict = { }\n",
    "\n",
    "train_df = pd.read_csv(DIR /'Dataset'/'Preprocessed'/f'W_OD_rab.csv', sep='\\t')\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df[target] \n",
    "\n",
    "X['GLIM']  = X['GLIM'].astype('int').astype('category')\n",
    "X['REG']   = X['REG'].astype('int').astype('category')\n",
    "\n",
    "\n",
    "\n",
    "#scoring = make_scorer(nrmse , greater_is_better=False )\n",
    "SCORING = make_scorer(mean_squared_error, squared=False, greater_is_better=False)\n",
    "CV = KFold(n_splits=KFOLD, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "\n",
    "# param distribution\n",
    "n_estimators      =  np.arange(100, 1500,50) # overfits\n",
    "max_depth         =  list(range(5,100,3)); max_depth.append(None); # overfits\n",
    "max_features      =  list(range(1, X.shape[1]))  # overfits \n",
    "#min_samples_leaf  =  np.linspace(1, 0.5*X.shape[0], 10)# underfits\n",
    "min_samples_leaf  =  list(range(1, int(0.2*X.shape[0]), 1))# underfits\n",
    "min_samples_split =  np.linspace(2, X.shape[0], 500, dtype='int') # underfits\n",
    "\n",
    "#min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Update dict with Extra Trees\n",
    "search_space = { \n",
    "         'ttr__regressor__n_estimators'     : n_estimators, \n",
    "         'ttr__regressor__max_depth'        : max_depth, \n",
    "         'ttr__regressor__max_features'     : max_features,  \n",
    "         'ttr__regressor__min_samples_leaf' : min_samples_leaf,  \n",
    "         'ttr__regressor__min_samples_split': min_samples_split,  \n",
    "}\n",
    "\n",
    " #Create train and test set  \n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, shuffle=True)\n",
    "\n",
    "\n",
    "for key, seach_value in tqdm_notebook(search_space.items(), desc = 'Processing: '):\n",
    "    print(key)\n",
    "    numeric_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "               (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "               (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ], \n",
    "        #remainder='passthrough', verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "\n",
    "    regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "\n",
    "    steps=[(\"preprocessor\", preprocessor), \n",
    "           #(\"regressor\", regressor), \n",
    "           ('ttr', TransformedTargetRegressor(regressor=regressor, transformer=numeric_transformer))\n",
    "          ]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # GRIDSEACH\n",
    "    #grid_reg_gs = GridSearchCV(pipeline, \n",
    "    #                    param_grid={key: seach_value},  \n",
    "    #                    n_iter=N_ITER,\n",
    "    #                    return_train_score=True, \n",
    "    #                    random_state=RANDOM_STATE ,\n",
    "    #                    scoring=SCORING, \n",
    "    #                    cv=CV, \n",
    "    #                    verbose=VERBOSE, \n",
    "    #                    n_jobs=N_JOBS\n",
    "    #               )\n",
    "    \n",
    "    #grid_reg_gs.fit(X, y)\n",
    "    #grid_search_dict[key] = grid_reg_gs\n",
    "    \n",
    "    #RANDOMSEARCH\n",
    "    grid_reg_rs = RandomizedSearchCV(pipeline, \n",
    "                        param_distributions={key: seach_value},\n",
    "                        n_iter=N_ITER,\n",
    "                        return_train_score=True, \n",
    "                        random_state=RANDOM_STATE ,\n",
    "                        scoring=SCORING, \n",
    "                        cv=CV, \n",
    "                        verbose=VERBOSE, \n",
    "                        n_jobs=N_JOBS\n",
    "                   )\n",
    "    grid_reg_rs.fit(X, y)\n",
    "    grid_search_dict[key] = grid_reg_rs\n",
    "\n",
    "    # HALVINGGRIDSEARCH\n",
    "    #grid_halving_cv = HalvingGridSearchCV(pipeline,\n",
    "    #                    param_grid = {key: seach_value}, \n",
    "    #                    n_iter=N_ITER,\n",
    "    #                    return_train_score=True, \n",
    "    #                    random_state=RANDOM_STATE ,\n",
    "    #                    scoring=SCORING, \n",
    "    #                    cv=CV, \n",
    "    #                    verbose=VERBOSE, \n",
    "    #                    n_jobs=N_JOBS\n",
    "    #                    )\n",
    "\n",
    "    #grid_halving_cv = HalvingRandomSearchCV(pipeline,\n",
    "    #                    param_grid = {key: seach_value}, \n",
    "    #                    n_iter=N_ITER,\n",
    "    #                    return_train_score=True, \n",
    "    #                    random_state=RANDOM_STATE ,\n",
    "    #                    scoring=SCORING, \n",
    "    #                    cv=CV, \n",
    "    #                    verbose=VERBOSE, \n",
    "    #                    n_jobs=N_JOBS\n",
    "    #                   )\n",
    "    #grid_halving_cv.fit(X, y)\n",
    "    \n",
    "    #grid_search_dict[key] = grid_halving_cv\n",
    "print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e733a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt_params)\n",
    "for key, grid in grid_search_dict.items():\n",
    "    utils.plot_grid_search(grid, catgeories=[False]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3)\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    " \n",
    "fig.set_size_inches(20,30)\n",
    "axs = axs.ravel()\n",
    "\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "#sns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for key, grid in grid_search_dict.items():\n",
    "    \n",
    "    grid_df = pd.DataFrame.from_dict(grid.cv_results_)\n",
    "    grid_df['mean_test_score'] = grid_df['mean_test_score'].round(1) *-1\n",
    "    grid_df['mean_train_score'] = grid_df['mean_train_score'].round(1) *-1\n",
    "    \n",
    "    sns.scatterplot(f'param_{key}', 'mean_test_score' , \n",
    "                 data=grid_df, \n",
    "                 ax=axs[i] ,  color='red', label ='Test',)\n",
    "    sns.scatterplot(f'param_{key}', 'mean_train_score' , \n",
    "                 data=grid_df , \n",
    "                 ax=axs[i] , color='darkgrey', label ='Train',)\n",
    "    i+=1\n",
    "    \n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0663647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KFOLD=5\n",
    "N_ITER=150\n",
    "RANDOM_STATE=42\n",
    "N_JOBS=3\n",
    "VERBOSE=4\n",
    "\n",
    "# readjust the ranges\n",
    "\n",
    "# param distribution\n",
    "n_estimators      =  np.arange(250, 550,50) # overfits\n",
    "max_depth         =  list(range(10,25,1)); max_depth.append(None); # overfits\n",
    "max_features      =  list(range(3, 8))  # overfits \n",
    "min_samples_leaf  =  list(range(1, 50, 1))# underfits\n",
    "min_samples_split =  list(range(2, 100, 5)) # underfits\n",
    "\n",
    "\n",
    "\n",
    "# Update dict with Extra Trees\n",
    "search_space = { \n",
    "         'ttr__regressor__n_estimators'     : n_estimators, \n",
    "         'ttr__regressor__max_depth'        : max_depth, \n",
    "         'ttr__regressor__max_features'     : max_features,  \n",
    "         'ttr__regressor__min_samples_leaf' : min_samples_leaf,  \n",
    "         'ttr__regressor__min_samples_split': min_samples_split,  \n",
    "}\n",
    "\n",
    " #Create train and test set  \n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "numeric_transformer = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "           (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "           (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ], \n",
    "    #remainder='passthrough', verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "\n",
    "steps=[(\"preprocessor\", preprocessor), \n",
    "       #(\"regressor\", regressor), \n",
    "       ('ttr', TransformedTargetRegressor(regressor=regressor, transformer=numeric_transformer))\n",
    "      ]\n",
    "\n",
    "# Initialize Pipeline object\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRIDSEACH\n",
    "#grid_reg_gs = GridSearchCV(pipeline, \n",
    "    #                    param_grid=search_space, \n",
    "    #                    return_train_score=True, \n",
    "    #                    random_state=RANDOM_STATE ,\n",
    "    #                    scoring=SCORING, \n",
    "    #                    cv=CV, \n",
    "    #                    verbose=VERBOSE, \n",
    "    #                    n_jobs=N_JOBS\n",
    "    #          )\n",
    "\n",
    "\n",
    "#print('#'*60)\n",
    "#print(f\"Now tuning.\")\n",
    "\n",
    "#grid_reg_gs.fit(X, y)\n",
    "\n",
    "#best_params = grid_reg_gs.best_params_\n",
    "#best_score = grid_reg_gs.best_score_\n",
    "\n",
    "#RANDOMSEARCH\n",
    "\n",
    "\n",
    "\n",
    "#grid_reg_rs =  RandomizedSearchCV(pipeline, \n",
    "    #                    param_distributions=search_space, \n",
    "    #                    n_iter=N_ITER,\n",
    "    #                    return_train_score=True, \n",
    "    #                    random_state=RANDOM_STATE ,\n",
    "    #                    scoring=SCORING, \n",
    "    #                    cv=CV, \n",
    "    #                    verbose=VERBOSE, \n",
    "    #                    n_jobs=N_JOBS\n",
    "    #         )\n",
    "\n",
    "#print('#'*60)\n",
    "#print(f\"Now tuning.\")\n",
    "\n",
    "#grid_reg_rs.fit(X, y)\n",
    "\n",
    "#best_params = grid_reg_rs.best_params_\n",
    "#best_score = grid_reg_rs.best_score_\n",
    "\n",
    "\n",
    "# HALVINGGRIDSEARCH\n",
    "#halving_cv = HalvingGridSearchCV(pipeline, \n",
    "#                        param_grid=search_space, \n",
    "#                        return_train_score=True, \n",
    "#                        random_state=RANDOM_STATE ,\n",
    "#                        scoring=SCORING, \n",
    "#                        cv=CV, \n",
    "#                        verbose=VERBOSE, \n",
    "#                        n_jobs=N_JOBS\n",
    "#             )\n",
    "\n",
    "'''halving_cv = HalvingRandomSearchCV(pipeline, \n",
    "                                param_distributions=search_space, \n",
    "                                return_train_score=True, \n",
    "                                random_state=RANDOM_STATE ,\n",
    "                                scoring=SCORING, \n",
    "                                cv=CV, \n",
    "                                verbose=VERBOSE, \n",
    "                                n_jobs=N_JOBS\n",
    "             )\n",
    "\n",
    "#print('#'*60)\n",
    "#print(f\"Now tuning.\")\n",
    "\n",
    "halving_cv.fit(X, y)\n",
    "\n",
    "best_params = halving_cv.best_params_\n",
    "best_score = halving_cv.best_score_'''\n",
    "\n",
    "###BEYSIANSEARCH\n",
    "\n",
    "# Initialize BaysSearch object   \n",
    "\n",
    "Bayesian_cv = BayesSearchCV(\n",
    "    pipeline,\n",
    "    # (parameter space, # of evaluations)\n",
    "    search_space,\n",
    "    n_iter = N_ITER, \n",
    "    cv = CV, \n",
    "    verbose = VERBOSE, \n",
    "    n_jobs=N_JOBS, \n",
    "    scoring= SCORING,\n",
    "    return_train_score=True,\n",
    "    #refit=False,\n",
    "    random_state=RANDOM_STATE \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Print message to user\n",
    "print('#'*60)\n",
    "print(f\"Now tuning.\")\n",
    "\n",
    "# Fit gscv\n",
    "Bayesian_cv.fit(X, y)  \n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = Bayesian_cv.best_params_\n",
    "best_score = Bayesian_cv.best_score_\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring evaluation comparing baseline and tuned algorithm\n",
    "    \n",
    "for key, scoring in tqdm_notebook( scores_cv.items(), desc='Scoring: ' ):\n",
    "    \n",
    "    numeric_transformer = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "               (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "               (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ], \n",
    "        #remainder='passthrough', verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "\n",
    "    regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "\n",
    "    steps=[(\"preprocessor\", preprocessor), \n",
    "           #(\"regressor\", regressor), \n",
    "           ('ttr', TransformedTargetRegressor(regressor=regressor, transformer=numeric_transformer))\n",
    "          ]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "\n",
    "\n",
    "    score_default  = np.mean(cross_validate(\n",
    "                pipeline, \n",
    "                X, \n",
    "                y,\n",
    "                 n_jobs=N_JOBS,\n",
    "                scoring=scoring,\n",
    "                  cv=CV)['test_score'])\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    score_tuned  = np.mean(cross_validate(\n",
    "                pipeline, \n",
    "                X, \n",
    "                y,\n",
    "                n_jobs=N_JOBS,\n",
    "                scoring=scoring,\n",
    "                  cv=CV)['test_score'])\n",
    "    print(f'{key} before: {score_default:.2f}; after: {score_tuned:.2f}; ratio: {(score_default - score_tuned)/score_default*100:.2f}% \\n')\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving results\n",
    "\n",
    "best_parms_df = pd.DataFrame.from_dict([best_params])\n",
    "\n",
    "# save best results and model\n",
    "best_parms_df.to_csv(DIR /'Hyperparameters'/f'BS_hyperparameter.csv' , index=False, sep='\\t')\n",
    "\n",
    "joblib.dump(Bayesian_cv, DIR /'Hyperparameters'/f'BSCV_model.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dropped_columns = ['mean_fit_time', 'std_fit_time',  'params', 'mean_score_time', 'std_score_time']\n",
    "\n",
    "# all hyp results\n",
    "# gives traing and validation results\n",
    "bs_grid_df = pd.DataFrame(Bayesian_cv.cv_results_)\n",
    "\n",
    "bs_grid_df = bs_grid_df.sort_values(f'rank_test_score', ascending=True).reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "bs_grid_df['mean_test_score'] = bs_grid_df['mean_test_score'] *-1\n",
    "bs_grid_df['mean_train_score'] = bs_grid_df['mean_train_score'] *-1\n",
    "\n",
    "split_test = [f'split{x}_test_score' for x in range(KFOLD)]\n",
    "split_train = [f'split{x}_train_score' for x in range(KFOLD)]\n",
    "\n",
    "dropped_columns.extend(split_test)\n",
    "dropped_columns.extend(split_train)\n",
    "\n",
    "\n",
    "#save results\n",
    "\n",
    "bs_grid_df.to_csv(DIR/'Hyperparameters'/f'BSCV_grid.csv' , index=False, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb4253",
   "metadata": {},
   "source": [
    "# 4. Hyperparameters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23509eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3)\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "fig.set_size_inches(20,30)\n",
    "axs = axs.ravel()\n",
    "\n",
    "params = ['param_ttr__regressor__n_estimators', \n",
    " 'param_ttr__regressor__max_depth',\n",
    " 'param_ttr__regressor__max_features',\n",
    " 'param_ttr__regressor__min_samples_leaf',\n",
    "'param_ttr__regressor__min_samples_split'\n",
    "         ]\n",
    "\n",
    "\n",
    "grid_df = pd.read_csv(DIR/'Hyperparameters'/f'BSCV_grid.csv', sep='\\t')\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "#sns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\n",
    "\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i,param in enumerate(params):\n",
    "    \n",
    "    grid_df['mean_test_score'] = grid_df['mean_test_score'] \n",
    "    grid_df['mean_train_score'] = grid_df['mean_train_score'] \n",
    "    \n",
    "    sns.lineplot(f'{param}', 'mean_test_score' , \n",
    "                 data=grid_df, \n",
    "                 ax=axs[i] ,  color='red', label ='Test',)\n",
    "    sns.lineplot(f'{param}', 'mean_train_score' , \n",
    "                 data=grid_df , \n",
    "                 ax=axs[i] , color='darkgrey', label ='Train',)\n",
    "    i+=1\n",
    "    \n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77630e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = [False, False, False, False, False, ]\n",
    "\n",
    "\n",
    "    \n",
    "bscv_i =  DIR/'Hyperparameters'/f'BSCV_model.pkl'\n",
    "\n",
    "bscv = joblib.load(bscv_i)\n",
    "utils.plot_grid_search(bscv, catgeories=categories)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bb623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
