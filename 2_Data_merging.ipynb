{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439b1091",
   "metadata": {},
   "source": [
    "# 1-Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536a619",
   "metadata": {},
   "source": [
    "import necesaary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0841861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from agrid.grid import Grid\n",
    "from pathlib import Path\n",
    "\n",
    "import os, sys, pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "from scipy import stats, interpolate, spatial, io\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Arc \n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import pyproj as proj\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "\n",
    "#parent directory\n",
    "\n",
    "dir_p = Path().resolve() \n",
    "\n",
    "#constants\n",
    "km = 1000\n",
    "milli = 0.001\n",
    "micro = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "# create grid for each region\n",
    "# crs Coordinate reference system\n",
    "\n",
    "#EPSG is projection\n",
    "# 0.2 degrees equal roughly 20 km\n",
    "\n",
    "World = Grid(res=[0.2, 0.2], up=world_lat_max, down=world_lat_min)\n",
    "\n",
    "\n",
    "# africa grid low resolution 50 x 50 km\n",
    "\n",
    "Africa =    Grid(res=[0.5, 0.5],  left = afr_lon_min, right= afr_lon_max, up=afr_lat_max , down=afr_lat_min)\n",
    "\n",
    "\n",
    "#dictionary of all grids\n",
    "\n",
    "grids = {}\n",
    "\n",
    "grids['Afr'] = Africa\n",
    "grids['World'] = World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5077a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ease looping with dictionaries\n",
    "\n",
    "regions_a = [ 'Afr' ]\n",
    "\n",
    "\n",
    "regions_Total = ['World' ,'Afr']\n",
    "\n",
    "\n",
    "# raster exenets to adjust map\n",
    "raster_extent_Afr = [grids['Afr'].extent[0], grids['Afr'].extent[1], grids['Afr'].extent[3], grids['Afr'].extent[2]]\n",
    "raster_extent_World = [grids['World'].extent[0], grids['World'].extent[1], grids['World'].extent[3], grids['World'].extent[2]]\n",
    "\n",
    "# to correct plot maps\n",
    "raster_extents = {}\n",
    "\n",
    "raster_extents['Afr'] = raster_extent_Afr\n",
    "raster_extents['World'] = raster_extent_World\n",
    "\n",
    "\n",
    "# list of latitudes and longitudes\n",
    "lon_dict = {}\n",
    "lat_dict = {}\n",
    "\n",
    "lon_dict['Afr'] = [afr_lon_min, afr_lon_max]\n",
    "lon_dict['World'] = [world_lon_min, world_lon_max]\n",
    "\n",
    "lat_dict['Afr'] = [afr_lat_min, afr_lat_max]\n",
    "lat_dict['World'] = [world_lat_min, world_lat_max]\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4182eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "''' \n",
    "obs['REF_n'] = [ 'MOHO','LAB', 'RHO_C', 'SV', 'PV', 'CTD',\n",
    "             'RHO_L', 'DEM', \n",
    "                'VOLC_DIST_W', 'A_MEDIAN_W', 'FA', 'SI','LITH_MANTLE', \n",
    "                'EMAG2_CLASS', 'GEOID', 'BG',\n",
    "              'GLIM']'''\n",
    "\n",
    "\n",
    "\n",
    "obs['OBS_REF'] = ['CTD' ,  'SI',\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            'GEOID','FA','DEM','BG', 'EMAG2_CLASS',\n",
    "                   'RHO_L', 'RHO_C', \n",
    "                  'VOLC_DIST_W', 'REG', 'GLIM']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "obs['LABELS'] = ['CTD',  'Shape index', 'LAB depth', 'Moho depth', \n",
    "                'S$_V$ 150km', 'P$_V$ 150km', \n",
    "                'Geoid', 'Free air', 'DEM', 'Bouguer', 'Mag.', \n",
    "                'Lith. ρ', 'Crust ρ',  \n",
    "                 'Volcano d.', 'GliM', 'REG', ]  \n",
    "    \n",
    "    \n",
    "# 'vp/vs'\n",
    "# Units to display in plots etc\n",
    "obs['UNITS'] = ['km',  'si', 'km', 'km',\n",
    "             '$\\delta$ v_s %','$\\delta$ v_p %', \n",
    "             'm', 'mGal', 'm', 'mGal',  'f(nT)', \n",
    "                 'kg/m$^3$', 'kg/m$^3$',\n",
    "                'km',  'class', 'class']\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs['V_RANGE'] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-250,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,15),]\n",
    "    \n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"topo\", ]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0,14,10,16,17,9, 2,1,5,13,12, 8,11,]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "#obs = obs.reindex(new_index)\n",
    "\n",
    "obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 12\n",
    "print(obs)\n",
    "\n",
    "n_obs = len(obs)\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'heat-flow (mW/m2)'\n",
    "coord = ['lon', 'lat']\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "\n",
    "features = obs['OBS_REF'].tolist()\n",
    "\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.append(target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93c3de",
   "metadata": {},
   "source": [
    "# Interpolation from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/nvkelso/natural-earth-vector\n",
    "# natural earth 10 m land shape\n",
    "ne_10m_land = dir_p / 'data'/ 'Shapefiles'/'NE'/ 'ne_10m_land.shp'\n",
    "continents = dir_p /'data'/ 'Shapefiles'/'continents'/ 'continent.shp'\n",
    "\n",
    "# LAND is water vs land\n",
    "#Continet describes each continent\n",
    "\n",
    "#assign_shape Rasterize vector polygons to grid \n",
    "\n",
    "for region in regions_Total:\n",
    "    # Use continental plates instead\n",
    "    grids[region].ds['LAND'] = (('Y', 'X'), grids[region].assign_shape(ne_10m_land, \n",
    "                                               'scalerank', map_to_int = False, burn_val = 1))\n",
    "\n",
    "\n",
    "    grids[region].ds['CONTINENT'] = (('Y', 'X'), grids[region].assign_shape(continents, \n",
    "                                               'CONTINENT', map_to_int = True))\n",
    "    \n",
    "    \n",
    "    \n",
    "    grids[region].map_grid('CONTINENT', raster_extent= raster_extents[region], cmap='jet', figsize=(10,10))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5831df",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfe6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine arc distance\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    Haversine formula returns distance between pairs of coordinates.\n",
    "    coordinates as numpy arrays, lists or real\n",
    "    The haversine formula determines the great-circle distance between \n",
    "    two points on a sphere given their longitudes and latitudes\n",
    "    '''\n",
    "    p = 0.017453292519943295 # pi/180\n",
    "    a = 0.5 - np.cos((lat2-lat1)*p)/2 + np.cos(lat1*p)*np.cos(lat2*p) * (1-np.cos((lon2-lon1)*p)) / 2\n",
    "    return 12742.0176 * np.arcsin(np.sqrt(a)) # returns in km\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908bf5a",
   "metadata": {},
   "source": [
    "function to make inverse distance weight interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2949a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Invdisttree:\n",
    "  \n",
    "    def __init__( self, X, z, leafsize=10, stat=0 ):\n",
    "        assert len(X) == len(z), \"len(X) %d != len(z) %d\" % (len(X), len(z))\n",
    "        self.tree = KDTree( X, leafsize=leafsize )  # build the tree\n",
    "        self.z = z\n",
    "        self.stat = stat\n",
    "        self.wn = 0\n",
    "        self.wsum = None;\n",
    "\n",
    "    def __call__( self, q, nnear=6, eps=0, p=1, weights=None ):\n",
    "            # nnear nearest neighbours of each query point --\n",
    "        q = np.asarray(q)\n",
    "        qdim = q.ndim\n",
    "        if qdim == 1:\n",
    "            q = np.array([q])\n",
    "        if self.wsum is None:\n",
    "            self.wsum = np.zeros(nnear)\n",
    "\n",
    "        self.distances, self.ix = self.tree.query( q, k=nnear, eps=eps )\n",
    "        interpol = np.zeros( (len(self.distances),) + np.shape(self.z[0]) )\n",
    "        jinterpol = 0\n",
    "        for dist, ix in zip( self.distances, self.ix ):\n",
    "            if nnear == 1:\n",
    "                wz = self.z[ix]\n",
    "            elif dist[0] < 1e-10:\n",
    "                wz = self.z[ix[0]]\n",
    "            else:  # weight z s by 1/dist --\n",
    "                w = 1 / dist**p\n",
    "                if weights is not None:\n",
    "                    w *= weights[ix]  # >= 0\n",
    "                w /= np.sum(w)\n",
    "                wz = np.dot( w, self.z[ix] )\n",
    "                if self.stat:\n",
    "                    self.wn += 1\n",
    "                    self.wsum += w\n",
    "            interpol[jinterpol] = wz\n",
    "            jinterpol += 1\n",
    "        return interpol if qdim > 1  else interpol[0]\n",
    "\n",
    "#...............................................................................\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    N = 10000\n",
    "    Ndim = 2\n",
    "    Nask = N  # N Nask 1e5: 24 sec 2d, 27 sec 3d on mac g4 ppc\n",
    "    Nnear = 8  # 8 2d, 11 3d => 5 % chance one-sided -- Wendel, mathoverflow.com\n",
    "    leafsize = 10\n",
    "    eps = .1  # approximate nearest, dist <= (1 + eps) * true nearest\n",
    "    p = 1  # weights ~ 1 / distance**p\n",
    "    cycle = .25\n",
    "    seed = 1\n",
    "    \n",
    "     # python this.py N= ...\n",
    "\n",
    "    np.random.seed(seed )\n",
    "    np.set_printoptions( 3, threshold=100, suppress=True )  # .3f\n",
    "\n",
    "    print( \"\\nInvdisttree:  N %d  Ndim %d  Nask %d  Nnear %d  leafsize %d  eps %.2g  p %.2g\" % (\n",
    "        N, Ndim, Nask, Nnear, leafsize, eps, p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3646808",
   "metadata": {},
   "source": [
    "# read in dataseys from xyz folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f076e",
   "metadata": {},
   "source": [
    "Interpolation for wolrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf6274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region = 'World'\n",
    "\n",
    "\n",
    "seismic_sv_f = dir_p / 'data'/'dataset'/'XYZ'/'d_Shear_Wave_Speed'/'SV_W_150.xyz'\n",
    "\n",
    "seismic_pv_f = dir_p / 'data'/'dataset'/'XYZ'/'e_Pressure_Wave_Speed'/'PV_W_150.xyz'\n",
    "\n",
    "\n",
    "CTD_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'f_CTD'/'CTD.xyz'\n",
    "\n",
    "\n",
    "LAB_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'b_LAB'/'LAB.xyz'\n",
    "\n",
    "\n",
    "MOHO_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'a_Moho_Depth'/'MOHO.xyz'\n",
    "\n",
    "curveture_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'n_Shape_Index_Curvature' / 'SI.xyz'\n",
    "\n",
    "\n",
    "rho_c_xyz_o =  dir_p / 'data' /'dataset'/ 'XYZ'/'j_Crustal_Average_Density'/'RHO_C.xyz'\n",
    "rho_l_xyz_o =   dir_p / 'data' /'dataset'/ 'XYZ'/'i_Lithosphere_Average_Density'/'RHO_L.xyz'\n",
    "\n",
    "\n",
    "geoid_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'l_geoid_height' / 'EIGEN-6C4_geoid.xyz'\n",
    "FA_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'k_Free_Air' / 'EIGEN-6C4_gravity_anomaly_cl.xyz'\n",
    "bg_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'m_Bouguer' / 'EIGEN-6C4_gravity_anomaly_bg.xyz'\n",
    "\n",
    "\n",
    "files = [seismic_sv_f,seismic_pv_f, CTD_xyz_o,\n",
    "    MOHO_xyz_o , curveture_xyz_o, LAB_xyz_o, rho_c_xyz_o, rho_l_xyz_o, \n",
    "          geoid_xyz_o, FA_xyz_o, bg_xyz_o\n",
    "    \n",
    "]\n",
    "\n",
    "labels = ['SV', 'PV', 'CTD',  'MOHO', 'SI', 'LAB','RHO_C', 'RHO_L', \n",
    "          'GEOID', 'FA', 'BG'\n",
    "         ]\n",
    "\n",
    "\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "\n",
    "    df = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "    # real values\n",
    "    X = df[['lat','lon',]].values\n",
    "    y = df.iloc[:,2].values.reshape(-1,1)\n",
    "\n",
    "    # coordinates\n",
    "    coords = np.stack([  grids[region].ds.coords['YV'].values.ravel() , \n",
    "                    grids[region].ds.coords['XV'].values.ravel(), ], axis=1)\n",
    "\n",
    "\n",
    "    invdisttree = Invdisttree( X, y, leafsize=leafsize, stat=1 )\n",
    "    interpol = invdisttree( coords, nnear=Nnear, eps=eps, p=p )\n",
    "\n",
    "\n",
    "    grids[region].ds[f'{labels[i]}'] = (('Y', 'X'),  interpol.reshape(grids[region].ny,grids[region].nx))\n",
    "\n",
    "    #save \n",
    "    grids[region].grid_to_raster(grids[region].ds[f'{labels[i]}'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_{labels[i]}.nc')\n",
    "    \n",
    "    print(f' terminated {labels[i]}')\n",
    "    \n",
    "\n",
    "print(f' terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region= 'World'\n",
    "\n",
    "volc_max_dist = 100\n",
    "a_max_dist = 250\n",
    "\n",
    "\n",
    "# columns axis -1 of pairs lon lat all pairs of coord in the world\n",
    "\n",
    "HGHF_ra = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_ra.xyz'\n",
    "HGHF_rab = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_rab.xyz'\n",
    "\n",
    "volc_f = dir_p / 'data' / 'dataset'/'XYZ'/'c_Distance_to_nearest_volcano'/'volcanos.xyz'\n",
    "volc_final = pd.read_csv(volc_f, sep='\\t')\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "model_coords = np.stack([grids[region].lon.ravel(), grids[region].lat.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "max_dist = np.sum(grids[region].res)/1.5 \n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for ix,file in enumerate([HGHF_ra, HGHF_rab]):\n",
    "    \n",
    "    print(file)\n",
    "    hf_final = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "   \n",
    "    ''' spatial.KDTree This class provides an index into a set of k-dimensional points which can be used to rapidly \n",
    "    look up the nearest neighbors of any point.\n",
    "    retuen distance of Return only neighbors within this max_dist\n",
    "    search in world pairs comapre it to df pairs get the distance and index in world\n",
    "    .\n",
    "    '''\n",
    "\n",
    "    #hf_dict[f'hf_best_{region}'] = hf_final\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hf_coords = np.stack([hf_final['lon'], hf_final['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hf_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    hf_final.loc[:,'grid_index'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "\n",
    "    dists_km = distance(hf_final['lat'].values, hf_final['lon'].values, \n",
    "                    grids[region].lat.ravel()[indexs],  grids[region].lon.ravel()[indexs])\n",
    "\n",
    "    hf_final.loc[:,'dist_from_grid'] = dists_km*km\n",
    "\n",
    "\n",
    "    # this process is a bit faster but not complete for world\n",
    "    # not incorprated into the grid\n",
    "    ###### voclano distance\n",
    "\n",
    "    print(region)\n",
    "    volc_lon = volc_final['lon'].values\n",
    "    volc_lat = volc_final['lat'].values\n",
    "\n",
    "\n",
    "    lon_hf= hf_final['lon'].values\n",
    "    lat_hf = hf_final['lat'].values\n",
    "\n",
    "    volc_dists = np.zeros_like(lon_hf)\n",
    "    ii = range(len(lon_hf))\n",
    "    print(ii)\n",
    "\n",
    "    # get the minmium distance in a from volc to best and good\n",
    "    for _lat, _lon, i in zip(lat_hf, lon_hf, ii):   \n",
    "        volc_distances = distance(_lat, _lon, volc_lat, volc_lon)\n",
    "        volc_dists[i] = np.nanmin(volc_distances)\n",
    "\n",
    "    hf_final.loc[:,'VOLC_DIST'] = volc_dists \n",
    "    hf_final.loc[:,'VOLC_DIST_W']= np.clip(1 - volc_dists/volc_max_dist, 0, 1)  \n",
    "    #hf_final.loc[:,'VOLC_DIST_W']= np.clip(volc_dists/volc_max_dist, 0, 1) * 100\n",
    "\n",
    "    print(f'Volc Terminated ')\n",
    "\n",
    "   \n",
    "    df_dict[ix] = hf_final\n",
    "\n",
    "\n",
    "hf_final_ra = df_dict[0]\n",
    "hf_final_rab = df_dict[1]\n",
    "\n",
    "print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7144fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for extra measured data even if they are not reliable  integrate ABC ABCD\n",
    "\n",
    "HGHF_rabc = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_rabc.xyz'\n",
    "HGHF_rabcd = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_rabcd.xyz'\n",
    "\n",
    "\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for ix,file in enumerate([HGHF_rabc, HGHF_rabcd]):\n",
    "    \n",
    "    print(file)\n",
    "    hf_final = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "   \n",
    "    ''' spatial.KDTree This class provides an index into a set of k-dimensional points which can be used to rapidly \n",
    "    look up the nearest neighbors of any point.\n",
    "    retuen distance of Return only neighbors within this max_dist\n",
    "    search in world pairs comapre it to df pairs get the distance and index in world\n",
    "    .\n",
    "    '''\n",
    "\n",
    "    #hf_dict[f'hf_best_{region}'] = hf_final\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hf_coords = np.stack([hf_final['lon'], hf_final['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hf_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    hf_final.loc[:,'grid_index'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "\n",
    "    dists_km = distance(hf_final['lat'].values, hf_final['lon'].values, \n",
    "                    grids[region].lat.ravel()[indexs],  grids[region].lon.ravel()[indexs])\n",
    "\n",
    "    hf_final.loc[:,'dist_from_grid'] = dists_km*km\n",
    "\n",
    "\n",
    "    # this process is a bit faster but not complete for world\n",
    "    # not incorprated into the grid\n",
    "    ###### voclano distance\n",
    "\n",
    "    print(region)\n",
    "    volc_lon = volc_final['lon'].values\n",
    "    volc_lat = volc_final['lat'].values\n",
    "\n",
    "\n",
    "    lon_hf= hf_final['lon'].values\n",
    "    lat_hf = hf_final['lat'].values\n",
    "\n",
    "    volc_dists = np.zeros_like(lon_hf)\n",
    "    ii = range(len(lon_hf))\n",
    "    print(ii)\n",
    "\n",
    "    # get the minmium distance in a from volc to best and good\n",
    "    for _lat, _lon, i in zip(lat_hf, lon_hf, ii):   \n",
    "        volc_distances = distance(_lat, _lon, volc_lat, volc_lon)\n",
    "        volc_dists[i] = np.nanmin(volc_distances)\n",
    "\n",
    "    hf_final.loc[:,'VOLC_DIST'] = volc_dists \n",
    "    hf_final.loc[:,'VOLC_DIST_W']= np.clip(1 - volc_dists/volc_max_dist, 0, 1) \n",
    "    #hf_final.loc[:,'VOLC_DIST_W']= np.clip(volc_dists/volc_max_dist, 0, 1) * 100\n",
    "\n",
    "    print(f'Volc Terminated ')\n",
    "\n",
    "                    \n",
    "    df_dict[ix] = hf_final\n",
    "\n",
    "\n",
    "hf_final_rabc = df_dict[0]\n",
    "hf_final_rabcd = df_dict[1]\n",
    "\n",
    "print('Terminated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88c05d",
   "metadata": {},
   "source": [
    "interpolatation for Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seismic_sv_f = dir_p / 'data'/'dataset'/'XYZ'/'d_Shear_Wave_Speed'/'SV_A_150.xyz'\n",
    "\n",
    "seismic_pv_f = dir_p / 'data'/'dataset'/'XYZ'/'e_Pressure_Wave_Speed'/'PV_A_150.xyz'\n",
    "\n",
    "\n",
    "seismic_sv_speed_f = dir_p / 'data'/'dataset'/'XYZ'/'d_Shear_Wave_Speed'/'SV_A_150_speed.xyz'\n",
    "\n",
    "seismic_pv_speed_f = dir_p / 'data'/'dataset'/'XYZ'/'e_Pressure_Wave_Speed'/'PV_A_150_speed.xyz'\n",
    "\n",
    "CTD_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'f_CTD'/'CTD.xyz'\n",
    "\n",
    "LAB_xyz_o =  dir_p / 'data'/'dataset'/'XYZ'/'b_LAB'/'LAB.xyz'\n",
    "MOHO_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'a_Moho_Depth'/'MOHO.xyz'\n",
    "\n",
    "\n",
    "curveture_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'n_Shape_Index_Curvature' / 'SI.xyz'\n",
    "\n",
    "rho_c_xyz_o =  dir_p / 'data'/'dataset'/'XYZ'/'j_Crustal_Average_Density'/'RHO_C.xyz'\n",
    "rho_l_xyz_o =  dir_p / 'data'/'dataset'/'XYZ'/'i_Lithosphere_Average_Density'/'RHO_L.xyz'\n",
    "\n",
    "\n",
    "geoid_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'l_geoid_height' / 'EIGEN-6C4_geoid.xyz'\n",
    "FA_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'k_Free_Air' / 'EIGEN-6C4_gravity_anomaly_cl.xyz'\n",
    "bg_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'m_Bouguer' / 'EIGEN-6C4_gravity_anomaly_bg.xyz'\n",
    "\n",
    "\n",
    "files = [seismic_sv_f,seismic_pv_f, seismic_sv_speed_f,seismic_pv_speed_f, CTD_xyz_o,\n",
    "    MOHO_xyz_o , curveture_xyz_o, LAB_xyz_o, rho_c_xyz_o, rho_l_xyz_o, \n",
    "          geoid_xyz_o, FA_xyz_o, bg_xyz_o\n",
    "    \n",
    "]\n",
    "\n",
    "labels = ['SV', 'PV', 'SV_SPEED', 'PV_SPEED','CTD',  'MOHO', 'SI', 'LAB','RHO_C', 'RHO_L', \n",
    "          'GEOID', 'FA', 'BG'\n",
    "         ]\n",
    "\n",
    "region = 'Afr'\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "\n",
    "    df = pd.read_csv(file,sep='\\t')\n",
    "\n",
    "    df.sort_values(by=['lon','lat'], ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # real values\n",
    "    X = df[['lat','lon']].values\n",
    "    y = df.iloc[:,2].values.reshape(-1,1)\n",
    "\n",
    "    # coordinates\n",
    "    coords = np.stack([  grids[region].ds.coords['YV'].values.ravel() , \n",
    "                    grids[region].ds.coords['XV'].values.ravel(), ], axis=1)\n",
    "\n",
    "\n",
    "    invdisttree = Invdisttree( X, y, leafsize=leafsize, stat=1 )\n",
    "    interpol = invdisttree( coords, nnear=Nnear, eps=eps, p=p )\n",
    "\n",
    "\n",
    "    grids[region].ds[f'{labels[i]}'] = (('Y', 'X'),  interpol.reshape(grids[region].ny,grids[region].nx))\n",
    "\n",
    "    #save \n",
    "    grids[region].grid_to_raster(grids[region].ds[f'{labels[i]}'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_{labels[i]}.nc')\n",
    "\n",
    "    print(f' terminated {labels[i]}')\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37f83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "volc_max_dist = 100\n",
    "a_max_dist = 250\n",
    "\n",
    "\n",
    "# this process is slow to map all grid\n",
    "\n",
    "geod = proj.Geod(ellps='WGS84')\n",
    "vol_int = volc_final[volc_final['lon'].between(afr_lon_min, afr_lon_max) & volc_final['lat'].between(afr_lat_min, afr_lat_max)]\n",
    "v_lon = vol_int['lon'].values\n",
    "v_lat = vol_int['lat'].values\n",
    "\n",
    "region ='Afr'\n",
    "\n",
    "volcs = np.zeros(grids[region].nn)\n",
    "\n",
    "\n",
    "v_n = len(v_lon)\n",
    "\n",
    "v_k = 1\n",
    "\n",
    "\n",
    "lats = grids[region].ds['lat'].values\n",
    "lons = grids[region].ds['lon'].values\n",
    "\n",
    "for x in range(grids[region].nx):\n",
    "    print('.')\n",
    "    for y in range(grids[region].ny):\n",
    "        lat = lats[y,x]\n",
    "        lon = lons[y,x]\n",
    "        _, _, ds = geod.inv(v_n*[lon], v_n*[lat], v_lon, v_lat)\n",
    "        idx = np.argpartition(ds, v_k)[:v_k]\n",
    "        volcs[y,x] = np.sum(np.take(ds, idx))/v_k/km\n",
    "\n",
    "grids[region].ds['VOLC_DIST'] = (('Y', 'X'), volcs)\n",
    "grids[region].ds['VOLC_DIST_W'] = (('Y', 'X'), np.clip(1 - volcs/volc_max_dist, 0, 1)) \n",
    "#grids[region].ds['VOLC_DIST_W']= np.clip(volcs/volc_max_dist, 0, 1) * 100\n",
    "\n",
    "\n",
    "#save \n",
    "grids[region].grid_to_raster(grids[region].ds[f'VOLC_DIST_W'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_VOLC_DIST_W.nc')\n",
    "grids[region].grid_to_raster(grids[region].ds[f'VOLC_DIST'], save_name=dir_p /'Grids'/'inputs'/f'{region}_VOLC_DIST.nc')\n",
    "\n",
    "print(f'Terminated {region}')\n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138bd3c",
   "metadata": {},
   "source": [
    "common datassets for all grids \n",
    "\n",
    "- no interpolation needed \n",
    "- nearest interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "etopo_f = dir_p / 'data' /'dataset'/ 'Reference'/'h_Elevation' / 'ETOPO1_Bed_g_geotiff.tif'\n",
    "emag2v3_f =  dir_p / 'data' /'dataset'/ 'Reference'/'g_Magnetic'/'EMAG2_V3_UpCont_DataTiff_m0.tif'\n",
    "glim_f = dir_p / 'data' /'dataset'/ 'Reference'/'p_Global_Lithological_Map' / 'glim_wgs84_0point5deg.txt.asc'\n",
    "reg_f = dir_p / 'data'/'dataset'/'XYZ'/'o_Tectonic_Regionalization_Class'/'REG.xyz'\n",
    "\n",
    "\n",
    "\n",
    "print('\\nDEM : ', end='')\n",
    "for region in regions_Total:\n",
    "    print(f' {region} ,', end='')\n",
    "    etopo_f = etopo_f\n",
    "    grids[region].ds['DEM'] = (('Y', 'X'), grids[region].read_raster(etopo_f, src_crs=4326))\n",
    "    print('DEM ,', end='')\n",
    "\n",
    "    grids[region].grid_to_raster(grids[region].ds[f'DEM'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_DEM.nc')\n",
    "       \n",
    "print('\\nTerminated')\n",
    "\n",
    "\n",
    "# correct values to limit them [-1,1]\n",
    "def mag_log(data, C=400, clip_min=-1, clip_max = 1):\n",
    "     return np.clip(np.sign(data)*np.log(1+np.abs(data)/C), clip_min, clip_max)\n",
    "\n",
    "print('\\nMagnatic : ', end='')\n",
    "for region in regions_Total:\n",
    "    print(f' {region} ,', end='')\n",
    "    grids[region].ds['EMAG2'] = (('Y', 'X'), grids[region].read_raster(emag2v3_f, \n",
    "                                                   src_crs=4326, no_data = -99999))\n",
    "    grids[region].ds['EMAG2_CLASS'] = (('Y', 'X'), mag_log(grids[region].ds['EMAG2'].values)) \n",
    "    \n",
    "    grids[region].grid_to_raster(grids[region].ds[f'EMAG2_CLASS'], save_name=dir_p /  'Grids'/'inputs'/f'{region}_EMAG2_CLASS.nc')\n",
    "    grids[region].grid_to_raster(grids[region].ds[f'EMAG2'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_EMAG2.nc')\n",
    "\n",
    "    \n",
    "    \n",
    "print('\\nTerminated')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\nScematic geology : ', end='')\n",
    "for region in regions_Total:\n",
    "    print(f' {region} ,', end='')\n",
    "    #GliM as it is\n",
    "    grids[region].ds['GLIM'] = (('Y', 'X'), grids[region].read_raster(glim_f, src_crs=4326))\n",
    "    #grids[region].ds['GLIM'] = grids[region].ds['GLIM'].where(grids[region].ds['GLIM']>0, np.nan)\n",
    "    #grids[region].ds['GLIM'] = grids[region].ds['GLIM'].where(grids[region].ds['GLIM']<16, np.nan)\n",
    "    grids[region].ds['GLIM'] = grids[region].ds['GLIM'].where(grids[region].ds['GLIM']>0, 15)\n",
    "    grids[region].ds['GLIM'] = grids[region].ds['GLIM'].where(grids[region].ds['GLIM']<17,15)\n",
    "    \n",
    "    print('GLIM..', end='')\n",
    "    \n",
    "    grids[region].grid_to_raster(grids[region].ds[f'GLIM'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_GLIM.nc')\n",
    "\n",
    "\n",
    "print('Terminated')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\nClasses: ', end='')\n",
    "for region in regions_Total:\n",
    "    print(f'{region}..', end='')\n",
    "    grids[region].ds['REG'] = (('Y', 'X'), \n",
    "                grids[region].read_ascii(reg_f, \n",
    "                               x_col=0, y_col=1, data_col=2, skiprows = 1, interpol='nearest') )\n",
    "    \n",
    "    grids[region].grid_to_raster(grids[region].ds[f'REG'], save_name=dir_p / 'Grids'/'inputs'/f'{region}_REG.nc')\n",
    "\n",
    "print('terminated', end='')    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8087a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import grids to veirfy that interpolated values were saved\n",
    "    \n",
    "for region in regions_Total:   \n",
    "    if region == 'Afr':\n",
    "        files_afr = [filename for filename in os.listdir(dir_p / 'Grids'/'inputs') if (filename.startswith(f'{region}'))]\n",
    "        labels_afr = [ filename.replace(f'{region}_', '').replace('.nc', '')  for filename in files_afr]\n",
    "        for label, file in zip(labels_afr,files_afr):\n",
    "            print(file)\n",
    "            path = dir_p / 'Grids'/'inputs'/f'{file}'\n",
    "            grids[region].ds[f'{label}'] = (('Y', 'X'), grids[region].read_raster(path , src_crs=4326))\n",
    "    else :\n",
    "        files = [filename for filename in os.listdir(dir_p / 'Grids'/'inputs') if filename.startswith(f'{region}')]\n",
    "        labels = [ filename.replace(f'{region}_', '').replace('.nc', '')  for filename in files]\n",
    "        for label, file in zip(labels,files):\n",
    "            path = dir_p / 'Grids'/'inputs'/f'{file}'\n",
    "            grids[region].ds[f'{label}'] = (('Y', 'X'), grids[region].read_raster(path , src_crs=4326))\n",
    "\n",
    "\n",
    "    print('terminated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64610d7f",
   "metadata": {},
   "source": [
    "# 2 - Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22127d",
   "metadata": {},
   "source": [
    "get index from grid and distance and transfer it to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer obs values from grid to hf integrate grid index\n",
    "\n",
    "\n",
    "training_f = dir_p /'data'/'dataset'/'Preprocessed'\n",
    "\n",
    "hf_final_ra_copy = copy.deepcopy(hf_final_ra)\n",
    "hf_final_rab_copy = copy.deepcopy(hf_final_rab)\n",
    "\n",
    "\n",
    "region = 'World'\n",
    "\n",
    "for hf_final,label in zip([hf_final_ra_copy, hf_final_rab_copy], ['ra_int.csv','rab_int.csv']):\n",
    "    model_coords = np.stack([grids[region].lon.ravel(), grids[region].lat.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum(grids[region].res)/1.5 \n",
    "\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hf_coords = np.stack([hf_final['lon'], hf_final['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hf_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    hf_final.loc[:,'grid_index'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "\n",
    "    dists_km = distance(hf_final['lat'].values, hf_final['lon'].values, \n",
    "                    grids[region].lat.ravel()[indexs],  grids[region].lon.ravel()[indexs])\n",
    "\n",
    "    hf_final.loc[:,'dist_from_grid'] = dists_km*km\n",
    "\n",
    "\n",
    "    for observable in list(grids[region].ds):\n",
    "            hf_final.loc[:,observable] = grids[region].ds[observable].values.ravel()[hf_final.loc[:,'grid_index'].values] \n",
    "            \n",
    "\n",
    "    local = 'W'\n",
    "    hf_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    hf_final.reset_index( inplace=True,drop=True)\n",
    "    hf_final.to_csv(training_f/f'Training_{local}_{label}' , index=False, header=True)\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hf_final_ra_copy = copy.deepcopy(hf_final_ra)\n",
    "hf_final_rab_copy = copy.deepcopy(hf_final_rab)\n",
    "\n",
    "region = 'World'\n",
    "\n",
    "for hf_final,label in zip([hf_final_ra_copy, hf_final_rab_copy], ['W_merged_ra_int.csv','W_merged_rab_int.csv']):\n",
    "    \n",
    "    hf_final_Afr = hf_final[(hf_final['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hf_final['lat'].between(afr_lat_min, afr_lat_max))]\n",
    "    \n",
    "\n",
    "    \n",
    "    region ='World'\n",
    "\n",
    "\n",
    "    \n",
    "    for observable in list(grids[region].ds):\n",
    "            hf_final.loc[:,observable] = grids[region].ds[observable].values.ravel()[hf_final.loc[:,'grid_index'].values] \n",
    "            \n",
    "    #######\n",
    "\n",
    "    \n",
    "    \n",
    "    region = 'Afr'\n",
    "    \n",
    "     # from grid afr take values into world\n",
    "    model_coords = np.stack([grids[region].lon.ravel(), grids[region].lat.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum(grids[region].res)/1.5 \n",
    "\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hf_coords = np.stack([hf_final_Afr['lon'], hf_final_Afr['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hf_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    \n",
    "    #modify grid index of world extracted africa with Africas\n",
    "    hf_final_Afr.loc[:,'grid_index_afr'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "\n",
    "    dists_km = distance(hf_final_Afr['lat'].values, hf_final_Afr['lon'].values, \n",
    "                    grids[region].lat.ravel()[indexs],  grids[region].lon.ravel()[indexs])\n",
    "\n",
    "    #hf_final_Afr.loc[:,'dist_from_grid'] = dists_km*km\n",
    "    \n",
    "    #transfer value of africa into world\n",
    "    for observable in list(grids[region].ds):\n",
    "            hf_final_Afr.loc[:,observable] = grids[region].ds[observable].values.ravel()[hf_final_Afr.loc[:,'grid_index_afr'].values] \n",
    "    hf_final_Afr.drop(['grid_index_afr'], inplace=True, axis=1)\n",
    "    \n",
    "    \n",
    "     #######\n",
    "\n",
    "\n",
    "    print(len(hf_final))\n",
    "    hf_final_copy = hf_final[~(hf_final['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hf_final['lat'].between(afr_lat_min, afr_lat_max))]\n",
    "    \n",
    "\n",
    "    print(len(hf_final_copy))\n",
    "\n",
    "\n",
    "    frames = [hf_final_copy,hf_final_Afr ]\n",
    "    hf_final_copy = pd.concat(frames).reset_index(drop=True)\n",
    "\n",
    "    print(len(hf_final_copy))\n",
    "    \n",
    "\n",
    "    hf_final_copy.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    hf_final_copy.reset_index( inplace=True,drop=True)\n",
    "    hf_final_copy.to_csv(training_f/f'Training_{label}' , index=False, header=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa28b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "\n",
    "\n",
    "for hf_final,label in zip([hf_final_ra_copy, hf_final_rab_copy], ['ra_int.csv','rab_int.csv']):\n",
    "\n",
    "    region = 'Afr'\n",
    "    \n",
    "    gt_afr_df = hf_final[(hf_final['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hf_final['lat'].between(afr_lat_min, afr_lat_max))].reset_index(drop=True)\n",
    "\n",
    "    model_coords = np.stack([grids[region].lon.ravel(), grids[region].lat.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum(grids[region].res)/1.5 \n",
    "    \n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hf_coords = np.stack([gt_afr_df['lon'], gt_afr_df['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hf_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    gt_afr_df.loc[:,'grid_index'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "\n",
    "    dists_km = distance(gt_afr_df['lat'].values, gt_afr_df['lon'].values, \n",
    "                    grids[region].lat.ravel()[indexs],  grids[region].lon.ravel()[indexs])\n",
    "\n",
    "    gt_afr_df.loc[:,'dist_from_grid'] = dists_km*km\n",
    "    \n",
    "    for observable in list(grids[region].ds):\n",
    "            gt_afr_df.loc[:,observable] = grids[region].ds[observable].values.ravel()[gt_afr_df.loc[:,'grid_index'].values] \n",
    "    \n",
    "    \n",
    "    gt_afr_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    gt_afr_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    gt_afr_df.to_csv(training_f/f'Training_{region}_{label}' ,  index=False, header=True)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11670daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "\n",
    "hf_final_rabc_copy = copy.deepcopy(hf_final_rabc)\n",
    "hf_final_rabcd_copy = copy.deepcopy(hf_final_rabcd)\n",
    "\n",
    "for hf_final,label in zip([hf_final_rabc_copy, hf_final_rabcd_copy], ['rabc_int.csv','rabcd_int.csv']):\n",
    "\n",
    "    region = 'Afr'\n",
    "    \n",
    "    gt_afr_df = hf_final[(hf_final['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hf_final['lat'].between(afr_lat_min, afr_lat_max))].reset_index(drop=True)\n",
    "\n",
    "    model_coords = np.stack([grids[region].lon.ravel(), grids[region].lat.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum(grids[region].res)/1.5 \n",
    "    \n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hf_coords = np.stack([gt_afr_df['lon'], gt_afr_df['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hf_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    gt_afr_df.loc[:,'grid_index'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "\n",
    "    dists_km = distance(gt_afr_df['lat'].values, gt_afr_df['lon'].values, \n",
    "                    grids[region].lat.ravel()[indexs],  grids[region].lon.ravel()[indexs])\n",
    "\n",
    "    gt_afr_df.loc[:,'dist_from_grid'] = dists_km*km\n",
    "    \n",
    "    for observable in list(grids[region].ds):\n",
    "            gt_afr_df.loc[:,observable] = grids[region].ds[observable].values.ravel()[gt_afr_df.loc[:,'grid_index'].values] \n",
    "    \n",
    "\n",
    "    gt_afr_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    gt_afr_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    gt_afr_df.to_csv(training_f/f'Training_{region}_{label}',  index=False, header=True)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59616c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
