{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b0a55a",
   "metadata": {},
   "source": [
    "# 1. Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bd497",
   "metadata": {},
   "source": [
    "donlowad datasets from their sources\n",
    "\n",
    "get all datasets into xyz format (lon, lat, observabble)\n",
    "\n",
    "Rename columns into lon lat observable\n",
    "\n",
    "make sure that longitude is in -180 - 180 format\n",
    "\n",
    "remove missing latitudes and longitues\n",
    "\n",
    "save xyz files into xyz folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ee669",
   "metadata": {},
   "source": [
    "# Import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10219e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agrid.grid import Grid\n",
    "from pathlib import Path\n",
    "import os, sys, pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "from scipy import stats, interpolate, spatial, io\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Arc \n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import pyproj as proj\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06a408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "#parent directory\n",
    "\n",
    "dir_p = Path().resolve() \n",
    "\n",
    "#constants\n",
    "km = 1000\n",
    "milli = 0.001\n",
    "micro = 0.000001\n",
    "\n",
    "# fig size for presentation\n",
    "fig_pres_small = (4,3)\n",
    "\n",
    "fig_pres_small_cbar = (4,2)\n",
    "#aspect ratio\n",
    "fig_pres_large = (16,9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "\n",
    "# create grid for each region\n",
    "# crs Coordinate reference system\n",
    "\n",
    "#EPSG is projection\n",
    "# 0.2 degrees equal roughly 20 km\n",
    "\n",
    "World = Grid(res=[0.2, 0.2], up=world_lat_max, down=world_lat_min)\n",
    "\n",
    "# africa grid\n",
    "\n",
    "Africa =    Grid(res=[0.2, 0.2],  left = afr_lon_min, right= afr_lon_max, up=afr_lat_max , down=afr_lat_min)\n",
    "\n",
    "# africa grid low resolution 50 x 50 km\n",
    "\n",
    "Africa_50 =    Grid(res=[0.5, 0.5],  left = afr_lon_min, right= afr_lon_max, up=afr_lat_max , down=afr_lat_min)\n",
    "\n",
    "\n",
    "#dictionary of all grids\n",
    "\n",
    "grids = {}\n",
    "\n",
    "grids['Afr'] = Africa\n",
    "grids['Afr_50'] = Africa_50\n",
    "grids['World'] = World\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c0ce0",
   "metadata": {},
   "source": [
    "create dictionaries to ease looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5077a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ease looping with dictionaries\n",
    "\n",
    "regions_a_a5 = [ 'Afr','Afr_50' ]\n",
    "\n",
    "regions_w_a = [ 'World' ,'Afr',]\n",
    "\n",
    "regions_w_a= [ 'World' ,'Afr']\n",
    "\n",
    "regions_a_a5 = [ 'Afr', 'Afr_50']\n",
    "\n",
    "regions_w_a_a5 = [ 'World' ,'Afr', 'Afr_50']\n",
    "\n",
    "\n",
    "\n",
    "regions_Total = ['World' ,'Afr', 'Afr_50',]\n",
    "\n",
    "\n",
    "\n",
    "# raster exenets to adjust map\n",
    "raster_extent_Afr = [grids['Afr'].extent[0], grids['Afr'].extent[1], grids['Afr'].extent[3], grids['Afr'].extent[2]]\n",
    "raster_extent_Afr_50 = [grids['Afr_50'].extent[0], grids['Afr_50'].extent[1], grids['Afr_50'].extent[3], grids['Afr_50'].extent[2]]\n",
    "raster_extent_World = [grids['World'].extent[0], grids['World'].extent[1], grids['World'].extent[3], grids['World'].extent[2]]\n",
    "\n",
    "# to correct plot maps\n",
    "raster_extents = {}\n",
    "\n",
    "raster_extents['Afr'] = raster_extent_Afr\n",
    "raster_extents['Afr_50'] = raster_extent_Afr_50\n",
    "raster_extents['World'] = raster_extent_World\n",
    "\n",
    "\n",
    "# list of latitudes and longitudes\n",
    "lon_dict = {}\n",
    "lat_dict = {}\n",
    "\n",
    "lon_dict['Afr'] = [afr_lon_min, afr_lon_max]\n",
    "lon_dict['Afr_50'] = [afr_lon_min, afr_lon_max]\n",
    "lon_dict['World'] = [world_lon_min, world_lon_max]\n",
    "\n",
    "lat_dict['Afr'] = [afr_lat_min, afr_lat_max]\n",
    "lat_dict['Afr_50'] = [afr_lat_min, afr_lat_max]\n",
    "lat_dict['World'] = [world_lat_min, world_lat_max]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7641c32",
   "metadata": {},
   "source": [
    "import datasets and transform them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85d5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before z: 10.7214\n",
      "before lon : -180.0\n",
      "after z: 0.10721399999999999\n",
      "after lon : -180.0\n",
      "#####\n",
      "before Z : 660.0\n",
      "lon : -25.0\n",
      "after Z : 0.10001935359009095\n",
      "#####\n",
      "before Z : 660.0\n",
      "lon : -25.0\n",
      "before Z : 2.48\n",
      "lon : 0.0\n",
      "after Z : 0.0248\n",
      "after lon : 0.0\n",
      "#####\n",
      "true 0.0\n",
      "before Z : 64.0\n",
      "lon : -44.0\n",
      "after Z : 64.0\n",
      "#####\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "# sv and pv calcualtion for africa only once\n",
    "\n",
    "\n",
    "seismic_w_sv_i_f = dir_p / 'data'/'dataset'/'Reference'/'d_Shear_Wave_Speed'/'SL2013sv_dVs_25km-0.5d_50+.xyz'\n",
    "seismic_rel_w_sv_xyz_o_f = dir_p / 'data'/'dataset'/'XYZ'/'d_Shear_Wave_Speed'/'SV_W_150.xyz'\n",
    "\n",
    "seismic_sv_i_f = dir_p / 'data'/'dataset'/'Reference'/'d_Shear_Wave_Speed'/'AF2019_vsvp.mod'\n",
    "seismic_rel_a_sv_xyz_o_f  = dir_p / 'data'/'dataset'/'XYZ'/'d_Shear_Wave_Speed'/'SV_A_150.xyz'\n",
    "seismic_a_sv_xyz_o_f  = dir_p / 'data'/'dataset'/'XYZ'/'d_Shear_Wave_Speed'/'SV_A_150_speed.xyz'\n",
    "\n",
    "\n",
    "seismic_w_pv_i_f = dir_p / 'data'/'dataset'/'Reference'/'e_Pressure_Wave_Speed'/'DETOX-P1.txt'\n",
    "seismic_rel_w_pv_xyz_o_f = dir_p / 'data'/'dataset'/'XYZ'/'e_Pressure_Wave_Speed'/'PV_W_150.xyz'\n",
    "\n",
    "\n",
    "seismic_a_pv_i_f = dir_p / 'data'/'dataset'/'Reference'/'e_Pressure_Wave_Speed'/'AFRP20_RF_CR1_MOD.txt'\n",
    "seismic_rel_a_pv_xyz_o_f = dir_p / 'data'/'dataset'/'XYZ'/'e_Pressure_Wave_Speed'/'PV_A_150.xyz'\n",
    "seismic_a_pv_xyz_o_f = dir_p / 'data'/'dataset'/'XYZ'/'e_Pressure_Wave_Speed'/'PV_A_150_speed.xyz'\n",
    "\n",
    "# Read geo data\n",
    "\n",
    "#is data -180 to 180\n",
    "\n",
    "# SV input world\n",
    "\n",
    "\n",
    "\n",
    "init_df = pd.read_csv(seismic_w_sv_i_f,header=None, skiprows=1, skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "print(f'before z: {init_df[5].max()}')\n",
    "print(f'before lon : {init_df[1].min()}')\n",
    "init_df[5] = init_df[5].multiply(0.01)\n",
    "print(f'after z: {init_df[5].max()}')\n",
    "print(f'after lon : {init_df[1].min()}')\n",
    "print('#####')\n",
    "\n",
    "seismic_w_sv_df_150_df = init_df[init_df[0]==150][[1,2,5]]\n",
    "#seismic_w_sv_df = copy.deepcopy(seismic_w_sv_df_150_df.iloc[:,[1,2,5]]\n",
    "\n",
    "\n",
    "seismic_w_sv_df_150_df.columns = ['lon', 'lat', 'dVs(%)']\n",
    "\n",
    "    \n",
    "seismic_w_sv_df_150_df['dVs(%)'] = round(seismic_w_sv_df_150_df['dVs(%)'],4)\n",
    "\n",
    "seismic_w_sv_df_150_df.replace(' ', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "seismic_w_sv_df_150_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "seismic_w_sv_df_150_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "\n",
    "seismic_w_sv_df_150_df.to_csv(seismic_rel_w_sv_xyz_o_f, header=True, \n",
    "                              index=False,  sep='\\t')\n",
    "\n",
    "\n",
    "#######\n",
    "# SV africa\n",
    "\n",
    "\n",
    "seismic_a_sv_df = pd.read_csv(seismic_sv_i_f   , skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'before Z : {seismic_a_sv_df.iloc[:, 2].max()}')\n",
    "print(f'lon : {seismic_a_sv_df.iloc[:, 0].min()}')\n",
    "\n",
    "seismic_a_sv_df[9] = seismic_a_sv_df.iloc[:,8] / seismic_a_sv_df.iloc[:,7]\n",
    "print(f'after Z : {seismic_a_sv_df.iloc[:,9] .max()}')\n",
    "print('#####')\n",
    "\n",
    "seismic_a_sv_final_df = seismic_a_sv_df[seismic_a_sv_df.iloc[:,2]==150].iloc[:,[0,1 , 9]]\n",
    "\n",
    "\n",
    "seismic_a_sv_final_df.columns = ['lon', 'lat', 'dVs(%)']\n",
    "\n",
    "\n",
    "    \n",
    "seismic_a_sv_final_df['dVs(%)'] = round(seismic_a_sv_final_df['dVs(%)'],4)    \n",
    "   \n",
    "seismic_a_sv_final_df.replace(' ', np.nan, inplace=True)\n",
    "seismic_a_sv_final_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "seismic_a_sv_final_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "seismic_a_sv_final_df.to_csv(seismic_rel_a_sv_xyz_o_f, index=False, header=True, sep='\\t')\n",
    "\n",
    "#######\n",
    "# SV africa speed\n",
    "\n",
    "\n",
    "seismic_a_sv_df = pd.read_csv(seismic_sv_i_f   , skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'before Z : {seismic_a_sv_df.iloc[:, 2].max()}')\n",
    "print(f'lon : {seismic_a_sv_df.iloc[:, 0].min()}')\n",
    "\n",
    "\n",
    "seismic_a_sv_final_df = seismic_a_sv_df[seismic_a_sv_df.iloc[:,2]==150].iloc[:,[0,1 , 6]]\n",
    "\n",
    "\n",
    "seismic_a_sv_final_df.columns = ['lon', 'lat', 'Vs(Km/s)']\n",
    "\n",
    "\n",
    "    \n",
    "seismic_a_sv_final_df['Vs(Km/s)'] = round(seismic_a_sv_final_df['Vs(Km/s)'],4)/1000   \n",
    "\n",
    "\n",
    "   \n",
    "seismic_a_sv_final_df.replace(' ', np.nan, inplace=True)\n",
    "seismic_a_sv_final_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "seismic_a_sv_final_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "seismic_a_sv_final_df.to_csv(seismic_a_sv_xyz_o_f, index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "#PV world\n",
    "\n",
    "seismic_w_pv_df = pd.read_csv(seismic_w_pv_i_f, skiprows=3,\n",
    "                              header=None, engine='python', skip_blank_lines=True,   na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f'before Z : {seismic_w_pv_df[2] .max()}')\n",
    "print(f'lon : {seismic_w_pv_df.iloc[:,0].min()}')\n",
    "seismic_w_pv_df[2] = seismic_w_pv_df.iloc[:,2].multiply(0.01)\n",
    "print(f'after Z : {seismic_w_pv_df.iloc[:,2] .max()}')\n",
    "print(f'after lon : { seismic_w_pv_df.iloc[:,0].min()}')\n",
    "print('#####')\n",
    "\n",
    "seismic_w_pv_df.replace(' ', np.nan, inplace=True)\n",
    "\n",
    "seismic_w_pv_df.columns = ['lon', 'lat', 'dVp(%)']\n",
    "\n",
    "\n",
    "if seismic_w_pv_df.iloc[:,0].min() >= 0 :\n",
    "    print(f'true {seismic_w_pv_df.iloc[:,0].min()}')\n",
    "    \n",
    "    mask = seismic_w_pv_df.iloc[:,0]> 180\n",
    "    seismic_w_pv_df.loc[mask,'lon'] -= 360\n",
    "    \n",
    "seismic_w_pv_df['dVp(%)'] = round(seismic_w_pv_df['dVp(%)'],4)\n",
    "\n",
    "\n",
    "seismic_w_pv_df_final = seismic_w_pv_df[['lon', 'lat', 'dVp(%)']]\n",
    "seismic_w_pv_df_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "seismic_w_pv_df_final.reset_index( inplace=True,drop=True)\n",
    "\n",
    "seismic_w_pv_df_final.to_csv(seismic_rel_w_pv_xyz_o_f, header=True, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "#PV africa\n",
    "\n",
    "seismic_a_pv_df = pd.read_csv(seismic_a_pv_i_f, header=None ,sep='\\s+',  \n",
    "                               engine='python', skip_blank_lines=True,   na_filter=True)\n",
    "\n",
    "seismic_a_pv_150_df = seismic_a_pv_df[seismic_a_pv_df.iloc[:,0]==150][[2,1,3]]\n",
    "#seismic_a_pv_150_copy_df = copy.deepcopy(seismic_a_pv_150_df.iloc[:, [2,1,3]])\n",
    "\n",
    "print(f'before Z : {seismic_a_pv_150_df[2] .max()}')\n",
    "print(f'lon : {seismic_a_pv_150_df[1] .min()}')\n",
    "seismic_a_pv_150_df[3] = seismic_a_pv_150_df[3].multiply(0.01)\n",
    "print(f'after Z : {seismic_a_pv_150_df[2] .max()}')\n",
    "print('#####')\n",
    "\n",
    "seismic_a_pv_150_df.columns =  ['lon', 'lat', 'dVp(%)']\n",
    "\n",
    "    \n",
    "seismic_a_pv_150_df['dVp(%)'] = round(seismic_a_pv_150_df['dVp(%)'],4)\n",
    "\n",
    "\n",
    "\n",
    "seismic_a_pv_150_df.replace(' ', np.nan, inplace=True)\n",
    "seismic_a_pv_150_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "seismic_a_pv_150_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "seismic_a_pv_150_df.to_csv(seismic_rel_a_pv_xyz_o_f, header=True, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "########\n",
    "\n",
    "\n",
    "#PV africa spped\n",
    "\n",
    "seismic_a_pv_df = pd.read_csv(seismic_a_pv_i_f, header=None ,sep='\\s+',  \n",
    "                               engine='python', skip_blank_lines=True,   na_filter=True)\n",
    "\n",
    "seismic_a_pv_150_df = seismic_a_pv_df[seismic_a_pv_df.iloc[:,0]==150][[2,1,4]]\n",
    "#seismic_a_pv_150_copy_df = copy.deepcopy(seismic_a_pv_150_df.iloc[:, [2,1,3]])\n",
    "\n",
    "\n",
    "seismic_a_pv_150_df.columns =  ['lon', 'lat', 'Vp(Km/s)']\n",
    "\n",
    "    \n",
    "seismic_a_pv_150_df['Vp(Km/s)'] = round(seismic_a_pv_150_df['Vp(Km/s)'],4)\n",
    "\n",
    "\n",
    "\n",
    "seismic_a_pv_150_df.replace(' ', np.nan, inplace=True)\n",
    "\n",
    "seismic_a_pv_150_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "seismic_a_pv_150_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "seismic_a_pv_150_df.to_csv(seismic_a_pv_xyz_o_f, header=True, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a150c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min z: -7.4117378\n",
      "min lon : -179.5\n",
      "max z: 160.59855\n",
      "max lon : 179.5\n",
      "min z: 1\n",
      "min lon : -180.0\n",
      "max z: 6\n",
      "max lon : 180.0\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CTD_Gard_Hasterok_f = dir_p / 'data'/'dataset'/'Reference'/'f_CTD'/'Gard&Hasterok2021.txt'\n",
    "CTD_Gard_Hasterok_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'f_CTD'/'CTD.xyz'\n",
    "\n",
    "\n",
    "REG_f = dir_p / 'data'/'dataset'/'Reference'/'o_Tectonic_Regionalization_Class'/'SL2013sv_Cluster_2d'\n",
    "REG_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'o_Tectonic_Regionalization_Class'/'REG.xyz'\n",
    "\n",
    "\n",
    "\n",
    "CTD_gh_df = pd.read_csv(CTD_Gard_Hasterok_f ,header=None,   delim_whitespace=True)\n",
    "print(f'min z: {CTD_gh_df.iloc[:,2].min()}')\n",
    "print(f'min lon : {CTD_gh_df.iloc[:,0].min()}')\n",
    "print(f'max z: {CTD_gh_df.iloc[:,2].max()}')\n",
    "print(f'max lon : {CTD_gh_df.iloc[:,0].max()}')\n",
    "CTD_gh_df.columns =  ['lon', 'lat', 'ctd']\n",
    "\n",
    "\n",
    "CTD_gh_df['ctd'] = round(CTD_gh_df['ctd'],4)\n",
    "\n",
    "CTD_gh_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "CTD_gh_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert CTD_gh_df['lon'].max() <= 180, f\" max lon {CTD_gh_df['lon'].max()}\"\n",
    "assert CTD_gh_df['lon'].min() >= -180, f\" min lon {CTD_gh_df['lon'].min()}\"\n",
    "assert CTD_gh_df['lat'].max() <= 90, f\" max lat {CTD_gh_df['lat'].max()}\"\n",
    "assert CTD_gh_df['lat'].min() >= -90, f\" min lat {CTD_gh_df['lon'].min()}\"\n",
    "\n",
    "CTD_gh_df .to_csv(CTD_Gard_Hasterok_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reg_df = pd.read_csv(REG_f ,header=None,   delim_whitespace=True)\n",
    "print(f'min z: {reg_df.iloc[:,2].min()}')\n",
    "print(f'min lon : {reg_df.iloc[:,0].min()}')\n",
    "print(f'max z: {reg_df.iloc[:,2].max()}')\n",
    "print(f'max lon : {reg_df.iloc[:,0].max()}')\n",
    "reg_df.columns =  ['lon', 'lat', 'classes']\n",
    "  \n",
    "\n",
    "\n",
    "reg_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "reg_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert reg_df['lon'].max() <= 180, f\" max lon {reg_df['lon'].max()}\"\n",
    "assert reg_df['lon'].min() >= -180, f\" min lon {reg_df['lon'].min()}\"\n",
    "assert reg_df['lat'].max() <= 90, f\" max lat {reg_df['lat'].max()}\"\n",
    "assert reg_df['lat'].min() >= -90, f\" min lat {reg_df['lon'].min()}\"\n",
    "\n",
    "reg_df.to_csv(REG_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a380163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min z: 6.9941\n",
      "min lon : 0.0\n",
      "max z: 70.9321\n",
      "max lon : 359.0\n",
      "true 0.0\n",
      "after lon min -179.0\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MOHO_new_global_f = dir_p / 'data'/'dataset'/'Reference'/'a_Moho_Depth'/'New_Global_Moho.DAT'\n",
    "MOHO_new_global_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'a_Moho_Depth'/'MOHO.xyz'\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "moho_ng_df = pd.read_csv(MOHO_new_global_f ,header=None, skiprows=1, engine='python', \n",
    "                         encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "print(f'min z: {moho_ng_df .iloc[:,2].min()}')\n",
    "print(f'min lon : {moho_ng_df .iloc[:,0].min()}')\n",
    "print(f'max z: {moho_ng_df .iloc[:,2].max()}')\n",
    "print(f'max lon : {moho_ng_df .iloc[:,0].max()}')\n",
    "moho_ng_df.columns =  ['lon', 'lat', 'Moho_depth']\n",
    "\n",
    "    \n",
    "if moho_ng_df.iloc[:,0].min() >= 0 :\n",
    "    print(f'true {moho_ng_df.iloc[:,0].min()}')\n",
    "    \n",
    "    mask = moho_ng_df.iloc[:,0]> 180\n",
    "    moho_ng_df.loc[mask,'lon'] -= 360\n",
    "    \n",
    "print(f'after lon min {moho_ng_df[\"lon\"].min()}')\n",
    "\n",
    "moho_ng_df['Moho_depth'] = round(moho_ng_df['Moho_depth'],4)\n",
    "\n",
    "\n",
    "moho_ng_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "moho_ng_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "\n",
    "assert moho_ng_df['lon'].max() <= 180, f\" max lon {moho_ng_df['lon'].max()}\"\n",
    "assert moho_ng_df['lon'].min() >= -180, f\" min lon {moho_ng_df['lon'].min()}\"\n",
    "assert moho_ng_df['lat'].max() <= 90, f\" max lat {moho_ng_df['lat'].max()}\"\n",
    "assert moho_ng_df['lat'].min() >= -90, f\" min lat {moho_ng_df['lon'].min()}\"\n",
    "moho_ng_df.to_csv(MOHO_new_global_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977b0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min z: -0.99939\n",
      "min lon : -180.0\n",
      "max lon : 949.0\n",
      "max z : 0.99932\n",
      "1618202\n",
      "Outlier max False    1618201\n",
      "True           1\n",
      "Name: lon, dtype: int64\n",
      "outlier min False    1618202\n",
      "Name: lon, dtype: int64\n",
      "max outlier vlaue              0        1         2         3         4      5         6\n",
      "1305745  949.0  0.00814  0.007882 -0.000396  0.012604  0.013  0.006091\n",
      "min outlier value Empty DataFrame\n",
      "Columns: [0, 1, 2, 3, 4, 5, 6]\n",
      "Index: []\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "curveture_f = dir_p / 'data' /'dataset'/ 'Reference'/'n_Shape_Index_Curvature' / 'GOCE_Curvature_Topo_Iso_Corr.txt'\n",
    "curveture_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'n_Shape_Index_Curvature' / 'SI.xyz'\n",
    "\n",
    "\n",
    "\n",
    "si_df = pd.read_csv(curveture_f ,header=None,  engine='python',skiprows=1,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "print(f'min z: {si_df .iloc[:,2].min()}')\n",
    "print(f'min lon : {si_df .iloc[:,0].min()}')\n",
    "print(f'max lon : {si_df .iloc[:,0].max()}')\n",
    "print(f'max z : {si_df .iloc[:,2].max()}')\n",
    "SI_final = si_df[[0,1,2]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SI_final.columns =  ['lon', 'lat', 'SI']\n",
    "if SI_final['lon'].min() >= 0 :\n",
    "    SI_final['lon'] = SI_final['lon'] - 180\n",
    "\n",
    "SI_final['SI'] = round(SI_final['SI'],4)\n",
    "\n",
    "mask_gt = SI_final['lon'] > 180\n",
    "mask_lt = SI_final['lon'] < -180\n",
    "\n",
    "mask_gt_d = si_df[0] > 180\n",
    "mask_lt_d = si_df[0] < -180\n",
    "\n",
    "\n",
    "print(len(SI_final))\n",
    "print(f'Outlier max {mask_gt.value_counts()}')\n",
    "print(f'outlier min {mask_lt.value_counts()}')\n",
    "#print(f'max outlier vlaue {SI_final.loc[SI_final[mask_gt].index,:]}')\n",
    "#print(f'min outlier value {SI_final.loc[SI_final[mask_lt].index,:]}')\n",
    "print(f'max outlier vlaue {si_df.loc[si_df[mask_gt_d].index,:]}')\n",
    "print(f'min outlier value {si_df.loc[si_df[mask_lt_d].index,:]}')\n",
    "\n",
    "SI_final.drop( SI_final[mask_gt].index , inplace=True)\n",
    "SI_final.drop( SI_final[mask_lt].index , inplace=True)\n",
    "\n",
    "assert SI_final['lon'].max() <= 180, f\" max lon {SI_final['lon'].max()}\"\n",
    "assert SI_final['lon'].min() >= -180, f\" min lon {SI_final['lon'].min()}\"\n",
    "assert SI_final['lat'].max() <= 90, f\" max lat {SI_final['lat'].max()}\"\n",
    "assert SI_final['lat'].min() >= -90, f\" min lat {SI_final['lon'].min()}\"\n",
    "\n",
    "SI_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "SI_final.reset_index( inplace=True,drop=True)\n",
    "\n",
    "SI_final.to_csv(curveture_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390c07a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min z: -6540.0\n",
      "min lon : -179.0\n",
      "max z: 5320.0\n",
      "max lon : 179.0\n",
      "min z: 45.0\n",
      "min lon : 0.0\n",
      "len: 12232\n",
      "max z: 300.0\n",
      "max lon : 359.0\n",
      "len: 12232\n",
      "true 0.0\n",
      "after -179.0\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LithoRef_f  = dir_p / 'data' /'dataset'/ 'Reference'/'b_LAB' / 'LithoRef18.xyz'\n",
    "\n",
    "rho_c_xyz_o =  dir_p / 'data' /'dataset'/ 'XYZ'/'j_Crustal_Average_Density'/'RHO_C.xyz'\n",
    "rho_l_xyz_o =  dir_p / 'data' /'dataset'/ 'XYZ'/'i_Lithosphere_Average_Density'/'RHO_L.xyz'\n",
    "\n",
    "LAB_NG_f =  dir_p / 'data' /'dataset'/ 'Reference'/'b_LAB'/'New_Global_LAB.DAT'\n",
    "LAB_NG_xyz_o =  dir_p / 'data' /'dataset'/ 'XYZ'/'b_LAB'/'LAB.xyz'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lith_df = pd.read_csv(LithoRef_f ,header=None,  engine='python',skiprows=9,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "print(f'min z: {lith_df .iloc[:,2].min()}')\n",
    "print(f'min lon : {lith_df .iloc[:,0].min()}')\n",
    "print(f'max z: {lith_df.iloc[:,2].max()}')\n",
    "print(f'max lon : {lith_df .iloc[:,0].max()}')\n",
    "\n",
    "lab_final = lith_df.iloc[:,[0,1,4]]\n",
    "lab_final.columns = ['lon', 'lat', 'LAB_depth']\n",
    "\n",
    "\n",
    "\n",
    "lab_final['LAB_depth'] = round(lab_final['LAB_depth'],4)\n",
    "\n",
    "lab_final['LAB_depth'] = lab_final['LAB_depth']/(-km)\n",
    "lab_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "lab_final.reset_index( inplace=True,drop=True)\n",
    "\n",
    "rho_c_final = lith_df.iloc[:,[0,1,5]]\n",
    "rho_c_final.columns = ['lon', 'lat', 'RHO_C']\n",
    "\n",
    "\n",
    "\n",
    "rho_c_final['RHO_C'] = round(rho_c_final['RHO_C'],4)\n",
    "\n",
    "rho_c_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "rho_c_final.reset_index( inplace=True,drop=True)\n",
    "\n",
    "rho_l_final = lith_df.iloc[:,[0,1,6]]\n",
    "rho_l_final.columns = ['lon', 'lat', 'RHO_L']\n",
    "\n",
    "\n",
    "rho_l_final['RHO_L'] = round(rho_l_final['RHO_L'],4)\n",
    "\n",
    "rho_l_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "rho_l_final.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert lab_final['lon'].max() <= 180, f\" max lon {lab_final['lon'].max()}\"\n",
    "assert lab_final['lon'].min() >= -180, f\" min lon {lab_final['lon'].min()}\"\n",
    "assert lab_final['lat'].max() <= 90, f\" max lat {lab_final['lat'].max()}\"\n",
    "assert lab_final['lat'].min() >= -90, f\" min lat {lab_final['lon'].min()}\"\n",
    "\n",
    "assert rho_c_final['lon'].max() <= 180, f\" max lon {rho_c_final['lon'].max()}\"\n",
    "assert rho_c_final['lon'].min() >= -180, f\" min lon {rho_c_final['lon'].min()}\"\n",
    "assert rho_c_final['lat'].max() <= 90, f\" max lat {rho_c_final['lat'].max()}\"\n",
    "assert rho_c_final['lat'].min() >= -90, f\" min lat {rho_c_final['lon'].min()}\"\n",
    "\n",
    "assert rho_l_final['lon'].max() <= 180, f\" max lon {rho_l_final['lon'].max()}\"\n",
    "assert rho_l_final['lon'].min() >= -180, f\" min lon {rho_l_final['lon'].min()}\"\n",
    "assert rho_l_final['lat'].max() <= 90, f\" max lat {rho_l_final['lat'].max()}\"\n",
    "assert rho_l_final['lat'].min() >= -90, f\" min lat {rho_l_final['lon'].min()}\"\n",
    "\n",
    "\n",
    "rho_c_final.to_csv(rho_c_xyz_o , index=False, header=True, sep='\\t')\n",
    "rho_l_final.to_csv(rho_l_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "LAB_NG_df = pd.read_csv(LAB_NG_f ,header=None,  engine='python',skiprows=1,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "print(f'min z: {LAB_NG_df .iloc[:,2].min()}')\n",
    "print(f'min lon : {LAB_NG_df .iloc[:,0].min()}')\n",
    "print(f'len: {len(LAB_NG_df)}')\n",
    "print(f'max z: {LAB_NG_df .iloc[:,2].max()}')\n",
    "print(f'max lon : {LAB_NG_df .iloc[:,0].max()}')\n",
    "print(f'len: {len(LAB_NG_df)}')\n",
    "LAB_NG_df.columns = ['lon', 'lat', 'LAB_depth']\n",
    "\n",
    "if LAB_NG_df.iloc[:,0].min() >= 0 :\n",
    "    print(f'true {LAB_NG_df.iloc[:,0].min()}')\n",
    "    \n",
    "    mask = LAB_NG_df.iloc[:,0]> 180\n",
    "    LAB_NG_df.loc[mask,'lon'] -= 360\n",
    "    \n",
    "    \n",
    "print(f'after {LAB_NG_df[\"lon\"].min()}')\n",
    "\n",
    "LAB_NG_df['LAB_depth'] = round(LAB_NG_df['LAB_depth'],4)\n",
    "\n",
    "LAB_NG_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "LAB_NG_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert LAB_NG_df['lon'].max() <= 180, f\" max lon {LAB_NG_df['lon'].max()}\"\n",
    "assert LAB_NG_df['lon'].min() >= -180, f\" min lon {LAB_NG_df['lon'].min()}\"\n",
    "assert LAB_NG_df['lat'].max() <= 90, f\" max lat {LAB_NG_df['lat'].max()}\"\n",
    "assert LAB_NG_df['lat'].min() >= -90, f\" min lat {LAB_NG_df['lon'].min()}\"\n",
    "\n",
    "LAB_NG_df.to_csv(LAB_NG_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1dedc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min z: -106.184427114513\n",
      "min lon : -180.0\n",
      "max z: 84.917290827278\n",
      "max lon : 180.0\n",
      "min z: -301.300980351904\n",
      "min lon : -180.0\n",
      "max z: 522.25583386214\n",
      "max lon : 180.0\n",
      "min z: -631.218235051554\n",
      "min lon : -180.0\n",
      "max z: -631.218235051554\n",
      "max lon : 180.0\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "geoid_f = dir_p / 'data' /'dataset'/ 'Reference'/'l_geoid_height' / 'EIGEN-6C4_geoid.gdf'\n",
    "geoid_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'l_geoid_height' / 'EIGEN-6C4_geoid.xyz'\n",
    "\n",
    "FA_f = dir_p / 'data' /'dataset'/ 'Reference'/'k_Free_Air' / 'EIGEN-6C4_gravity_anomaly_cl.gdf'\n",
    "FA_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'k_Free_Air' / 'EIGEN-6C4_gravity_anomaly_cl.xyz'\n",
    "\n",
    "bg_f = dir_p / 'data' /'dataset'/ 'Reference'/'m_Bouguer' / 'EIGEN-6C4_gravity_anomaly_bg.gdf'\n",
    "bg_xyz_o = dir_p / 'data' /'dataset'/ 'XYZ'/'m_Bouguer' / 'EIGEN-6C4_gravity_anomaly_bg.xyz'\n",
    "\n",
    "\n",
    "\n",
    "geoid_df = pd.read_csv(geoid_f ,header=None,  engine='python',skiprows=36,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "print(f'min z: {geoid_df.iloc[:,2].min()}')\n",
    "print(f'min lon : {geoid_df.iloc[:,0].min()}')\n",
    "print(f'max z: {geoid_df.iloc[:,2].max()}')\n",
    "print(f'max lon : {geoid_df.iloc[:,0].max()}')\n",
    "\n",
    "geoid_df.columns = ['lon', 'lat', 'mgal']\n",
    "\n",
    "\n",
    "geoid_df = geoid_df.round(4)\n",
    "\n",
    "\n",
    "geoid_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "geoid_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert geoid_df['lon'].max() <= 180, f\" max lon {geoid_df['lon'].max()}\"\n",
    "assert geoid_df['lon'].min() >= -180, f\" min lon {geoid_df['lon'].min()}\"\n",
    "assert geoid_df['lat'].max() <= 90, f\" max lat {geoid_df['lat'].max()}\"\n",
    "assert geoid_df['lat'].min() >= -90, f\" min lat {geoid_df['lon'].min()}\"\n",
    "\n",
    "geoid_df.to_csv(geoid_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "#####\n",
    "FA_df = pd.read_csv(FA_f ,header=None,  engine='python',skiprows=36,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'min z: {FA_df .iloc[:,2].min()}')\n",
    "print(f'min lon : {FA_df .iloc[:,0].min()}')\n",
    "print(f'max z: {FA_df .iloc[:,2].max()}')\n",
    "print(f'max lon : {FA_df .iloc[:,0].max()}')\n",
    "\n",
    "FA_df.columns = ['lon', 'lat', 'mgal']\n",
    "\n",
    "FA_df = FA_df.round(4)\n",
    "\n",
    "\n",
    "FA_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "FA_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert FA_df['lon'].max() <= 180, f\" max lon {FA_df['lon'].max()}\"\n",
    "assert FA_df['lon'].min() >= -180, f\" min lon {FA_df['lon'].min()}\"\n",
    "assert FA_df['lat'].max() <= 90, f\" max lat {FA_df['lat'].max()}\"\n",
    "assert FA_df['lat'].min() >= -90, f\" min lat {FA_df['lon'].min()}\"\n",
    "\n",
    "FA_df.to_csv(FA_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "bg_df = pd.read_csv(bg_f ,header=None,  engine='python',skiprows=37,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'min z: {bg_df.iloc[:,2].min()}')\n",
    "print(f'min lon : {bg_df.iloc[:,0].min()}')\n",
    "print(f'max z: {bg_df.iloc[:,2].min()}')\n",
    "print(f'max lon : {bg_df.iloc[:,0].max()}')\n",
    "\n",
    "bg_df.columns = ['lon', 'lat', 'mgal']\n",
    "\n",
    "\n",
    "bg_df = bg_df.round(4)\n",
    "\n",
    "\n",
    "bg_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "bg_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert bg_df['lon'].max() <= 180, f\" max lon {bg_df['lon'].max()}\"\n",
    "assert bg_df['lon'].min() >= -180, f\" min lon {bg_df['lon'].min()}\"\n",
    "assert bg_df['lat'].max() <= 90, f\" max lat {bg_df['lat'].max()}\"\n",
    "assert bg_df['lat'].min() >= -90, f\" min lat {bg_df['lon'].min()}\"\n",
    "\n",
    "bg_df.to_csv(bg_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e8e38ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min z: 1\n",
      "min lon : -180.0\n",
      "max z: 180.0\n",
      "max lon : 6\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "REG_f = dir_p / 'data'/'dataset'/'Reference'/'o_Tectonic_Regionalization_Class'/'SL2013sv_Cluster_2d'\n",
    "REG_o = dir_p / 'data'/'dataset'/'Reference'/'o_Tectonic_Regionalization_Class'/'REG.xyz'\n",
    "REG_xyz_o = dir_p / 'data'/'dataset'/'XYZ'/'o_Tectonic_Regionalization_Class'/'REG.xyz'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reg_df = pd.read_csv(REG_f ,header=None,   delim_whitespace=True)\n",
    "print(f'min z: {reg_df.iloc[:,2].min()}')\n",
    "print(f'min lon : {reg_df.iloc[:,0].min()}')\n",
    "print(f'max z: {reg_df.iloc[:,0].max()}')\n",
    "print(f'max lon : {reg_df.iloc[:,2].max()}')\n",
    "reg_df.columns =  ['lon', 'lat', 'classes']\n",
    "\n",
    "\n",
    "\n",
    "if reg_df['lon'].min() >= 0 :\n",
    "    reg_df['lon'] = reg_df['lon'] - 180\n",
    "    \n",
    "\n",
    "\n",
    "reg_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "reg_df.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert reg_df['lon'].max() <= 180, f\" max lon {reg_df['lon'].max()}\"\n",
    "assert reg_df['lon'].min() >= -180, f\" min lon {reg_df['lon'].min()}\"\n",
    "assert reg_df['lat'].max() <= 90, f\" max lat {reg_df['lat'].max()}\"\n",
    "assert reg_df['lat'].min() >= -90, f\" min lat {reg_df['lon'].min()}\"\n",
    "\n",
    "reg_df.to_csv(REG_o , index=False, header=True, sep='\\t')\n",
    "reg_df.to_csv(REG_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4457134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69729\n",
      "69377\n",
      "12707\n",
      "5792\n",
      "12707\n",
      "12707\n",
      "12707\n",
      "5792\n",
      "12707\n",
      "35647\n",
      "46113\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "#create two datsets for A ratibng and b ratings\n",
    "HGHF_file = dir_p / 'data' / 'dataset'/ 'Reference'/ 'q_Heat_flow'/'NGHF.csv'\n",
    "HGHF_xyz_o_ra = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_ra.xyz'\n",
    "HGHF_xyz_o_rab = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_rab.xyz'\n",
    "HGHF_xyz_o_rabc = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_rabc.xyz'\n",
    "HGHF_xyz_o_rabcd = dir_p / 'data' / 'dataset'/ 'XYZ'/ 'q_Heat_flow'/'NGHF_rabcd.xyz'\n",
    "\n",
    "\n",
    "hf = pd.read_csv(HGHF_file)\n",
    "# change heat flow to mili\n",
    "\n",
    "elev_cut = -1000\n",
    "\n",
    "\n",
    "\n",
    "recorabcd_total = pd.read_csv(HGHF_file)\n",
    "\n",
    "print(len(recorabcd_total))\n",
    "hf_clean = recorabcd_total.dropna(subset = ['longitude', 'latitude', 'heat-flow (mW/m2)'])\n",
    "print(len(hf_clean))\n",
    "\n",
    "hf_no_pole = hf_clean[hf_clean['latitude'].between(world_lat_min, world_lat_max, inclusive='both')]\n",
    "#hf_final = hf_no_pole[(hf_no_pole ['code6']=='A') & (hf_no_pole ['elevation (m)']>elev_cut)][['longitude', 'latitude','heat-flow (mW/m2)']]\n",
    "\n",
    "hf_final_a = hf_no_pole[(hf_no_pole ['code6']=='A') & (hf_no_pole ['elevation (m)']>elev_cut)][['longitude', 'latitude','heat-flow (mW/m2)']]\n",
    "hf_final_b = hf_no_pole[(hf_no_pole ['code6']=='B') & (hf_no_pole ['elevation (m)']>elev_cut)][['longitude', 'latitude','heat-flow (mW/m2)']]\n",
    "hf_final = pd.concat([hf_final_a, hf_final_b])\n",
    "\n",
    "hf_final_c  = hf_no_pole[(hf_no_pole ['code6']=='C') & (hf_no_pole ['elevation (m)']>elev_cut)][['longitude', 'latitude','heat-flow (mW/m2)']]\n",
    "hf_final_c = pd.concat([hf_final_a, hf_final_b, hf_final_c])\n",
    "\n",
    "\n",
    "hf_final_d = hf_no_pole[(hf_no_pole ['elevation (m)']>elev_cut)][['longitude', 'latitude','heat-flow (mW/m2)']]\n",
    "\n",
    "\n",
    "######A rating\n",
    "\n",
    "hf_final_a.columns = ['lon', 'lat', 'heat-flow (mW/m2)']\n",
    "\n",
    "\n",
    "hf_final_a = hf_final_a.round(3)\n",
    "\n",
    "hf_final_a.replace(' ', np.nan, inplace=True)\n",
    "hf_final_a.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hf_final_a.reset_index( inplace=True,drop=True)\n",
    "\n",
    "assert hf_final_a['lon'].max() <= 180, f\" max lon {hf_final_a['lon'].max()}\"\n",
    "assert hf_final_a['lon'].min() >= -180, f\" min lon {hf_final_a['lon'].min()}\"\n",
    "assert hf_final_a['lat'].max() <= 90, f\" max lat {hf_final_a['lat'].max()}\"\n",
    "assert hf_final_a['lat'].min() >= -90, f\" min lat {hf_final_a['lon'].min()}\"\n",
    "\n",
    "hf_final_a.to_csv(HGHF_xyz_o_ra , index=False, header=True, sep='\\t')\n",
    "\n",
    "###### A+ B rating\n",
    "\n",
    "hf_final.columns = ['lon', 'lat', 'heat-flow (mW/m2)']\n",
    "\n",
    "\n",
    "hf_final = hf_final.round(3)\n",
    "\n",
    "hf_final.replace(' ', np.nan, inplace=True)\n",
    "hf_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hf_final.reset_index( inplace=True,drop=True)\n",
    "print(len(hf_final))\n",
    "\n",
    "assert hf_final['lon'].max() <= 180, f\" max lon {hf_final['lon'].max()}\"\n",
    "assert hf_final['lon'].min() >= -180, f\" min lon {hf_final['lon'].min()}\"\n",
    "assert hf_final['lat'].max() <= 90, f\" max lat {hf_final['lat'].max()}\"\n",
    "assert hf_final['lat'].min() >= -90, f\" min lat {hf_final['lon'].min()}\"\n",
    "\n",
    "hf_final.to_csv(HGHF_xyz_o_rab , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "print(len(hf_final_a))\n",
    "print(len(hf_final))\n",
    "\n",
    "\n",
    "###### A+ B + C rating\n",
    "\n",
    "hf_final_c.columns = ['lon', 'lat', 'heat-flow (mW/m2)']\n",
    "\n",
    "\n",
    "hf_final_c = hf_final_c.round(3)\n",
    "\n",
    "hf_final_c.replace(' ', np.nan, inplace=True)\n",
    "hf_final_c.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hf_final_c.reset_index( inplace=True,drop=True)\n",
    "print(len(hf_final))\n",
    "\n",
    "assert hf_final_c['lon'].max() <= 180, f\" max lon {hf_final_c['lon'].max()}\"\n",
    "assert hf_final_c['lon'].min() >= -180, f\" min lon {hf_final_c['lon'].min()}\"\n",
    "assert hf_final_c['lat'].max() <= 90, f\" max lat {hf_final_c['lat'].max()}\"\n",
    "assert hf_final_c['lat'].min() >= -90, f\" min lat {hf_final_c['lon'].min()}\"\n",
    "\n",
    "hf_final_c.to_csv(HGHF_xyz_o_rabc , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "###### A+ B + C rating and elevation\n",
    "\n",
    "hf_final_d.columns = ['lon', 'lat', 'heat-flow (mW/m2)']\n",
    "\n",
    "\n",
    "hf_final_d = hf_final_d.round(3)\n",
    "\n",
    "hf_final_d.replace(' ', np.nan, inplace=True)\n",
    "hf_final_d.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hf_final_d.reset_index( inplace=True,drop=True)\n",
    "print(len(hf_final))\n",
    "\n",
    "assert hf_final_d['lon'].max() <= 180, f\" max lon {hf_final_d['lon'].max()}\"\n",
    "assert hf_final_d['lon'].min() >= -180, f\" min lon {hf_final_d['lon'].min()}\"\n",
    "assert hf_final_d['lat'].max() <= 90, f\" max lat {hf_final_d['lat'].max()}\"\n",
    "assert hf_final_d['lat'].min() >= -90, f\" min lat {hf_final_d['lon'].min()}\"\n",
    "\n",
    "hf_final_d.to_csv(HGHF_xyz_o_rabcd , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "print(len(hf_final_a))\n",
    "print(len(hf_final))\n",
    "\n",
    "print(len(hf_final_c))\n",
    "print(len(hf_final_d))\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bdb264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Latitude  Longitude\n",
      "0       50.170      6.850\n",
      "1       45.775      2.970\n",
      "2       42.170      2.530\n",
      "3       38.870     -4.020\n",
      "4       42.600     11.930\n",
      "...        ...        ...\n",
      "1249    -2.228    101.430\n",
      "1250    -2.418    101.776\n",
      "1251    -2.480    102.026\n",
      "1252    -8.416    118.207\n",
      "1253   -78.374    165.015\n",
      "\n",
      "[2652 rows x 2 columns]\n",
      "1398\n",
      "1254\n",
      "2652\n",
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "#volc_h_f =  r'./data/dataset/Reference/r_Distance_to_nearest_volcano/GVP_Volcano_List_Holocene.xlsx'\n",
    "#volc_p_f = r'./data/dataset/Reference/r_Distance_to_nearest_volcano/GVP_Volcano_List_Pleistocene.xlsx'\n",
    "#volc_xyz_o = f'./data/dataset/XYZ/r_Distance_to_nearest_volcano/volcanos.xyz'\n",
    "\n",
    "    \n",
    "volc_h_f =   dir_p / 'data' / 'dataset'/ 'Reference'/'c_Distance_to_nearest_volcano'/'GVP_Volcano_List_Holocene.xlsx'\n",
    "volc_p_f =  dir_p / 'data' / 'dataset'/ 'Reference'/'c_Distance_to_nearest_volcano'/'GVP_Volcano_List_Pleistocene.xlsx'\n",
    "volc_xyz_o =  dir_p / 'data' / 'dataset'/'XYZ'/'c_Distance_to_nearest_volcano'/'volcanos.xyz'\n",
    "\n",
    "volc_h = pd.read_excel(volc_h_f, header=1)\n",
    "volc_p = pd.read_excel(volc_p_f, header=1)\n",
    "volc_df = pd.concat([volc_h, volc_p])\n",
    "\n",
    "print(volc_df[['Latitude','Longitude']])\n",
    "cols = volc_df.columns.to_list()\n",
    "cols[1], cols[0] = cols[0], cols[1]\n",
    "volc_df.loc[:,cols]\n",
    "\n",
    "print(len(volc_h))\n",
    "print(len(volc_p))\n",
    "print(len(volc_df))\n",
    "\n",
    "volc_final = volc_df[['Latitude','Longitude']]\n",
    "volc_final.columns = [ 'lat', 'lon']\n",
    "columns_titles = [ 'lon', 'lat', ]\n",
    "\n",
    "\n",
    "\n",
    "volc_final = volc_final.round(2)\n",
    "volc_final = volc_final.reindex(columns=columns_titles)\n",
    "\n",
    "volc_final.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "volc_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "assert volc_final['lon'].max() <= 180, f\" max lon {volc_final['lon'].max()}\"\n",
    "assert volc_final['lon'].min() >= -180, f\" min lon {volc_final['lon'].min()}\"\n",
    "assert volc_final['lat'].max() <= 90, f\" max lat {volc_final['lat'].max()}\"\n",
    "assert volc_final['lat'].min() >= -90, f\" min lat {volc_final['lon'].min()}\"\n",
    "\n",
    "volc_final.to_csv(volc_xyz_o , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad97066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
