{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439b1091",
   "metadata": {},
   "source": [
    "# 1-Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536a619",
   "metadata": {},
   "source": [
    "import necesaary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0841861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys, pickle, copy, pygmt, pooch, shutil\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import spatial\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import pyproj as proj\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "import AFQ_utils as utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "crs_to='epsg:4326'\n",
    "crs_from='epsg:4326'\n",
    "projection = 'M5.4i'\n",
    "#parent directory\n",
    "\n",
    "DIR = Path().resolve() \n",
    "\n",
    "#constants\n",
    "km = 1000\n",
    "\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "region_afr = [afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max]\n",
    "region_world = [world_lon_min, world_lon_max, world_lat_min, world_lat_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fd6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create necessray folders\n",
    "\n",
    "path = os.path.join(DIR, 'Fig')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = os.path.join(DIR, 'Grids')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = os.path.join(DIR, 'Grids', 'Inputs')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = os.path.join(DIR, 'Grids', 'Outputs')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = os.path.join(DIR, 'Hyperparameters')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "path = os.path.join(DIR, 'KPI')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "path = os.path.join(DIR, 'Dataset')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = os.path.join(DIR, 'Dataset', 'Preprocessed')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = os.path.join(DIR, 'Dataset', 'References')\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "references = ['a_Moho_Depth', 'b_LAB', 'c_Distance_to_Nearest_Volcano', 'd_SV', 'e_PV',\n",
    "              'f_CTD', 'g_Magnetic', 'h_DEM', 'i_Crustal_Rho', 'i_Lithospheric_Rho', 'k_Free_Air',\n",
    "              'l_Geoid', 'm_Bouguer','n_Shape_Index', 'o_Tectonic_Regionalization', \n",
    "              'p_GLiM', 'q_Heat_Flow']\n",
    "\n",
    "for reference in references :\n",
    "    path = os.path.join(DIR, 'Dataset', 'References', reference)\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4182eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBS_AFR</th>\n",
       "      <th>LABELS_gmt</th>\n",
       "      <th>LABELS</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>UNITS_gmt</th>\n",
       "      <th>V_RANGE</th>\n",
       "      <th>V_RANGE_AFR</th>\n",
       "      <th>CMAPS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBS_REF</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CTD</th>\n",
       "      <td>CTD</td>\n",
       "      <td>CTD</td>\n",
       "      <td>CTD</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(0, 50)</td>\n",
       "      <td>(0, 50)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI</th>\n",
       "      <td>SI</td>\n",
       "      <td>Shape index</td>\n",
       "      <td>Shape index</td>\n",
       "      <td>si</td>\n",
       "      <td>si</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAB</th>\n",
       "      <td>LAB</td>\n",
       "      <td>LAB</td>\n",
       "      <td>LAB</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(0, 300)</td>\n",
       "      <td>(50, 250)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOHO</th>\n",
       "      <td>MOHO</td>\n",
       "      <td>Moho</td>\n",
       "      <td>Moho</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(15, 60)</td>\n",
       "      <td>(20, 50)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SV</th>\n",
       "      <td>SV_SPEED</td>\n",
       "      <td>S@_v@ 150km</td>\n",
       "      <td>$S_v$ @150km</td>\n",
       "      <td>$\\delta$$S_v$ %</td>\n",
       "      <td>km/s</td>\n",
       "      <td>(-0.075, 0.075)</td>\n",
       "      <td>(-0.075, 0.075)</td>\n",
       "      <td>SCM/roma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PV</th>\n",
       "      <td>PV_SPEED</td>\n",
       "      <td>P@_v@ 150km</td>\n",
       "      <td>$P_v$ @150km</td>\n",
       "      <td>$\\delta$$P_v$ %</td>\n",
       "      <td>km/s</td>\n",
       "      <td>(-0.02, 0.02)</td>\n",
       "      <td>(-0.02, 0.02)</td>\n",
       "      <td>SCM/roma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <td>GEOID</td>\n",
       "      <td>Geoid</td>\n",
       "      <td>Geoid</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>(-45, 45)</td>\n",
       "      <td>(-45, 45)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEM</th>\n",
       "      <td>DEM</td>\n",
       "      <td>DEM</td>\n",
       "      <td>DEM</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>(-2200, 2200)</td>\n",
       "      <td>(-2200, 2200)</td>\n",
       "      <td>SCM/oleron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FA</th>\n",
       "      <td>FA</td>\n",
       "      <td>Free air</td>\n",
       "      <td>Free air</td>\n",
       "      <td>mGal</td>\n",
       "      <td>mGal</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BG</th>\n",
       "      <td>BG</td>\n",
       "      <td>Bouguer</td>\n",
       "      <td>Bouguer</td>\n",
       "      <td>mGal</td>\n",
       "      <td>mGal</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMAG2_CLASS</th>\n",
       "      <td>EMAG2</td>\n",
       "      <td>Mag.</td>\n",
       "      <td>Mag.</td>\n",
       "      <td>f(nT)</td>\n",
       "      <td>f(nT)</td>\n",
       "      <td>(-0.4, 0.4)</td>\n",
       "      <td>(-200, 200)</td>\n",
       "      <td>SCM/bilbao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RHO_L</th>\n",
       "      <td>RHO_L</td>\n",
       "      <td>Lith. rho</td>\n",
       "      <td>Lith. ρ</td>\n",
       "      <td>kg/m$^3$</td>\n",
       "      <td>kg/m@+3@+</td>\n",
       "      <td>(3260, 3360)</td>\n",
       "      <td>(3260, 3360)</td>\n",
       "      <td>SCM/batlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RHO_C</th>\n",
       "      <td>RHO_C</td>\n",
       "      <td>Crust rho</td>\n",
       "      <td>Crust ρ</td>\n",
       "      <td>kg/m$^3$</td>\n",
       "      <td>kg/m@+3@+</td>\n",
       "      <td>(2650, 2950)</td>\n",
       "      <td>(2650, 2950)</td>\n",
       "      <td>SCM/batlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLC_DIST_W</th>\n",
       "      <td>VOLC_DIST</td>\n",
       "      <td>Volcano</td>\n",
       "      <td>Volcano</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 100)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REG</th>\n",
       "      <td>REG</td>\n",
       "      <td>REG</td>\n",
       "      <td>REG</td>\n",
       "      <td>class</td>\n",
       "      <td>class</td>\n",
       "      <td>(1, 6)</td>\n",
       "      <td>(1, 6)</td>\n",
       "      <td>gmt/categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLIM</th>\n",
       "      <td>GLIM</td>\n",
       "      <td>GliM</td>\n",
       "      <td>GliM</td>\n",
       "      <td>class</td>\n",
       "      <td>class</td>\n",
       "      <td>(1, 16)</td>\n",
       "      <td>(1, 15)</td>\n",
       "      <td>gmt/categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               OBS_AFR   LABELS_gmt        LABELS            UNITS  UNITS_gmt          V_RANGE      V_RANGE_AFR            CMAPS\n",
       "OBS_REF                                                                                                                         \n",
       "CTD                CTD          CTD           CTD               km         km          (0, 50)          (0, 50)       SCM/bamako\n",
       "SI                  SI  Shape index   Shape index               si         si          (-1, 1)          (-1, 1)         SCM/broc\n",
       "LAB                LAB          LAB           LAB               km         km         (0, 300)        (50, 250)       SCM/bamako\n",
       "MOHO              MOHO         Moho          Moho               km         km         (15, 60)         (20, 50)       SCM/bamako\n",
       "SV            SV_SPEED  S@_v@ 150km  $S_v$ @150km  $\\delta$$S_v$ %       km/s  (-0.075, 0.075)  (-0.075, 0.075)         SCM/roma\n",
       "PV            PV_SPEED  P@_v@ 150km  $P_v$ @150km  $\\delta$$P_v$ %       km/s    (-0.02, 0.02)    (-0.02, 0.02)         SCM/roma\n",
       "GEOID            GEOID        Geoid         Geoid                m          m        (-45, 45)        (-45, 45)       SCM/bamako\n",
       "DEM                DEM          DEM           DEM                m          m    (-2200, 2200)    (-2200, 2200)       SCM/oleron\n",
       "FA                  FA     Free air      Free air             mGal       mGal      (-100, 100)      (-100, 100)         SCM/broc\n",
       "BG                  BG      Bouguer       Bouguer             mGal       mGal      (-100, 100)      (-100, 100)         SCM/broc\n",
       "EMAG2_CLASS      EMAG2         Mag.          Mag.            f(nT)      f(nT)      (-0.4, 0.4)      (-200, 200)       SCM/bilbao\n",
       "RHO_L            RHO_L    Lith. rho       Lith. ρ         kg/m$^3$  kg/m@+3@+     (3260, 3360)     (3260, 3360)       SCM/batlow\n",
       "RHO_C            RHO_C    Crust rho       Crust ρ         kg/m$^3$  kg/m@+3@+     (2650, 2950)     (2650, 2950)       SCM/batlow\n",
       "VOLC_DIST_W  VOLC_DIST      Volcano       Volcano               km         km           (0, 1)         (0, 100)         SCM/broc\n",
       "REG                REG          REG           REG            class      class           (1, 6)           (1, 6)  gmt/categorical\n",
       "GLIM              GLIM         GliM          GliM            class      class          (1, 16)          (1, 15)  gmt/categorical"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "obs[\"OBS_REF\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2_CLASS\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST_W\", \"REG\", \"GLIM\"]\n",
    "\n",
    "obs[\"OBS_AFR\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV_SPEED\",\"PV_SPEED\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST\", \"REG\", \"GLIM\"]\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "\n",
    "\n",
    "# Labels for plots etc\n",
    "obs[\"LABELS_gmt\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"S@_v@ 150km\", \"P@_v@ 150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. rho\", \"Crust rho\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]  \n",
    "\n",
    "\n",
    "obs[\"LABELS\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"$S_v$ @150km\", \"$P_v$ @150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. ρ\", \"Crust ρ\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]\n",
    "    \n",
    "# \"vp/vs\"\n",
    "# Units to display in plots etc\n",
    "obs[\"UNITS\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"$\\delta$$S_v$ %\",\"$\\delta$$P_v$ %\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m$^3$\", \"kg/m$^3$\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "\n",
    "\n",
    "\n",
    "obs[\"UNITS_gmt\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"km/s\",\"km/s\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m@+3@+\", \"kg/m@+3@+\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs[\"V_RANGE\"] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,16),]\n",
    "\n",
    "\n",
    "    \n",
    "obs[\"V_RANGE_AFR\"] = [(0,50), (-1,1),(50,250),(20,50),\n",
    "          (-0.075,0.075), (-0.02,0.02), \n",
    "          (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-200, 200), \n",
    "               (3260, 3360), (2650, 2950),\n",
    "              (0,100), (1,6),(1,15),]\n",
    "\n",
    "\n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"categorical\", ]\n",
    "\n",
    "obs[\"CMAPS\"] = [\"SCM/bamako\",  \"SCM/broc\", \"SCM/bamako\", \"SCM/bamako\", \n",
    "             \"SCM/roma\",\"SCM/roma\", \n",
    "             \"SCM/bamako\", \"SCM/broc\", \"SCM/oleron\", \"SCM/broc\", \"SCM/bilbao\",            \n",
    "                \"SCM/batlow\", \"SCM/batlow\",\n",
    "               \"SCM/broc\",  \"gmt/categorical\",\"gmt/categorical\", ]\n",
    "\n",
    "new_index = [0,1,2,3,4,5,6,8,7,9,10,11,12,13,14,15]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "obs = obs.reindex(new_index)\n",
    "\n",
    "#obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n",
    "obs.set_index(['OBS_REF'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18bb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'GHF'\n",
    "coord = ['lon', 'lat']\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "\n",
    "features = obs.index.tolist()\n",
    "\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.append(target)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e97ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing_world = 0.2 # 12 arcmin\n",
    "\n",
    "\n",
    "# create coordinates\n",
    "cols_world =np.linspace(world_lon_min, world_lon_max, 5* (world_lon_max - world_lon_min)-1 )\n",
    "rows_world = np.linspace(world_lat_min, world_lat_max, 5* (world_lat_max - world_lat_min) -1 )\n",
    "row_meshgrid_world ,col_meshgrid_world = np.meshgrid( rows_world, cols_world, indexing='ij')\n",
    "\n",
    "\n",
    "# put data into a dataset\n",
    "ds_world = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "         #variables=([\"Y\", \"X\"])\n",
    "    ),\n",
    "    coords=dict(\n",
    "        \n",
    "        YV=([\"Y\", \"X\"], row_meshgrid_world),\n",
    "        XV=([\"Y\", \"X\"], col_meshgrid_world),\n",
    "        lat=([\"Y\", \"X\"], row_meshgrid_world),\n",
    "        lon=([\"Y\", \"X\"], col_meshgrid_world),\n",
    "        Y=([\"Y\"], rows_world),\n",
    "        X=([\"X\"], cols_world),\n",
    "        \n",
    "    ),\n",
    "    attrs=dict(description=\"coords with matrices\"),\n",
    ")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "\n",
    "spacing_afr = 0.5\n",
    "\n",
    "\n",
    "# create coordinates\n",
    "cols_afr =np.linspace(afr_lon_min, afr_lon_max, 2* (afr_lon_max - afr_lon_min)-1 )\n",
    "rows_afr = np.linspace(afr_lat_min, afr_lat_max, 2* (afr_lat_max - afr_lat_min) -1 )\n",
    "row_meshgrid_afr ,col_meshgrid_afr = np.meshgrid( rows_afr, cols_afr, indexing='ij')\n",
    "\n",
    "\n",
    "# put data into a dataset\n",
    "ds_afr = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "         #variables=([\"Y\", \"X\"])\n",
    "    ),\n",
    "    coords=dict(\n",
    "        \n",
    "        YV=([\"Y\", \"X\"], row_meshgrid_afr),\n",
    "        XV=([\"Y\", \"X\"], col_meshgrid_afr),\n",
    "        lat=([\"Y\", \"X\"], row_meshgrid_afr),\n",
    "        lon=([\"Y\", \"X\"], col_meshgrid_afr),\n",
    "        Y=([\"Y\"], rows_afr),\n",
    "        X=([\"X\"], cols_afr),\n",
    "        \n",
    "    ),\n",
    "    attrs=dict(description=\"coords with matrices\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452ef2c",
   "metadata": {},
   "source": [
    "# Global datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d62686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped files to be downaloded and extracted automatically in their respective directories \n",
    "ctd_url = 'https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/bvz2jz99xh-2.zip'\n",
    "emag2v3_url =  'https://www.ngdc.noaa.gov/geomag/data/EMAG2/EMAG2_V3_20170530/EMAG2_V3_20170530_UpCont.tif'\n",
    "etopo_url =  'https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/bedrock/grid_registered/georeferenced_tiff/ETOPO1_Bed_g_geotiff.zip'\n",
    "litho_rho_url = 'https://b8ea67cb-f91c-4c4e-ba10-1f57eef81dff.filesusr.com/archives/7f2185_e894260d28d94e1ea1bdfcc04d213ef0.zip?dn=LithoRef_model.zip'\n",
    "glim_url = 'https://doi.pangaea.de/10013/epic.39939.d001'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f29de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing.....\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8c564f73e745858faca8e19eac56e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster bounds: BoundingBox(left=0.0, bottom=-90.0, right=360.0, top=90.0) (5400, 10800)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# correct values to limit them [-1,1]\n",
    "def mag_log(data, C=400, clip_min=-1, clip_max = 1):\n",
    "     return np.clip(np.sign(data)*np.log(1+np.abs(data)/C), clip_min, clip_max)\n",
    "\n",
    "\n",
    "emag2v3_path = pooch.retrieve(\n",
    "    url=emag2v3_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#The emag dataset needs to be adjusted from 0 - 360 to -180 180\n",
    "\n",
    "kwargs = {'dstSRS':'EPSG:4326', \n",
    "          'srcSRS':'EPSG:4326',\n",
    "          'format' : 'GTiff', \n",
    "          'outputBounds': (0, -90, 360, 90) }\n",
    "\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "emag2v3_dir = DIR/ 'Dataset'/ 'References'/'g_Magnetic'\n",
    "emag2v3_f = 'EMAG2_V3_20170530_UpCont_m0.tif'\n",
    "gdal.Warp(emag2v3_f, emag2v3_path,  **kwargs)\n",
    "\n",
    "\n",
    "try:\n",
    "    #Path(DIR/emag2v3_f).rename(emag2v3_dir /emag2v3_f)\n",
    "    shutil.move(DIR/emag2v3_f, emag2v3_dir /emag2v3_f)\n",
    "except OSError as e:\n",
    "    print(f\"An error has occurred. Continuing anyways: {e}\", end='\\n')\n",
    "\n",
    "      \n",
    "print('Interpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'EMAG2'\n",
    "    ds_world[observable] = (('Y', 'X'),  \n",
    "                         utils.read_raster(f_name = emag2v3_dir /emag2v3_f, \n",
    "                                     ds=ds_world,\n",
    "                                   crs_from=crs_from, crs_to=crs_to, no_data = -99999))\n",
    "    \n",
    "    observable = 'EMAG2_CLASS'\n",
    "    ds_world[observable] = (('Y', 'X'),  mag_log(ds_world['EMAG2'].values)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d5953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping.....\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014424bc7a914f03a5d1e1ed4f99b35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster bounds: BoundingBox(left=-180.0, bottom=-90.0, right=180.0, top=90.0) (360, 720)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "glim_path = pooch.retrieve(\n",
    "    url=glim_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "print('Unzipping.....', end='\\n\\n')\n",
    "\n",
    "archive = ZipFile(glim_path, 'r', )\n",
    "\n",
    "\n",
    "\n",
    "glim_dir = DIR/ 'Dataset'/ 'References'/'p_Global_Lithological_Map'\n",
    "glim_f = archive.extract(archive.filelist[1], path= glim_dir)\n",
    "\n",
    "print('Interpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'GLIM'\n",
    "    ds_world[observable] = (('Y', 'X'), np.around(utils.read_raster(\n",
    "    f_name=glim_f, skiprows=6,  ds =ds_world,\n",
    "        interpol='nearest', crs_from=crs_from, crs_to=crs_to),0))\n",
    "\n",
    "\n",
    "    ds_world[observable] = ds_world[observable].where(ds_world[observable].data>0, 16)\n",
    "    ds_world[observable] = ds_world[observable].where(ds_world[observable].data<16,16)\n",
    "\n",
    "\n",
    "glim_classes = [\"su\", \"vb\" ,\"ss\", \"pb\" , \"sm\" ,\"sc\" ,\"va\", \"mt\" ,\"pa\",\n",
    "\"vi\", \"wb\" ,\"py\", \"pi\" ,\"ev\" ,\"nd\" ,\"ig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed2f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data from 'https://b8ea67cb-f91c-4c4e-ba10-1f57eef81dff.filesusr.com/archives/7f2185_e894260d28d94e1ea1bdfcc04d213ef0.zip?dn=LithoRef_model.zip' to file 'C:\\Users\\Home\\AppData\\Local\\pooch\\pooch\\Cache\\6bef20d4e8800f3411c08a7d01a8b890-7f2185_e894260d28d94e1ea1bdfcc04d213ef0.zip'.\n",
      "100%|###############################################| 685k/685k [00:00<?, ?B/s]\n",
      "SHA256 hash of downloaded file: 0d63cb974d8a0d15387de1977c42f3a6d1bff9049235d7453172c6a3f94b6626\n",
      "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping.....\n",
      "\n",
      "min lon : -179.0\n",
      "min lat : -89.0\n",
      "min rho_c: 2558.31\n",
      "max lon : 179.0\n",
      "max lat : 89.0\n",
      "max rho_c: 2999.94\n",
      "#################\n",
      "min lon : -179.0\n",
      "min lat : -89.0\n",
      "min rho_l: 3234.79\n",
      "max lon : 179.0\n",
      "max lat : 89.0\n",
      "max rho_l: 3392.54\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37243fbe7fff466eb887bbb583882dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "litho_rho_path = pooch.retrieve(\n",
    "    url=litho_rho_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "print('Unzipping.....', end='\\n\\n')\n",
    "archive = ZipFile(litho_rho_path, 'r', )\n",
    "\n",
    "\n",
    "litho_rho_dir = DIR/ 'Dataset'/ 'References'/'i_Lithosphere_Average_Density'\n",
    "lith_roh_f = archive.extract(archive.filelist[0], path=litho_rho_dir)\n",
    "\n",
    "lith_df = pd.read_csv(lith_roh_f ,header=None,  engine='python',skiprows=9, \n",
    "                      encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "print(f'min lon : {lith_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {lith_df.iloc[:,1].min()}')\n",
    "print(f'min rho_c: {lith_df.iloc[:,5].min()}')\n",
    "\n",
    "print(f'max lon : {lith_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {lith_df.iloc[:,1].max()}')\n",
    "print(f'max rho_c: {lith_df.iloc[:,5].max()}')\n",
    "\n",
    "\n",
    "litho_c_df = lith_df.iloc[:,[0,1,5]]\n",
    "\n",
    "litho_c_df.columns = [0,1,2]\n",
    "\n",
    "litho_c_df[2] = litho_c_df[2].round(4)\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "print('#################')\n",
    "\n",
    "crust_rho_dir = DIR/ 'Dataset'/ 'References'/'i_Crustal_Average_Density'\n",
    "archive.extract(archive.filelist[0], path=crust_rho_dir)\n",
    "\n",
    "print(f'min lon : {lith_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {lith_df.iloc[:,1].min()}')\n",
    "print(f'min rho_l: {lith_df.iloc[:,6].min()}')\n",
    "\n",
    "print(f'max lon : {lith_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {lith_df.iloc[:,1].max()}')\n",
    "print(f'max rho_l: {lith_df.iloc[:,6].max()}')\n",
    "\n",
    "litho_l_df = lith_df.iloc[:,[0,1,6]]\n",
    "litho_l_df.columns = [0,1,2]\n",
    "\n",
    "litho_l_df[2] = litho_l_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for observable, df in tqdm_notebook(\n",
    "    zip(['RHO_L', 'RHO_C'], [litho_l_df, litho_c_df]), \n",
    "    total = 2, desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=df.values, ds=ds_world, crs_from=crs_from, \n",
    "        crs_to=crs_to, interpol='IDW'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402fc1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unzipping.....\n",
      "\n",
      "min lon : -179.5\n",
      "min lat : -88.5\n",
      "min z: -7.4117378\n",
      "max lon : 179.5\n",
      "max lat : 83.5\n",
      "max z: 160.59855\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa89779ef59440b4bb01d30fdc9566f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ctd_path = pooch.retrieve(\n",
    "    url=ctd_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('\\nUnzipping.....', end='\\n\\n')\n",
    "archive = ZipFile(ctd_path, 'r', )\n",
    "#glim_f = archive.read('glim_wgs84_0point5deg.txt.asc')\n",
    "\n",
    "ctd_dir = DIR/ 'Dataset'/ 'References'/'f_CTD'\n",
    "ctd_f = archive.extract(archive.filelist[0], path=ctd_dir)\n",
    "CTD_df = pd.read_csv(ctd_f ,header=None,  delim_whitespace=True)\n",
    "\n",
    "print(f'min lon : {CTD_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {CTD_df.iloc[:,1].min()}')\n",
    "print(f'min z: {CTD_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {CTD_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {CTD_df.iloc[:,1].max()}')\n",
    "print(f'max z: {CTD_df.iloc[:,2].max()}')\n",
    "\n",
    "CTD_df[2] = CTD_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'CTD'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=CTD_df.values, ds=ds_world, crs_from=crs_from, \n",
    "        crs_to=crs_to, interpol='IDW'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4d5966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping.....\n",
      "\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00300c2cde0f4dea995d328b2b05a89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster bounds: BoundingBox(left=-180.00833333333333, bottom=-90.00833333333328, right=180.00833333333333, top=90.00833333333334) (10801, 21601)\n"
     ]
    }
   ],
   "source": [
    "etopo_path = pooch.retrieve(\n",
    "    url=etopo_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('Unzipping.....', end='\\n\\n')\n",
    "archive = ZipFile(etopo_path, 'r', )\n",
    "\n",
    "\n",
    "dem_dir = DIR/ 'Dataset'/ 'References'/'h_Elevation'\n",
    "dem_f = archive.extract(archive.filelist[0], path=dem_dir)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "\n",
    "    observable = 'DEM'\n",
    "    ds_world[observable] = (('Y', 'X'), \n",
    "                            utils.read_raster(dem_f,   ds = ds_world, crs_from=crs_from, crs_to=crs_to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b6cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic downloads\n",
    "moho_url = 'https://zenodo.org/record/5730195/files/Global_Moho_WINTERC-G.xyz?download=1'\n",
    "lab_url = 'https://zenodo.org/record/5771863/files/WINTERC-G_LAB.lis?download=1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b9e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing.....\n",
      "\n",
      "min lon : 0.0\n",
      "min lat : -88.2\n",
      "min lab: 45.0\n",
      "max lon : 359.0\n",
      "max lat : 88.2\n",
      "max lab : 300.0\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07402d879d3c4b9bbe3de03d7f2ff6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab_path = pooch.retrieve(\n",
    "    url=lab_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "lab_df = pd.read_csv(lab_path ,header=None,  engine='python',skiprows=1,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "lab_df.set_index([0], inplace=True)\n",
    "\n",
    "\n",
    "print(f'min lon : {lab_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {lab_df.iloc[:,1].min()}')\n",
    "print(f'min lab: {lab_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {lab_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {lab_df.iloc[:,1].max()}')\n",
    "print(f'max lab : {lab_df.iloc[:,2].max()}')\n",
    "\n",
    "lab_df[2]  = lab_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'LAB'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=lab_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, set_center=True, interpol='IDW'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c876b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing.....\n",
      "\n",
      "min lon : 0.0\n",
      "min lat : -89.75\n",
      "min si: 8.33079\n",
      "max lon : 359.5\n",
      "max lat : 89.75\n",
      "max si : 68.6781\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df66715c86d1407cb57a4812694b4b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moho_path = pooch.retrieve(\n",
    "    url=moho_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "moho_df = pd.read_csv(moho_path ,header=None,  engine='python',skiprows=1,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'min lon : {moho_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {moho_df.iloc[:,1].min()}')\n",
    "print(f'min si: {moho_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {moho_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {moho_df.iloc[:,1].max()}')\n",
    "print(f'max si : {moho_df.iloc[:,2].max()}')\n",
    "\n",
    "moho_df[2]  = moho_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'MOHO'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=moho_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW',\n",
    "        set_center=True, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b35384f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets to be downloaded directly from their respective sources\n",
    "# then to be placed in their respective directories\n",
    "# sources of datasets\n",
    "# pv wave : https://www.earth.ox.ac.uk/~smachine/cgi/index.php?page=tomo_depth\n",
    "# Sv wave : https://www.dropbox.com/s/te4inov1586vus4/SL2013sv_0.5d-grd_v2.1.tar.bz2?dl=0\n",
    "# Geoid, Free-air, Bouguer : http://icgem.gfz-potsdam.de/calcgrid\n",
    "# shape index : https://www.3dearth.uni-kiel.de/en/public-data-products/copy_of_depth-to-moho-boundary\n",
    "# Tectonic regions : https://schaeffer.ca/models/sl2013sv-tectonic-regionalization/\n",
    "# Holocenes volcanoes https://volcano.si.edu/volcanolist_holocene.cfm\n",
    "# Pleistocene volcanoes https://volcano.si.edu/volcanolist_pleistocene.cfm\n",
    "#NGHF : 'https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1029%2F2019GC008389&file=2019GC008389-sup-0004-Data_Set_SI-S02.zip'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pv_g_f = DIR / 'Dataset'/'References'/'e_PV'/'DETOX-P1.txt'\n",
    "sv_g_f = DIR / 'Dataset'/'References'/'d_SV'/'SL2013sv_dVs_25km-0.5d_50+.xyz'\n",
    "geoid_f = DIR / 'Dataset'/ 'References'/'l_Geoid' / 'EIGEN-6C4_geoid.gdf'\n",
    "fa_f = DIR / 'Dataset'/ 'References'/'k_Free_Air' / 'EIGEN-6C4_gravity_anomaly_cl.gdf'\n",
    "bg_f = DIR / 'Dataset'/ 'References'/'m_Bouguer' / 'EIGEN-6C4_gravity_anomaly_bg.gdf'\n",
    "si_f = DIR / 'Dataset'/ 'References'/'n_Shape_Index' / 'GOCE_Curvature_Topo_Iso_Corr.txt'\n",
    "reg_f = DIR / 'Dataset'/'References'/'o_Tectonic_Regionalization'/'SL2013sv_Cluster_2d'\n",
    "volcs_hol_f =   DIR / 'Dataset'/ 'References'/'c_Distance_to_nearest_volcano'/'GVP_Volcano_List_Holocene.xlsx'\n",
    "volcs_plei_f =  DIR / 'Dataset'/ 'References'/'c_Distance_to_nearest_volcano'/'GVP_Volcano_List_Pleistocene.xlsx'\n",
    "hq_f = DIR / 'Dataset'/ 'References'/ 'q_Heat_Flow'/'NGHF.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f012d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min lon : -180.0\n",
      "min lat : -90.0\n",
      "min si: 1\n",
      "max lon : 180.0\n",
      "max lat : 90.0\n",
      "max si : 6\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4811fde364d6418e8fe03184c009f06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "reg_df = pd.read_csv(reg_f ,header=None,   delim_whitespace=True)\n",
    "print(f'min lon : {reg_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {reg_df.iloc[:,1].min()}')\n",
    "print(f'min si: {reg_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {reg_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {reg_df.iloc[:,1].max()}')\n",
    "print(f'max si : {reg_df.iloc[:,2].max()}')\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'REG'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=reg_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='nearest'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f7b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing.....\n",
      "min lon : -180.0\n",
      "min lat : -89.8\n",
      "min si: -0.99939\n",
      "max lon : 949.0\n",
      "max lat : 89.8\n",
      "max si : 0.99932\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31469e39784549759c0d8193fd3a5e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print('Importing.....')\n",
    "si_df = pd.read_csv(si_f ,header=None,  engine='python',skiprows=1, \n",
    "                    encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f'min lon : {si_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {si_df.iloc[:,1].min()}')\n",
    "print(f'min si: {si_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {si_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {si_df.iloc[:,1].max()}')\n",
    "print(f'max si : {si_df.iloc[:,2].max()}')\n",
    "\n",
    "si_f_df = si_df[[0,1,2]]\n",
    "si_f_df[2]  = si_f_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'SI'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=si_f_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509ca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing.....\n",
      "\n",
      "min lon : -180.0\n",
      "min lat : -90.0\n",
      "min geoid: -106.184427114513\n",
      "max lon : 180.0\n",
      "max lat : 90.0\n",
      "max z: 84.917290827278\n",
      "###############\n",
      "min lon : -180.0\n",
      "min lat : -90.0\n",
      "min geoid: -301.300980351904\n",
      "max lon : 180.0\n",
      "max lat : 90.0\n",
      "max z: 522.25583386214\n",
      "###############\n",
      "min lon : -180.0\n",
      "min lat : -90.0\n",
      "min geoid: -631.218235051554\n",
      "max lon : 180.0\n",
      "max lat : 90.0\n",
      "max z: 455.456168986875\n",
      "\n",
      "Interpolating.....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242d1c5c6af94436919322d31464da89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "geoid_df = pd.read_csv(geoid_f ,header=None,  engine='python',skiprows=36,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "print(f'min lon : {geoid_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {geoid_df.iloc[:,1].min()}')\n",
    "print(f'min geoid: {geoid_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {geoid_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {geoid_df.iloc[:,1].max()}')\n",
    "print(f'max z: {geoid_df.iloc[:,2].max()}')\n",
    "\n",
    "\n",
    "geoid_df = geoid_df.round(4)\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "print('###############')\n",
    "fa_df = pd.read_csv(fa_f ,header=None,  engine='python',skiprows=36,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'min lon : {fa_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {fa_df.iloc[:,1].min()}')\n",
    "print(f'min geoid: {fa_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {fa_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {fa_df.iloc[:,1].max()}')\n",
    "print(f'max z: {fa_df.iloc[:,2].max()}')\n",
    "\n",
    "fa_df = fa_df.round(4)\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "print('###############')\n",
    "bg_df = pd.read_csv(bg_f ,header=None,  engine='python',skiprows=37,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'min lon : {bg_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {bg_df.iloc[:,1].min()}')\n",
    "print(f'min geoid: {bg_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {bg_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {bg_df.iloc[:,1].max()}')\n",
    "print(f'max z: {bg_df.iloc[:,2].max()}')\n",
    "\n",
    "bg_df = bg_df.round(4)\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for observable, df in tqdm_notebook(zip(['GEOID', 'FA' , 'BG'], [geoid_df, fa_df, bg_df]), \n",
    "                                    total=3, desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ea373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "pv_g_df = pd.read_csv(pv_g_f, skiprows=3,\n",
    "                              header=None, engine='python', skip_blank_lines=True,   na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'lon min : {pv_g_df.iloc[:,0].min()}')\n",
    "print(f'lat min : {pv_g_df.iloc[:,1].min()}')\n",
    "print(f'lon max : {pv_g_df.iloc[:,0].max()}')\n",
    "print(f'lat max : {pv_g_df.iloc[:,1].max()}')\n",
    "print(f'pv min before: {pv_g_df[2] .min()}')\n",
    "print(f'pv max before: {pv_g_df[2] .max()}')\n",
    "\n",
    "\n",
    "# remove % by * 0.01 'dVp(%)'\n",
    "\n",
    "pv_g_df[2] = pv_g_df.iloc[:,2].multiply(0.01).round(4)\n",
    "print(f'pv min after: {pv_g_df[2] .min()}')\n",
    "print(f'pv max after: {pv_g_df[2] .max()}')\n",
    "\n",
    "\n",
    "print('#####')\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'PV'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=pv_g_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        set_center=True, crs_to=crs_to, interpol='IDW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "sv_init_df = pd.read_csv(sv_g_f,header=None, skiprows=1, skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "print(f'lon min : {sv_init_df.iloc[:,1].min()}')\n",
    "print(f'lat min : {sv_init_df.iloc[:,2].min()}')\n",
    "print(f'lon max : {sv_init_df.iloc[:,1].max()}')\n",
    "print(f'lat max : {sv_init_df.iloc[:,2].max()}')\n",
    "print(f'sv min before: {sv_init_df[5] .min()}')\n",
    "print(f'sv max before: {sv_init_df[5] .max()}')\n",
    "# remove % by * 0.01 'dVs(%)'\n",
    "\n",
    "sv_init_df[5] = sv_init_df.iloc[:,5].multiply(0.01).round(4)\n",
    "print(f'sv min after: {sv_init_df[5] .min()}')\n",
    "print(f'sv max after: {sv_init_df[5] .max()}')\n",
    "\n",
    "print('#####')\n",
    "\n",
    "sv_g_150_df = sv_init_df[sv_init_df[0]==150][[1,2,5]]\n",
    "#seismic_w_sv_df = copy.deepcopy(seismic_w_sv_df_150_df.iloc[:,[1,2,5]]\n",
    "\n",
    "\n",
    "sv_g_150_df.columns = [0,1,2]\n",
    "\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'SV'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=sv_g_150_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_max_dist = 100\n",
    "a_max_dist = 250\n",
    "\n",
    "\n",
    "volcs_hol_df = pd.read_excel(volcs_hol_f, header=1)\n",
    "volcs_plei_df = pd.read_excel(volcs_plei_f, header=1)\n",
    "volc_df = pd.concat([volcs_hol_df, volcs_plei_df])\n",
    "\n",
    "print(volc_df[['Latitude','Longitude']])\n",
    "cols = volc_df.columns.to_list()\n",
    "cols[1], cols[0] = cols[0], cols[1]\n",
    "volc_df.loc[:,cols]\n",
    "\n",
    "print(f' Hol: {len(volcs_hol_df)}')\n",
    "print(f' Plei: {len(volcs_plei_df)}')\n",
    "print(f' Total: {len(volc_df)}')\n",
    "\n",
    "volc_df = volc_df[['Latitude','Longitude']]\n",
    "volc_df.columns = [ 'lon', 'lat']\n",
    "\n",
    "\n",
    "volc_df = volc_df.round(2)\n",
    "\n",
    "print(f\" lon: {volc_df['lon'].min()} {volc_df['lon'].max()}\")\n",
    "print(f\" lat: {volc_df['lat'].min()} {volc_df['lat'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca30cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual inspection of imported datasets\n",
    "observables = ['RHO_L', 'RHO_C', 'CTD', 'DEM', 'GLIM', 'EMAG2_CLASS', 'MOHO', 'LAB',\n",
    "              'SV', 'PV', 'REG', 'BG', 'FA', 'GEOID', 'SI']\n",
    "\n",
    "data_types = [True,True, True, True, False , True , True,True, \n",
    "              True,True, False, True,True, True,True, ]\n",
    "\n",
    "scales = [False,False,False,False, False, True, False,False,          \n",
    "          False,False, False, False, False, False, False,]\n",
    "\n",
    "\n",
    "\n",
    "for observable, data_type, scale in tqdm_notebook(zip(observables, data_types, scales), \n",
    "                                          total=15, desc = 'Processing: '):\n",
    "    print('Plotting.....', end='\\n\\n')\n",
    "    sleep(0.01)\n",
    "#for observable, data_type, scale in zip(observables, data_types, scales):\n",
    "    label = obs.loc[observable, 'LABELS_gmt']\n",
    "    unit = obs.loc[observable, 'UNITS_gmt']\n",
    "    if (observable == 'GLIM') or (observable == 'REG') :\n",
    "        series=(obs.loc[observable, 'V_RANGE'][0], obs.loc[observable, 'V_RANGE'][1]+1, 1)\n",
    "    else:\n",
    "        min_range = obs.loc[observable, 'V_RANGE'][0]\n",
    "        max_range = obs.loc[observable, 'V_RANGE'][1]\n",
    "        series=(min_range, max_range,(max_range - min_range)/100 )\n",
    "    \n",
    "    fig = pygmt.Figure()\n",
    "    pygmt.makecpt(\n",
    "        cmap=obs.loc[observable, 'CMAPS'], #temp 19lev\n",
    "\n",
    "        #cmap='lajolla',\n",
    "        #series= \"1/17/1\",\n",
    "        #truncate = '-2000/2000',\n",
    "       # categorical=list(range(1,17)),\n",
    "        continuous=data_type,\n",
    "        reverse=scale,\n",
    "        \n",
    "       series = series,\n",
    "        # convert ['Adelie', 'Chinstrap', 'Gentoo'] to 'Adelie,Chinstrap,Gentoo'\n",
    "        #color_model=\"+c\" + \",\".join(glim_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    fig.basemap(frame=True, region=region_world, projection=projection,)\n",
    "    #avoid interpolation\n",
    "    fig.grdimage(ds_world[observable],\n",
    "                 interpolation='n+a' \n",
    "                )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    fig.coast(\n",
    "        #region = 'JP',\n",
    "        #shorelines=0.5,\n",
    "        #water=\"lightblue\", \n",
    "        shorelines=\"0.1p,black\",\n",
    "        #borders=[\"1/0.001p,black\"],\n",
    "        lakes=\"lightblue\",\n",
    "        rivers=\"lightblue\" ,\n",
    "        #borders=[\"1/0.5p,black\"],\n",
    "        #water='white',\n",
    "        )\n",
    "    fig.colorbar(frame=[\"af\", f'x+l\"{label}\"\\t\\t{unit}'])\n",
    "    fig.show(width=1000)\n",
    "    #fig.savefig(\"north-america-pygmt-map.png\",crop=True, dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed31da6",
   "metadata": {},
   "source": [
    "# Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start with shared observables with global models\n",
    "\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "\n",
    "    ds_afr['EMAG2'] = (('Y', 'X'),  utils.read_raster(f_name = emag2v3_dir /emag2v3_f, \n",
    "                                            ds=ds_afr, crs_to=crs_to, crs_from=crs_from, \n",
    "                                            interpol='nearest', no_data = -99999))\n",
    "    ds_afr['EMAG2_CLASS'] = (('Y', 'X'),  mag_log(ds_afr['EMAG2'].values)) \n",
    "\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'DEM'\n",
    "    ds_afr[observable] = (('Y', 'X'), utils.read_raster(\n",
    "            f_name=dem_f, ds=ds_afr, interpol='nearest', crs_from=crs_from, crs_to=crs_to))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'GLIM'\n",
    "    ds_afr[observable] = (('Y', 'X'), np.around(utils.read_raster(\n",
    "    f_name=glim_f, skiprows=6, interpol='nearest', ds=ds_afr, crs_from=crs_from, crs_to=crs_to),0))\n",
    "\n",
    "\n",
    "    ds_afr[observable] = ds_afr[observable].where(ds_afr[observable].data>0, 16)\n",
    "    ds_afr[observable] = ds_afr[observable].where(ds_afr[observable].data<16,16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "observables = ['BG', 'FA', 'GEOID', 'SI', 'REG', 'MOHO', 'LAB', 'CTD', 'RHO_L', 'RHO_C' ]\n",
    "dfs = [bg_df, fa_df, geoid_df, si_df, reg_df, moho_df, lab_df, CTD_df, litho_l_df, litho_c_df]\n",
    "interpols = ['IDW', 'IDW', 'IDW', 'IDW', 'nearest', 'IDW', 'IDW', 'IDW', 'IDW', 'IDW']\n",
    "centerings = [False, False, False, False, False, True, True, False, False, False]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for observable, df, interpol, centering in tqdm_notebook(\n",
    "    zip(observables, dfs, interpols, centerings), \n",
    "                                          total=10, desc = 'Processing: '):\n",
    "\n",
    "    sleep(0.01)\n",
    "    ds_afr[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "    data=df.values, ds=ds_afr, crs_from=crs_from, crs_to=crs_to, interpol=interpol,\n",
    "    set_center=centering))\n",
    "\n",
    "print('Terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the position of volcanoes\n",
    "\n",
    "\n",
    "\n",
    "geod = proj.Geod(ellps='WGS84' )\n",
    "vol_int = volc_df[volc_df['lon'].between(afr_lon_min, afr_lon_max) \n",
    "                     & volc_df['lat'].between(afr_lat_min, afr_lat_max)]\n",
    "v_lon = vol_int['lon'].values\n",
    "v_lat = vol_int['lat'].values\n",
    "\n",
    "v_n = len(v_lon)\n",
    "\n",
    "v_k = 1\n",
    "\n",
    "    \n",
    "ny = len(ds_afr.Y)\n",
    "nx = len(ds_afr.X)\n",
    "nn = (ny, nx)\n",
    "volcs = np.zeros(nn)\n",
    "\n",
    "lats = ds_afr['lat'].values\n",
    "lons = ds_afr['lon'].values\n",
    "\n",
    "# this process is slow to map all grid\n",
    "\n",
    "for x in tqdm_notebook(range(nx), desc='lon: ' ):\n",
    "    for y in tqdm_notebook(range(ny), desc='lat: ', leave=False ):\n",
    "        sleep(0.01)\n",
    "        lat = lats[y,x]\n",
    "        lon = lons[y,x]\n",
    "        _, _, ds_ = geod.inv(v_n*[lon], v_n*[lat], v_lon, v_lat)\n",
    "        idx = np.argpartition(ds_, v_k)[:v_k]\n",
    "        volcs[y,x] = np.sum(np.take(ds_, idx))/v_k/km\n",
    "\n",
    "ds_afr['VOLC_DIST'] = (('Y', 'X'), volcs)\n",
    "ds_afr['VOLC_DIST_W'] = (('Y', 'X'), np.clip(1 - volcs/volc_max_dist, 0, 1)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1225116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the modle from https://nlscelli.wixsite.com/ncseismology/af2019\n",
    "\n",
    "sv_a_f = DIR / 'Dataset'/'References'/'d_SV'/'AF2019_vsvp.mod'\n",
    "\n",
    "pv_afr_url = 'http://ds.iris.edu/files/products/emc/emc-files/AFRP20-RF-CR1-MOD-3D.r0.0.nc'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_afr_path = pooch.retrieve(\n",
    "    url=pv_afr_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "depth_150_km = 2\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "### dVp\n",
    "dvp = xr.load_dataset(pv_afr_path)['dvp'][depth_150_km,:,:].values.reshape(-1,1)\n",
    "dvp = np.round(dvp * 0.01, 4)\n",
    "pv_lon = xr.load_dataset(pv_afr_path)['longitude'].values\n",
    "pv_lat = xr.load_dataset(pv_afr_path)['latitude'].values\n",
    "lat_mesh ,lon_mesh = np.meshgrid( pv_lat, pv_lon, indexing='ij')\n",
    "\n",
    "lons = lon_mesh.reshape(-1,1)\n",
    "lats = lat_mesh.reshape(-1,1)\n",
    "dvp_arr = np.append(lons, lats, axis=1)\n",
    "dvp_arr = np.append(dvp_arr, dvp, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "### VP velocity\n",
    "\n",
    "\n",
    "pv = xr.load_dataset(pv_afr_path)['vp'][depth_150_km,:,:].values.reshape(-1,1)\n",
    "\n",
    "vp = np.round(pv, 4)\n",
    "vp_arr = np.append(lons, lats, axis=1)\n",
    "vp_arr = np.append(vp_arr, vp, axis=1)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "\n",
    "for observable, arr in tqdm_notebook(\n",
    "    zip(['PV', 'PV_Velocity' ], [dvp_arr, vp_arr]), \n",
    "                                          total=2, desc = 'Processing: '):\n",
    "\n",
    "    ds_afr[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=arr, ds=ds_afr, crs_from=crs_from, crs_to=crs_to, interpol='IDW'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60aaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "sv_init_df = pd.read_csv(sv_a_f   , skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'lon min : {sv_init_df[\"#lon\"].min()}')\n",
    "print(f'lat min : {sv_init_df[\"lat\"].min()}')\n",
    "print(f'lon max : {sv_init_df[\"#lon\"].max()}')\n",
    "print(f'lat max : {sv_init_df[\"lat\"].max()}')\n",
    "print(f'dVs min before: {sv_init_df[\"dVs\"] .min()}')\n",
    "print(f'dVs max before: {sv_init_df[\"dVs\"] .max()}')\n",
    "\n",
    "#calculate percentile dvs\n",
    "\n",
    "sv_init_df[\"dVs%\"] = sv_init_df[\"dVs\"] / sv_init_df[\"Vs_ref\"]\n",
    "print(f'dVs % min after: {sv_init_df[\"dVs%\"].min()}')\n",
    "print(f'dVs % max after: {sv_init_df[\"dVs%\"].max()}')\n",
    "\n",
    "\n",
    "dvs_df = sv_init_df[sv_init_df['depth']==150][['#lon','lat' , \"dVs%\"]]\n",
    "\n",
    "dvs_df[\"dVs%\"]  = dvs_df[\"dVs%\"].round(4)\n",
    "    \n",
    "dvs_df.columns = [0,1,2]\n",
    "\n",
    "#######\n",
    "# SV africa speed\n",
    "\n",
    "\n",
    "vs_df = sv_init_df[sv_init_df['depth']==150][['#lon','lat' , 'Vs_abs']]\n",
    "\n",
    "\n",
    "    \n",
    "vs_df['Vs_abs'] = vs_df['Vs_abs'].round(4)/1000   \n",
    "\n",
    "\n",
    "vs_df.columns = [0,1,2]\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for observable, df in tqdm_notebook(\n",
    "    zip(['SV', 'SV_Velocity' ], [dvs_df, vs_df]), \n",
    "                                          total=2, desc = 'Processing: '):\n",
    "\n",
    "    ds_afr[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=df.values, ds=ds_afr, crs_from=crs_from, crs_to=crs_to, interpol='IDW'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fab79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual inspection of imported datasets\n",
    "\n",
    "\n",
    "data_types = [True,True, True, True, True , True , True,True, \n",
    "              True, True,True, True,True, True, False, False ]\n",
    "scales = [False,False,False,False, False, False, False,False,\n",
    "          \n",
    "          False,False, False, False, False, False, False, False]\n",
    "\n",
    "for observable, data_type, scale in tqdm_notebook(zip(obs.index, data_types, scales), \n",
    "                                          total=16, desc = 'Processing: '):\n",
    "    print('Plotting.....', end='\\n\\n')\n",
    "    sleep(0.01)\n",
    "    label = obs.loc[observable, 'LABELS_gmt']\n",
    "    unit = obs.loc[observable, 'UNITS_gmt']\n",
    "    if (observable == 'GLIM') or (observable == 'REG') :\n",
    "        series=(obs.loc[observable, 'V_RANGE'][0], obs.loc[observable, 'V_RANGE'][1]+1, 1)\n",
    "    else:\n",
    "        min_range = obs.loc[observable, 'V_RANGE'][0]\n",
    "        max_range = obs.loc[observable, 'V_RANGE'][1]\n",
    "        series=(min_range, max_range,(max_range - min_range)/100 )\n",
    "\n",
    "    fig = pygmt.Figure()\n",
    "    cmap = pygmt.makecpt(\n",
    "        cmap=obs.loc[observable, 'CMAPS'], #temp 19lev\n",
    "        #cmap='lajolla',\n",
    "        #series= \"1/17/1\",\n",
    "        #truncate = '-2000/2000',\n",
    "       # categorical=list(range(1,17)),\n",
    "        continuous=data_type,\n",
    "        reverse=scale,\n",
    "\n",
    "       series = series,\n",
    "        # convert ['Adelie', 'Chinstrap', 'Gentoo'] to 'Adelie,Chinstrap,Gentoo'\n",
    "        #color_model=\"+c\" + \",\".join(glim_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    fig.basemap(frame=True, region=region_afr, projection=projection,)\n",
    "    #avoid interpolation\n",
    "    if (observable == 'GLIM') or (observable == 'REG') :\n",
    "        fig.grdimage(ds_afr[observable],\n",
    "                     interpolation='n+a' \n",
    "                    )\n",
    "    else:\n",
    "\n",
    "        fig.grdimage(ds_afr[observable]\n",
    "                    )\n",
    "    fig.coast(\n",
    "        #shorelines=0.5,\n",
    "        #water=\"lightblue\", \n",
    "        shorelines=\"0.1p,black\",\n",
    "        #borders=[\"1/0.001p,black\"],\n",
    "        lakes=\"lightblue\",\n",
    "        rivers=\"lightblue\" ,\n",
    "        #dcw=\"=AS,=EU+gdarkgrey\",\n",
    "        resolution = 'i',\n",
    "        )\n",
    "    fig.colorbar(frame=[\"af\", f'x+l\"{label}\"\\t\\t{unit}'])\n",
    "    fig.show(width=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908bf5a",
   "metadata": {},
   "source": [
    "function to make inverse distance weight interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3646808",
   "metadata": {},
   "source": [
    "# read in dataseys from xyz folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f076e",
   "metadata": {},
   "source": [
    "Interpolation for wolrd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88c05d",
   "metadata": {},
   "source": [
    "interpolatation for Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64610d7f",
   "metadata": {},
   "source": [
    "# 2 - Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract hq ratings\n",
    "\n",
    "# extracted from agrid Tobias Stal\n",
    "rabcd = ['A', 'B', 'C', 'D']\n",
    "rabc = ['A', 'B', 'C']\n",
    "rab = ['A', 'B']\n",
    "ra = ['A']\n",
    "exclude = ['Indian Ocean', \n",
    "           'Arctic Ocean', \n",
    "           'North Atlantic Ocean', \n",
    "           'South Atlantic Ocean', \n",
    "           'South Pacific Ocean', \n",
    "           'North Pacific Ocean']\n",
    "\n",
    "\n",
    "\n",
    "hq_in = pd.read_csv(hq_f)\n",
    "# convert hear to mili\n",
    "\n",
    "\n",
    "elev_range = (-5000,5000)\n",
    "elev_cut = -1000\n",
    "elev_high_cut = -250\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16, 5))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "\n",
    "hq_in = hq_in.rename(columns={'heat-flow (mW/m2)' :target, 'longitude': 'lon', 'latitude': 'lat'})\n",
    "    \n",
    "ax[1].set_xlim(elev_range)\n",
    "\n",
    "\n",
    "utils.print_latex_tab_line(d = hq_in[target], c = 'All records')\n",
    "\n",
    "utils.hq_hist(hq_in, target, ax=ax[0], label = f'All records (N={len(hq_in):,})')\n",
    "\n",
    "\n",
    "# Remove records with missing position or heat flow value in long lat heat\n",
    "hq_clean = hq_in.dropna(subset = ['lon', 'lat', target])\n",
    "utils.print_latex_tab_line(d = hq_clean[target], c = 'Excluded incomplete records')\n",
    "utils.hq_hist(hq_clean, target,ax=ax[0], label = f'Excluded incomplete records (N={len(hq_clean):,})')\n",
    "\n",
    "# Remove measurements at low elevation\n",
    "hq_elev = hq_clean[hq_clean['elevation (m)']>elev_cut]\n",
    "\n",
    "\n",
    "\n",
    "hq_clean['elevation (m)'].hist(bins=151, ax=ax[1], \n",
    "                               range = elev_range, \n",
    "                               label = 'Excluded',\n",
    "                              color = 'brown')\n",
    "\n",
    "hq_elev['elevation (m)'].hist(bins=151, ax=ax[1], \n",
    "                              range = elev_range, \n",
    "                              label = 'Used records',\n",
    "                             color = 'green')\n",
    "\n",
    "\n",
    "ax[1].axvline(x=elev_cut, color='gray', label=f'Cut-off elevation {elev_cut} m', lw=1)\n",
    "\n",
    "utils.print_latex_tab_line(d = hq_elev[target], c = f'Excluded deeper than {abs(elev_cut):,} m bsl')\n",
    "utils.hq_hist(hq_elev, target,ax=ax[0],  label = f'Excluded deeper than {abs(elev_cut)} m (N={len(hq_elev):,})') \n",
    "\n",
    "#hq_land = hq_clean.loc[~hq_clean['country'].isin(exclude)] # Exclude oceans \n",
    "\n",
    "# Remove any heat flow measurements from high lats for poles\n",
    "hq_no_pole = hq_elev[hq_elev['lat'].between(world_lat_min, world_lat_max, inclusive='both')]\n",
    "utils.print_latex_tab_line(d = hq_no_pole[target], c = 'Excluded high lats')\n",
    "utils.hq_hist(hq_no_pole, target, ax=ax[0], label = f'Excluded high lats (N={len(hq_no_pole):,})')\n",
    "\n",
    "\n",
    "# Select rabd rathings\n",
    "hq_rabcd = hq_no_pole.loc[hq_no_pole['code6'].isin(rabcd)] # Only rab measurements\n",
    "utils.hq_hist(hq_rabcd, target, ax=ax[0], label = f'Rating A-D (N={len(hq_rabcd):,})')\n",
    "utils.print_latex_tab_line(d = hq_rabcd[target], c = 'Rating A$^c$ , B$^d$ , C$^e$')  \n",
    "\n",
    "#hq_rabcd = hq_rabcd.rename(columns={target :target})\n",
    "\n",
    "# Select rab rathings\n",
    "hq_rabc = hq_no_pole.loc[hq_no_pole['code6'].isin(rabc)] # Only rab measurements\n",
    "utils.hq_hist(hq_rabc, target, ax=ax[0], label = f'Rating A-C (N={len(hq_rabc):,})')\n",
    "utils.print_latex_tab_line(d = hq_rabc[target], c = 'Rating A$^c$ , B$^d$ , C$^e$')  \n",
    "\n",
    "#hq_rabc = hq_rabc.rename(columns={target :target})\n",
    "\n",
    "\n",
    "# Select better ratings\n",
    "hq_rab = hq_no_pole.loc[hq_no_pole['code6'].isin(rab)] # Only very rab measurements\n",
    "utils.hq_hist(hq_rab, target,ax=ax[0],  label = f'Rating A-B (N={len(hq_rab):,})')\n",
    "utils.print_latex_tab_line(d = hq_rab[target], c = 'Rating A$^c$ , B$^d$')    \n",
    "\n",
    "#hq_rab = hq_rab.rename(columns={target :target})\n",
    "\n",
    "# Select ra ratings\n",
    "hq_ra = hq_no_pole.loc[hq_no_pole['code6'].isin(ra)] # Only the ra measurements\n",
    "utils.hq_hist(hq_ra, target, ax=ax[0], label = f'Rating A (N={len(hq_ra):,})')\n",
    "utils.print_latex_tab_line(d = hq_ra[target], c = 'Rating A$^c$')\n",
    "\n",
    "#hq_ra = hq_ra.rename(columns={target :target})\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.set_ylabel('N')\n",
    "\n",
    "ax[1].set_xlabel('Elevation (m)')\n",
    "ax[0].set_xlabel('Heat flow (mW/m2)')\n",
    "    \n",
    "ax[1].set_title('Cut-off elevation')\n",
    "ax[0].set_title('Distribution of heat flow values after removal of records')\n",
    "   \n",
    "plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22127d",
   "metadata": {},
   "source": [
    "get index from grid and distance and transfer it to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer obs values from grid to hq integrate grid index\n",
    "\n",
    "\n",
    "training_f = DIR /'Dataset'/'Preprocessed'\n",
    "\n",
    "hq_ra_w = hq_ra[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "hq_rab_w = hq_rab[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "\n",
    "\n",
    "# volcanoes\n",
    "geod = proj.Geod(ellps='WGS84' )\n",
    "vol_int = volc_df[volc_df['lon'].between(world_lon_min, world_lon_max) \n",
    "                     & volc_df['lat'].between(world_lat_min, world_lat_max)]\n",
    "\n",
    "#poistion of volvanoes inside targeted area\n",
    "v_lons = vol_int['lon'].values.ravel()\n",
    "v_lats = vol_int['lat'].values.ravel()\n",
    "\n",
    "for hq,label in tqdm_notebook(\n",
    "    zip([hq_ra_w, hq_rab_w], ['int_ra.csv','int_rab.csv']), \n",
    "                                          total=2, desc = 'Processing: '):\n",
    "\n",
    "    sleep(0.01)\n",
    "\n",
    "    model_coords = np.stack([ds_world.lon.values.ravel(), ds_world.lat.values.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum([spacing_world , spacing_world])/1.5 # equivalent to 15 km\n",
    "\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hq_coords = np.stack([hq['lon'], hq['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hq_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    hq.loc[:,'grid_index_world'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "    hq.loc[:,'grid_index_world'] = hq.loc[:,'grid_index_world'].astype('int')\n",
    "\n",
    "    dists_km = utils.distance(hq['lat'].values, hq['lon'].values, \n",
    "                    ds_world.lat.values.ravel()[indexs], ds_world.lon.values.ravel()[indexs])\n",
    "\n",
    "    hq.loc[:,'dist_from_grid_world'] = dists_km*km\n",
    "\n",
    "\n",
    "    for observable in list(ds_world):\n",
    "            hq.loc[:,observable] = ds_world[observable].values.ravel()[hq.loc[:,'grid_index_world'].values] \n",
    "            \n",
    "     \n",
    "    ###volcanoes\n",
    "\n",
    "\n",
    "\n",
    "    # this process is slow to map all grid\n",
    "\n",
    "    volc_dist = []\n",
    "    volc_dist_w = []\n",
    "    # this process is slow to map all grid\n",
    "    hq_lons, hq_lats = hq['lon'].values.ravel().reshape(-1,1) , hq['lat'].values.ravel().reshape(-1,1)\n",
    "    for i in tqdm_notebook(range(len(hq)), desc='Volcs: ' ):\n",
    "        sleep(0.01)\n",
    "        hq_lat = np.repeat(hq_lats[i], len(v_lats) , axis=0).reshape(-1,1)\n",
    "        hq_lon =np.repeat(hq_lons[i], len(v_lons) , axis=0).reshape(-1,1)\n",
    "        _, _, ds = geod.inv(v_lons, v_lats, hq_lon, hq_lat)\n",
    "        #idx = np.argpartition(ds, v_k)[:v_k]\n",
    "        #ds_volcs[i] = (('Y', 'X'), ds.reshape(nn_world)/v_k/km)\n",
    "        #volcs[i] = min(ds/v_k/km)\n",
    "        volc_dist.append(min(ds)[0]/1/km)\n",
    "        volc_dist_w.append(np.clip(1 - (min(ds)[0]/1/km)/volc_max_dist, 0, 1))\n",
    "        #hq_volc.loc[i, 'VOLC_DIST_W'] = np.clip(1 - hq_volc.loc[i, 'VOLC_DIST']/volc_max_dist, 0, 1)\n",
    "    hq['VOLC_DIST'] = volc_dist\n",
    "    hq['VOLC_DIST_W'] = volc_dist_w\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    hq.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    hq.reset_index( inplace=True,drop=True)\n",
    "    hq.to_csv(training_f/f'W_{label}' , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa28b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hq_ra_w = hq_ra[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "hq_rab_w = hq_rab[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "for hq,label in tqdm_notebook(\n",
    "    zip([hq_ra_w, hq_rab_w], [f'int_ra.csv',f'int_rab.csv']), \n",
    "                                          total=2, desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    hq_afr = hq[(hq['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hq['lat'].between(afr_lat_min, afr_lat_max))].reset_index(drop=True)\n",
    "\n",
    "    model_coords = np.stack([ds_afr.lon.values.ravel(), ds_afr.lat.values.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum([spacing_afr, spacing_afr])/1.5  # equivalent to 15 km\n",
    "\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world ds\n",
    "    hq_coords = np.stack([hq_afr['lon'], hq_afr['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world ds to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hq_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    hq_afr.loc[:,'grid_index_afr'] = indexs # This is the index to the cell in the ravel ds\n",
    "\n",
    "    hq_afr.loc[:,'grid_index_afr'] = hq_afr.loc[:,'grid_index_afr'].astype('int')\n",
    "\n",
    "    dists_km = utils.distance( hq_afr['lat'].values, hq_afr['lon'].values,\n",
    "                      ds_afr.lat.values.ravel()[indexs], ds_afr.lon.values.ravel()[indexs])\n",
    "\n",
    "    hq_afr.loc[:,'dist_from_grid'] = dists_km*km\n",
    "\n",
    "    for observable in list(ds_afr):\n",
    "            hq_afr.loc[:,observable] = ds_afr[observable].values.ravel()[hq_afr.loc[:,'grid_index_afr'].values] \n",
    "\n",
    "    hq_afr = hq_afr.rename({target:'GHF'})\n",
    "    hq_afr.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    hq_afr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    hq_afr.to_csv(training_f/f'Afr_{label}',  index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59616c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_world.to_netcdf(DIR/'Grids'/'Inputs'/\"ds_world.nc\", mode='w', \n",
    "                    engine='netcdf4')\n",
    "\n",
    "ds_afr.to_netcdf(DIR/'Grids'/'Inputs'/\"ds_afr.nc\", mode='w', \n",
    "                    engine='netcdf4')\n",
    "\n",
    "\n",
    "\n",
    "#ds_world = xr.load_dataset(DIR/'Grids'/'Inputs'/\"ds_world.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beac77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
