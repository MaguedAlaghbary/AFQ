{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys, pickle, copy, pygmt, operator\n",
    "\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing\n",
    "#import numba as nb\n",
    "#from numba import jit\n",
    "\n",
    "from sklearn.feature_selection import RFECV,RFE\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, cross_val_score\n",
    "\n",
    "from sklearn.metrics import make_scorer, explained_variance_score, mean_absolute_error, mean_squared_error, \\\n",
    "r2_score, max_error, median_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "crs_to='epsg:4326'\n",
    "crs_from='epsg:4326'\n",
    "projection = 'M5.4i'\n",
    "#parent directory\n",
    "\n",
    "DIR = Path().resolve() \n",
    "\n",
    "plt_params = {\n",
    "    'figure.titlesize' : 28,\n",
    "    \"axes.titlesize\" : 25, # main\n",
    "    \"axes.labelsize\" : 25,  # labels\n",
    "    \"axes.edgecolor\" : \"black\", \n",
    "    \"axes.linewidth\" : 1, \n",
    "    'xtick.labelsize': 20, # ticks\n",
    "    'ytick.labelsize': 20,\n",
    "    'legend.title_fontsize':17,\n",
    "    'legend.fontsize': 17,\n",
    "    'font.size': 20,\n",
    "}\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215924c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeabornFig2Grid():\n",
    "\n",
    "    def __init__(self, seaborngrid, fig,  subplot_spec):\n",
    "        self.fig = fig\n",
    "        self.sg = seaborngrid\n",
    "        self.subplot = subplot_spec\n",
    "        if isinstance(self.sg, sns.axisgrid.FacetGrid) or \\\n",
    "            isinstance(self.sg, sns.axisgrid.PairGrid):\n",
    "            self._movegrid()\n",
    "        elif isinstance(self.sg, sns.axisgrid.JointGrid):\n",
    "            self._movejointgrid()\n",
    "        self._finalize()\n",
    "\n",
    "    def _movegrid(self):\n",
    "        \"\"\" Move PairGrid or Facetgrid \"\"\"\n",
    "        self._resize()\n",
    "        n = self.sg.axes.shape[0]\n",
    "        m = self.sg.axes.shape[1]\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(n,m, subplot_spec=self.subplot)\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                self._moveaxes(self.sg.axes[i,j], self.subgrid[i,j])\n",
    "\n",
    "    def _movejointgrid(self):\n",
    "        \"\"\" Move Jointgrid \"\"\"\n",
    "        h= self.sg.ax_joint.get_position().height\n",
    "        h2= self.sg.ax_marg_x.get_position().height\n",
    "        r = int(np.round(h/h2))\n",
    "        self._resize()\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(r+1,r+1, subplot_spec=self.subplot)\n",
    "\n",
    "        self._moveaxes(self.sg.ax_joint, self.subgrid[1:, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_x, self.subgrid[0, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_y, self.subgrid[1:, -1])\n",
    "\n",
    "    def _moveaxes(self, ax, gs):\n",
    "        #https://stackoverflow.com/a/46906599/4124317\n",
    "        ax.remove()\n",
    "        ax.figure=self.fig\n",
    "        self.fig.axes.append(ax)\n",
    "        self.fig.add_axes(ax)\n",
    "        ax._subplotspec = gs\n",
    "        ax.set_position(gs.get_position(self.fig))\n",
    "        ax.set_subplotspec(gs)\n",
    "\n",
    "    def _finalize(self):\n",
    "        plt.close(self.sg.fig)\n",
    "        self.fig.canvas.mpl_connect(\"resize_event\", self._resize)\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def _resize(self, evt=None):\n",
    "        self.sg.fig.set_size_inches(self.fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "obs[\"OBS_REF\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2_CLASS\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST_W\", \"REG\", \"GLIM\"]\n",
    "\n",
    "obs[\"OBS_AFR\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV_Velocity\",\"PV_Velocity\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST\", \"REG\", \"GLIM\"]\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "\n",
    "\n",
    "# Labels for plots etc\n",
    "obs[\"LABELS_gmt\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"S@_v@ 150km\", \"P@_v@ 150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. rho\", \"Crust rho\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]  \n",
    "\n",
    "\n",
    "obs[\"LABELS\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"$S_v$ @150km\", \"$P_v$ @150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. ρ\", \"Crust ρ\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]\n",
    "    \n",
    "# \"vp/vs\"\n",
    "# Units to display in plots etc\n",
    "obs[\"UNITS\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"$\\delta$$S_v$ %\",\"$\\delta$$P_v$ %\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m$^3$\", \"kg/m$^3$\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "\n",
    "\n",
    "\n",
    "obs[\"UNITS_gmt\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"km/s\",\"km/s\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m@+3@+\", \"kg/m@+3@+\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs[\"V_RANGE\"] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,16),]\n",
    "\n",
    "\n",
    "    \n",
    "obs[\"V_RANGE_AFR\"] = [(0,50), (-1,1),(50,250),(20,50),\n",
    "          (-0.075,0.075), (-0.02,0.02), \n",
    "          (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-200, 200), \n",
    "               (3260, 3360), (2650, 2950),\n",
    "              (0,100), (1,6),(1,15),]\n",
    "\n",
    "\n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"categorical\", ]\n",
    "'''\n",
    "obs[\"CMAPS\"] = [\"SCM/bamako\",  \"SCM/broc\", \"SCM/bamako\", \"SCM/bamako\", \n",
    "             \"SCM/roma\",\"SCM/roma\", \n",
    "             \"SCM/bamako\", \"SCM/broc\", \"SCM/oleron\", \"SCM/broc\", \"SCM/bilbao\",            \n",
    "                \"SCM/batlow\", \"SCM/batlow\",\n",
    "               \"SCM/broc\",  \"gmt/categorical\",\"gmt/categorical\", ]'''\n",
    "\n",
    "\n",
    "new_index = [0,1,2,3,4,5,6,8,7,9,10,11,12,13,14,15]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "obs = obs.reindex(new_index)\n",
    "\n",
    "#obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n",
    "obs.set_index(['OBS_REF'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_afr = xr.load_dataset(DIR/'Grids'/'Inputs'/\"ds_afr.nc\")\n",
    "#ds_world = xr.load_dataset(dir_p/'Grids'/'inputs'/\"ds_world.nc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d31693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'GHF'\n",
    "coord = ['lon', 'lat']\n",
    "grid_index_world = 'grid_index_world'\n",
    "grid_index_afr ='grid_index_afr'\n",
    "\n",
    "hq_lower_bound = 10\n",
    "hq_upper_bound = 200\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "features_ghf = []\n",
    "\n",
    "\n",
    "\n",
    "features = obs.index.to_list()\n",
    "\n",
    "\n",
    "\n",
    "in_features = set(features)\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.append(grid_index_world)\n",
    "features_ex.append(grid_index_afr)\n",
    "\n",
    "features_ex.append(target)\n",
    "\n",
    "features_ghf = copy.deepcopy(features)\n",
    "features_ghf.append(target)\n",
    "\n",
    "\n",
    "features_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rating = 'rab'\n",
    "outlier = 'OD'\n",
    "\n",
    "\n",
    "file_label = f'{outlier}_{rating}'\n",
    "\n",
    "w_OD_rab_f =  DIR/'Dataset'/'Preprocessed'/f'W_OD_rab.csv'\n",
    "W_OD_rab = pd.read_csv(w_OD_rab_f, sep='\\t')\n",
    "\n",
    "#mask_afr = W_OD_rab['lon'].between(afr_lon_min, afr_lon_max)& W_OD_rab['lat'].between(afr_lat_min, afr_lat_max)\n",
    "#Afr_OD_rab = W_OD_rab[~mask_afr]\n",
    "\n",
    "Afr_OD_rab_f =  DIR/'Dataset'/'Preprocessed'/f'Afr_OD_rab.csv'\n",
    "Afr_OD_rab = pd.read_csv(Afr_OD_rab_f,  sep='\\t')\n",
    "\n",
    "\n",
    "Afr_NOD_rab_f =  DIR/'Dataset'/'Preprocessed'/f'Afr_NOD_rab.csv'\n",
    "Afr_NOD_rab = pd.read_csv(Afr_NOD_rab_f,sep='\\t')\n",
    "\n",
    "\n",
    "#######\n",
    "W_OD_rab['GLIM']  = W_OD_rab['GLIM'].astype('category')\n",
    "W_OD_rab['REG']  = W_OD_rab['REG'].astype('category')\n",
    "\n",
    "Afr_OD_rab['GLIM']  = Afr_OD_rab['GLIM'].astype('int').astype('category')\n",
    "Afr_OD_rab['REG']   = Afr_OD_rab['REG'].astype('int').astype('category')\n",
    "\n",
    "Afr_NOD_rab['GLIM']  = Afr_NOD_rab['GLIM'].astype('int').astype('category')\n",
    "Afr_NOD_rab['REG']   = Afr_NOD_rab['REG'].astype('int').astype('category')\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "X_w = W_OD_rab[features]\n",
    "y_w = W_OD_rab[target].values.reshape(-1,1) \n",
    "\n",
    "\n",
    "X_afr = Afr_OD_rab[features]\n",
    "y_afr = Afr_OD_rab[target].values.reshape(-1,1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bs_rfr_hyp =  DIR/'Hyperparameters'/f'BS_hyperparameter.csv'\n",
    "\n",
    "\n",
    "\n",
    "bs_rfr_hyp_df = pd.read_csv(bs_rfr_hyp, sep='\\t')\n",
    "\n",
    "\n",
    "best_params = bs_rfr_hyp_df.to_dict('r')[0]\n",
    "\n",
    "print(file_label)\n",
    "\n",
    "\n",
    "print(len(Afr_OD_rab))\n",
    "print(len(W_OD_rab))\n",
    "\n",
    "\n",
    "X_afr.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d87343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"Concordance correlation coefficient.\"\"\"\n",
    "    \n",
    "    y_true = y_true.ravel().reshape(-1,)\n",
    "    y_pred = y_pred.ravel().reshape(-1,)\n",
    "    # Remove NaNs\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    df = df.dropna()\n",
    "    y_true = df['y_true']\n",
    "    y_pred = df['y_pred']\n",
    "    # Pearson product-moment correlation coefficients\n",
    "    cor = np.corrcoef(y_true, y_pred)[0][1]\n",
    "    # Mean\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    # Variance\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    # Standard deviation\n",
    "    sd_true = np.std(y_true)\n",
    "    sd_pred = np.std(y_pred)\n",
    "    # Calculate CCC\n",
    "    numerator = 2 * cor * sd_true * sd_pred\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n",
    "    return numerator / denominator\n",
    "\n",
    "def nrmse(y_true, y_pred):\n",
    "    cost  = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    return cost/(y_true.mean()) \n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    cost  = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return (100 - cost) * 100\n",
    "\n",
    "def min_e(y_true, y_pred):\n",
    "    cost  = abs(y_test - y_predict)\n",
    "    return cost.min()\n",
    "\n",
    "def mpe(y_true, y_pred):\n",
    "    return (np.mean((y_test -y_predict)/y_test) , 2)\n",
    "\n",
    "\n",
    "rmse_score = make_scorer(mean_squared_error , squared=False, greater_is_better=False)\n",
    "\n",
    "scores_cv = {\n",
    "\n",
    "'RMSE'     :  rmse_score,\n",
    "#'NRMSE'    :  make_scorer(nrmse),\n",
    "'MAE'      :  make_scorer(mean_absolute_error),    \n",
    "#'MAPE'     :  make_scorer(mean_absolute_percentage_error ),\n",
    "#'ACC'      :  make_scorer(accuracy) ,\n",
    "#'MPE'      :  make_scorer(mpe),\n",
    "'CD'       :  make_scorer(r2_score),\n",
    "#'EV'       :  make_scorer(explained_variance_score),\n",
    "#'MAX_E'    :  make_scorer( max_error),\n",
    "#'MIN_E'    :  make_scorer(min_e),\n",
    "#'MedAE'    :  make_scorer(median_absolute_error),\n",
    "#'CCC'      :  make_scorer(concordance_correlation_coefficient),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cf0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rfecv this process is more stable\n",
    " \n",
    "KFOLD=3\n",
    "N_ITER=30\n",
    "RANDOM_STATE=42\n",
    "N_JOBS=1\n",
    "VERBOSE=2\n",
    "results_cv ={}\n",
    "\n",
    "\n",
    "CV = KFold(n_splits=KFOLD, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "\n",
    "#scoring = make_scorer(nrmse , greater_is_better=False )\n",
    "nrmse_score = make_scorer(nrmse , greater_is_better=False)\n",
    "#r2 = make_scorer(r2_score)\n",
    "#scoring = make_scorer(nrmse , greater_is_better=False )\n",
    "#ccc_score = make_scorer(concordance_correlation_coefficient , greater_is_better=True)\n",
    "\n",
    "scorings = {\n",
    "    'NRMSe':nrmse_score,\n",
    "    #'RMSE':'neg_root_mean_squared_error', \n",
    "    #'MAE':'neg_mean_absolute_error', \n",
    "    #'MAPE':'neg_mean_absolute_percentage_error', \n",
    "    'R2':'r2',\n",
    "    #'EV':'explained_variance'   \n",
    "    #'$p_c$': ccc_score,    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tuned_params = {item.replace('ttr__regressor__', 'rfecv__estimator__'): best_params[item] for item in best_params}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key, score in scorings.items():\n",
    "    \n",
    "    regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "    numeric_transformer = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "       (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    rfecv = RFECV(estimator=regressor, cv=CV, step=1, n_jobs=N_JOBS, \n",
    "                             scoring=score, verbose=VERBOSE)\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    steps=[(\"preprocessor\", preprocessor), \n",
    "           (\"rfecv\", rfecv)]\n",
    "\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline= Pipeline(steps=steps)\n",
    "    pipeline.set_params(**tuned_params)\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize RFECV object\n",
    "    #feature_selector = RFECV(pipeline['regressor'], cv = cv, step = 1, #n_jobs=-1, \n",
    "    #                         scoring = score, verbose = 1)\n",
    "    \n",
    "\n",
    "    # Fit RFECV\n",
    "    pipeline.fit(X_afr, y_afr)\n",
    "    #feature_selector .fit(X_w, np.ravel(y_w))\n",
    "    results_cv[key] = pipeline\n",
    "\n",
    "    # Get selected features\n",
    "    #feature_names = X_train_corr.columns\n",
    "    #selected_features = feature_names[feature_selector.support_].tolist()\n",
    "\n",
    "    print(f'terminated {key} {score}\\n')\n",
    "    print('terminated')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a366f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get RFECV progression score\n",
    "score_progression = pd.DataFrame()\n",
    "score_progression[\"NRMSe_RFECV\"] = results_cv['NRMSe']['rfecv'].cv_results_['mean_test_score'].round(3) * -1\n",
    "score_progression[\"R2_RFECV\"] = results_cv['R2']['rfecv'].cv_results_['mean_test_score'].round(3)\n",
    "score_progression[\"n_features\"]= list(range(1,17))\n",
    "score_progression = score_progression.set_index(\"n_features\")\n",
    "score_progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance \n",
    "# Get selected features data set\n",
    "# should be scaled\n",
    "\n",
    "sub_figs = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "# Set figure size and create barplot\n",
    "#sns.set(style=\"whitegrid\", color_codes=True, font_scale = 1.8)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(27, 6))\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "\n",
    "\n",
    "feature_importance = pd.DataFrame()\n",
    " \n",
    "\n",
    "# Load hyper parameter \n",
    "\n",
    "numeric_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "           (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "           (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ], \n",
    "    #remainder='passthrough', verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "steps=[(\"preprocessor\", preprocessor), \n",
    "       #(\"regressor\", regressor), \n",
    "       ('ttr', TransformedTargetRegressor(regressor=regressor, transformer=numeric_transformer))\n",
    "      ]\n",
    "\n",
    "# Initialize Pipeline object\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "pipeline.set_params(**best_params)\n",
    "\n",
    "\n",
    "pipeline.fit(X_afr,y_afr)\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "feature_importance[\"RI\"] = pipeline['ttr'].regressor_.feature_importances_.round(2)\n",
    "feature_importance[\"LABELS\"]= obs.loc[features , 'LABELS'].values\n",
    "feature_importance[\"OBS_REF\"] = obs.index\n",
    "feature_importance.set_index('LABELS', inplace=True)\n",
    "\n",
    "\n",
    "permutaion_ranking = permutation_importance(pipeline, X_afr, y_afr, n_repeats=10,\n",
    "                                    scoring=rmse_score, random_state=RANDOM_STATE)\n",
    "perm_sorted_idx = permutaion_ranking.importances_mean.argsort()\n",
    "\n",
    "\n",
    "obs.reset_index(drop=False).set_index('OBS_REF', inplace=True)\n",
    "\n",
    "# Get feature importance\n",
    "\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance[\"PFI\"] = permutaion_ranking.importances_mean\n",
    "\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by=\"RI\", ascending=True)\n",
    "ax1.barh(feature_importance.index, feature_importance['RI'], height=0.7)\n",
    "\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by=\"PFI\", ascending=True)\n",
    "ax2.barh(feature_importance.index, feature_importance['PFI'], height=0.7)\n",
    "\n",
    "ax3.boxplot(\n",
    "    permutaion_ranking.importances[perm_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=obs.loc[X_afr.iloc[:,perm_sorted_idx].columns , 'LABELS'].values,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig(DIR/'fig'/f\"fig_8_PFI_rab.jpg\", bbox_inches='tight', dpi=300 )\n",
    "\n",
    "\n",
    "# Save Figure\n",
    "#fig.savefig(dir_p/ 'fig'/\"fig_5.jpeg\", bbox_inches='tight', dpi=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db181328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual RFECV to calculate conribution of each feature to R2 and NRMSe\n",
    "\n",
    "for feature in obs.index:\n",
    "\n",
    "    print(feature)\n",
    "\n",
    "\n",
    "    X_train_afr_feature = X_afr[feature].to_frame()\n",
    "\n",
    "    numeric_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "               (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "               (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ], \n",
    "        #remainder='passthrough', verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "\n",
    "    regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "\n",
    "    steps=[(\"preprocessor\", preprocessor), \n",
    "           #(\"regressor\", regressor), \n",
    "           ('ttr', TransformedTargetRegressor(regressor=regressor, transformer=numeric_transformer))\n",
    "          ]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "\n",
    "\n",
    "    for key, score in scorings.items():\n",
    "        feature_importance.loc[obs.loc[feature, 'LABELS'], key] = np.mean(cross_validate(\n",
    "            pipeline, \n",
    "            X_train_afr_feature, \n",
    "            y_afr,\n",
    "            scoring=score,\n",
    "              cv=CV)['test_score'])\n",
    "\n",
    "\n",
    "feature_importance[['RI', 'PFI', 'NRMSe', 'R2']] = feature_importance[['RI', 'PFI', 'NRMSe', 'R2']].abs() \n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance.sort_values(by=\"RI\", ascending=False)\n",
    "\n",
    "feature_importance[\"CUMSUM\"] = feature_importance[\"RI\"].cumsum()\n",
    "score_progression[\"CUMSUM\"] = feature_importance[\"CUMSUM\"].values\n",
    "\n",
    "mask_RI_NRMSE = (feature_importance['RI'] < feature_importance['NRMSe']) & (feature_importance['NRMSe'] < feature_importance['R2'])\n",
    "\n",
    "mask_NRMSE_RI = (feature_importance['NRMSe'] < feature_importance['RI']) & (feature_importance['RI'] < feature_importance['R2']) \n",
    "    \n",
    "mask_R2_RI = (feature_importance['R2'] < feature_importance['RI']) & (feature_importance['RI'] < feature_importance['NRMSe'])\n",
    "\n",
    "\n",
    "mask_RI_R2 = (feature_importance['RI'] < feature_importance['R2']) & (feature_importance['R2'] < feature_importance['NRMSe'])\n",
    "\n",
    "mask_NRMSE_R2 = (feature_importance['NRMSe'] < feature_importance['R2']) & (feature_importance['R2'] < feature_importance['RI']) \n",
    "\n",
    "\n",
    "mask_R2_NRMSE = (feature_importance['R2'] < feature_importance['NRMSe']) & (feature_importance['NRMSe'] < feature_importance['RI'])\n",
    "\n",
    "\n",
    "\n",
    "#feature importance \n",
    "# Get selected features data set\n",
    "# should be scaled\n",
    "fig, axes = plt.subplots(1,2, figsize=(19,8),gridspec_kw={'width_ratios': [3, 2.5]})\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "\n",
    "axes.ravel()\n",
    "#sns.set(style=\"whitegrid\", color_codes=True, font_scale = 3)\n",
    "\n",
    "\n",
    "\n",
    "###1\n",
    "if mask_RI_NRMSE.any():\n",
    "    sns.barplot(x = feature_importance['R2'].values * mask_RI_NRMSE.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='lightblue',\n",
    "                )\n",
    "\n",
    "\n",
    "    sns.barplot(x = feature_importance['NRMSe'].values * mask_RI_NRMSE.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "    sns.barplot(x = feature_importance[\"RI\"].values * mask_RI_NRMSE.astype(int).values, \n",
    "                y = feature_importance.index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "\n",
    "###2\n",
    "if mask_NRMSE_RI.any():\n",
    "    sns.barplot(x = feature_importance['R2'].values * mask_NRMSE_RI.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='lightblue',\n",
    "                )\n",
    "\n",
    "\n",
    "    sns.barplot(x = feature_importance[\"RI\"].values * mask_NRMSE_RI.astype(int).values, \n",
    "                y = feature_importance.index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "\n",
    "    sns.barplot(x = feature_importance['NRMSe'].values * mask_NRMSE_RI.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "    \n",
    "\n",
    "###3\n",
    "if mask_R2_RI.any():\n",
    "    sns.barplot(x = feature_importance['NRMSe'].values * mask_R2_RI.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "\n",
    "\n",
    "    sns.barplot(x = feature_importance[\"RI\"].values * mask_R2_RI.astype(int).values, \n",
    "                y = feature_importance.index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "\n",
    "\n",
    "    \n",
    "    sns.barplot(x = feature_importance['R2'].values * mask_R2_RI.astype(int).values, \n",
    "            y = feature_importance.index, \n",
    "            ax=axes[0], color='lightblue',\n",
    "            )\n",
    "    \n",
    "\n",
    "###4\n",
    "if mask_RI_R2.any():\n",
    "    \n",
    "    sns.barplot(x = feature_importance['NRMSe'].values * mask_RI_R2.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "    \n",
    "    sns.barplot(x = feature_importance['R2'].values * mask_RI_R2.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='lightblue',\n",
    "                )\n",
    "\n",
    "    sns.barplot(x = feature_importance[\"RI\"].values * mask_RI_R2.astype(int).values, \n",
    "                y = feature_importance.index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "\n",
    "###5\n",
    "    \n",
    "if mask_NRMSE_RI.any():\n",
    "    sns.barplot(x = feature_importance['R2'].values * mask_NRMSE_RI.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='lightblue',\n",
    "                )\n",
    "\n",
    "\n",
    "    sns.barplot(x = feature_importance[\"RI\"].values * mask_NRMSE_RI.astype(int).values, \n",
    "                y = feature_importance.index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "\n",
    "    sns.barplot(x = feature_importance['NRMSe'].values * mask_NRMSE_RI.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "    \n",
    "####6\n",
    "if mask_R2_RI.any():\n",
    "    \n",
    "   \n",
    "    sns.barplot(x = feature_importance['NRMSe'].values * mask_R2_RI.astype(int).values, \n",
    "                y = feature_importance.index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "    sns.barplot(x = feature_importance[\"RI\"].values * mask_R2_RI.astype(int).values, \n",
    "                y = feature_importance.index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "    \n",
    "    sns.barplot(x = feature_importance['R2'].values * mask_R2_RI.astype(int).values, \n",
    "            y = feature_importance.index, \n",
    "            ax=axes[0], color='lightblue',\n",
    "            )\n",
    "    \n",
    "    \n",
    "### optimal\n",
    "\n",
    "'''\n",
    "if mask_RI_NRMSE.any():\n",
    "    sns.barplot(x = feature_importance.loc[mask_RI_NRMSE[0:11], 'R2'].values, \n",
    "                y = feature_importance.loc[mask_RI_NRMSE[0:11], 'R2'].index, \n",
    "                ax=axes[0], color='lightblue',\n",
    "                )\n",
    "\n",
    "\n",
    "    sns.barplot(x = feature_importance.loc[mask_RI_NRMSE[0:11],'NRMSe'].values, \n",
    "                y = feature_importance.loc[mask_RI_NRMSE[0:11],'NRMSe'].index, \n",
    "                ax=axes[0], color='orange', label='NRMSe',\n",
    "                )\n",
    "    sns.barplot(x = feature_importance.loc[mask_RI_NRMSE[0:11],\"RI\"].values, \n",
    "                y = feature_importance.loc[mask_RI_NRMSE[0:11],\"RI\"].index,  \n",
    "                ax=axes[0],  color='lightgreen',label=f\"\"\"Relative\n",
    "    importance\"\"\"\n",
    "                        )\n",
    "    \n",
    "''' \n",
    "\n",
    "\n",
    "axes[0].set_ylabel('')\n",
    "axes[0].grid(False)\n",
    "\n",
    "\n",
    "\n",
    "#axes[0].patch.set_edgecolor('black')  \n",
    "#axes[0].patch.set_linewidth('3') \n",
    "# Hide the right and top spines\n",
    "axes[0].spines.right.set_visible(False)\n",
    "axes[0].spines.top.set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "axes[0].yaxis.set_ticks_position('left')\n",
    "axes[0].xaxis.set_ticks_position('bottom')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes[0].locator_params(axis='x', nbins=7)\n",
    "#axes[0].legend(loc=[.55, .6], framealpha=0.5)\n",
    "\n",
    "axes[0].set_title(f'{sub_figs[0]})   Ranking of Observables', loc ='left', pad=10, size=25, x=-0.05, y=1.05)\n",
    "axes[0].set_xlabel(\"Normalized Score\")\n",
    "\n",
    "markersize = 12\n",
    "linewidth = 3\n",
    "for key, score in scorings.items():\n",
    "    #selected_features = feature_names[results[key].support_].tolist()\n",
    "    # Get Performance Data\n",
    "    #print(f\"Optimal number of features for {key} : {results[key].n_features_}\")\n",
    "    if key == 'NRMSe':\n",
    "        #results[key].support_rfecv_df = pd.DataFrame(results[key].ranking_,index=X_afr_lr.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "\n",
    "        axes[1].plot(score_progression.index, \n",
    "                 score_progression['NRMSe_RFECV'], color='orange', linewidth=linewidth,\n",
    "                   marker=\"^\", label='$NRMSe$', markersize=markersize,)\n",
    "\n",
    "\n",
    "    else:\n",
    "        axes[1].plot(score_progression.index,\n",
    "                 score_progression['R2_RFECV'], color='lightblue', \n",
    "                     linewidth=linewidth, marker=\"d\",  label='$R^2$', markersize=markersize,\n",
    "                       )\n",
    "\n",
    "\n",
    "axes[1].plot(score_progression.index, \n",
    "         score_progression['CUMSUM'], linewidth=linewidth, \n",
    "            color='lightgreen',  marker=\"o\", \n",
    "             label=f'''Importance''', markersize=markersize,)\n",
    "\n",
    "\n",
    "axes[1].set_xlabel(\"Number of observables\")\n",
    "\n",
    "axes[1].set_ylabel(\"Normalized Score\")\n",
    "axes[1].legend(loc=[.45, .23], frameon=False, fontsize=22, )\n",
    "#axes[1].legend(loc=[.01, .7])\n",
    "#plt.axesvline(results[key].n_features_ ,color='r')\n",
    "axes[1].set_title(f'{sub_figs[1]})   Progression of Scores and Misfit', loc ='left', pad=10, size=25,  x=-0.05, y=1.05)\n",
    "axes[1].locator_params(axis='y', nbins=10)\n",
    "axes[1].locator_params(axis='x', nbins=9)\n",
    "axes[1].grid(False)\n",
    "\n",
    "axes[1].spines.right.set_visible(False)\n",
    "axes[1].spines.top.set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "axes[1].yaxis.set_ticks_position('left')\n",
    "axes[1].xaxis.set_ticks_position('bottom')\n",
    "\n",
    "\n",
    "plt_params = {\n",
    "    #'figure.titlesize' : 28,\n",
    "    \"axes.titlesize\" : 22, # main\n",
    "    \"axes.labelsize\" : 22,  # labels\n",
    "    \"axes.edgecolor\" : \"black\", \n",
    "    \"axes.linewidth\" : 1.2, \n",
    "    'xtick.labelsize': 22, # ticks\n",
    "    'ytick.labelsize': 22,\n",
    "    #'legend.fontsize': 20,\n",
    "     'legend.handlelength': 5,\n",
    "    #'legend.title_fontsize':5,\n",
    "    #'legend.fontsize': 5,\n",
    "    #'font.size': 18,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Save Figure\n",
    "#fig.savefig(dir_p/'fig'/'presentation'/\"fig_p6d.jpeg\", bbox_inches='tight', dpi=300 )\n",
    "fig.savefig(DIR/'fig'/\"fig_5.pdf\", bbox_inches='tight', dpi=300 )\n",
    "fig.savefig(DIR/'fig'/\"fig_5.jpg\", bbox_inches='tight', dpi=300 )\n",
    "\n",
    "####\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6431a",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target values\n",
    "AFR_grid = pd.DataFrame({'X': ds_afr.XV.values.ravel(), 'Y': ds_afr.YV.values.ravel()})\n",
    "for feature in tqdm_notebook(features ,  desc=f'Processing: ', leave=False ):\n",
    "    sleep(0.01)\n",
    "    AFR_grid[feature] = ds_afr[feature].values.ravel()\n",
    "\n",
    "AFR_grid['GLIM']  = AFR_grid['GLIM'].astype('int').astype('category')\n",
    "AFR_grid['REG']  = AFR_grid['REG'].astype('int').astype('category')\n",
    "AFR_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9631b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "pipeline_dict = {}\n",
    "grid_dict = {}\n",
    "best_features = [ 16, 15, 14, 13, 12, 11,10,\n",
    "                 9,8,7,6,5,4\n",
    "                 ]\n",
    "\n",
    "labels = ['RFE_16', 'RFE_15', 'RFE_14', 'RFE_13', 'RFE_12', 'RFE_11', 'RFE_10', 'RFE_09', 'RFE_08', 'RFE_07', \n",
    "    'RFE_06', 'RFE_05', 'RFE_04', \n",
    "]\n",
    "\n",
    "#tuned_params = {item.replace('ttr__regressor__', 'ttr__regressor__estimator__'): best_params[item] for item in best_params}\n",
    "\n",
    "for best_feature, label in tqdm_notebook(\n",
    "    zip(best_features, labels), total=len(labels), desc='Training: '):\n",
    "    \n",
    "    print(best_feature)\n",
    "    \n",
    "    # this step gurantee consistency in feature ranking\n",
    "    feature_importance.sort_values(by=\"RI\", ascending=False, inplace=True)\n",
    "    features_reduced = feature_importance['OBS_REF'].values[0:best_feature]\n",
    "    \n",
    "    numeric_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "               (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "               (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ], \n",
    "        #remainder='passthrough', verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "\n",
    "    regressor = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "    steps=[(\"preprocessor\", preprocessor), \n",
    "           #(\"regressor\", regressor), \n",
    "           ('ttr', TransformedTargetRegressor(regressor=regressor, transformer=numeric_transformer))\n",
    "          ]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "\n",
    "  \n",
    "    trained_model = pipeline.fit(X_afr[features_reduced], y_afr )\n",
    "    \n",
    "   \n",
    "\n",
    "    ######### save\n",
    "\n",
    "   \n",
    "    pipeline_dict[f'{label}'] = trained_model\n",
    "    grid_dict[f'{label}'] = AFR_grid[features_reduced]\n",
    "    \n",
    "\n",
    "    print(f'RFE_{best_feature} fitting is terminated' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.sort_values(by=\"RI\", ascending=False, inplace=True)\n",
    "RF_features_df = pd.DataFrame({'Labels':feature_importance.index})\n",
    "RF_features_df['FI'] = feature_importance['RI'].values\n",
    "\n",
    "for key in pipeline_dict.keys(): \n",
    "    RF_features_df[f'{key}'] = pd.Series(pipeline_dict[key]['ttr'].regressor_.feature_importances_).round(2).sort_values()\n",
    "RF_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = {}\n",
    "xs = range(len(ds_afr.X))\n",
    "ys = range(len(ds_afr.Y))\n",
    "nn = (len(ds_afr.Y), len( ds_afr.X))\n",
    "\n",
    "\n",
    "hq_gt = Afr_OD_rab[[grid_index_afr, target]].set_index(grid_index_afr)\n",
    "hq_gt.index.names = ['index']\n",
    "\n",
    "for key, pipeline  in tqdm_notebook(pipeline_dict.items() , \n",
    "                                            desc=f'Modelling: '):\n",
    "    print(key)\n",
    "    pipeline = pipeline_dict[key]\n",
    "    AFR_grid = grid_dict[key]\n",
    "    AFR_Q_RFR = np.zeros(nn) # predicted HF value\n",
    "    predictions_df[key] = pd.DataFrame({'X': ds_afr.XV.values.ravel(), 'Y': ds_afr.YV.values.ravel()})\n",
    "\n",
    "    predictions_df[key]['Prediction'] = pipeline.predict(AFR_grid).reshape(-1,1)\n",
    "    predictions_df[key].index.names = ['index']\n",
    "    final_df = pd.merge(predictions_df[key], hq_gt,  how=\"left\", on=\"index\")\n",
    "    final_df.to_csv(DIR/'Grids'/'Outputs'/f'{key}_rab.xyz' , index=False, header=True, sep='\\t')\n",
    "    ds_afr[key] = (('Y', 'X'), predictions_df[key]['Prediction'].values.reshape(nn) )\n",
    "    \n",
    "ds_afr.to_netcdf(DIR/'Grids'/'Outputs'/\"ds_afr_rab.nc\", mode='w', \n",
    "                    engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1500a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    diff = ds_afr['RFE_16'] - ds_afr[label]\n",
    "    print(f'Difference between 16 and {label}: {np.round(diff.mean(), 2)}')\n",
    "print('##'*10)  \n",
    "for label in labels:\n",
    "    diff = ds_afr['RFE_11'] - ds_afr[label]\n",
    "    print(f'Difference between 11 and {label}: {np.round(diff.mean(), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef010e9",
   "metadata": {},
   "source": [
    "# RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4943c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_figs = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "#create two datsets for A ratibng and b ratings\n",
    "hq_f = DIR / 'Dataset'/ 'References'/'q_Heat_Flow'/'NGHF.csv'\n",
    "\n",
    "elev_cut = -1000\n",
    "\n",
    "record_total = pd.read_csv(hq_f)\n",
    "\n",
    "record_total = record_total.rename(columns={'heat-flow (mW/m2)': target, 'longitude': 'lon', 'latitude': 'lat'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hq_afr_lat = record_total[record_total['lat'].between(afr_lat_min, afr_lat_max, inclusive='both')]\n",
    "record_total_afr = hq_afr_lat[hq_afr_lat['lon'].between(afr_lon_min, afr_lon_max, inclusive='both')]\n",
    "\n",
    "\n",
    "\n",
    "hq_no_pole_afr = record_total_afr.dropna(subset = ['lon', 'lat', target])\n",
    "\n",
    "hq_no_pole_afr = hq_no_pole_afr[hq_no_pole_afr[target].between(hq_lower_bound, hq_upper_bound, inclusive='both')]\n",
    "\n",
    "\n",
    "#hq_no_pole_afr = hq_clean[hq_clean['lat'].between(world_lat_min, world_lat_max, inclusive='both')]\n",
    "hq_deep_afr = hq_no_pole_afr[(hq_no_pole_afr['elevation (m)']>elev_cut)][['lon', 'lat',target]]\n",
    "\n",
    "\n",
    "\n",
    "hq_final_a_afr = hq_no_pole_afr[(hq_no_pole_afr ['code6']=='A') & (hq_no_pole_afr ['elevation (m)']>elev_cut)][['lon', 'lat',target]]\n",
    "hq_final_b_afr= hq_no_pole_afr[(hq_no_pole_afr ['code6']=='B') & (hq_no_pole_afr ['elevation (m)']>elev_cut)][['lon', 'lat',target]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_feature = 11\n",
    "\n",
    "obs = obs.reset_index(drop=False).set_index('LABELS')\n",
    "\n",
    "reduced_datsets = obs.loc[feature_importance.iloc[0:Best_feature, 0].index, :]\n",
    "reduced_datsets = reduced_datsets.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs = obs.reset_index(drop=False).set_index('OBS_AFR')\n",
    "\n",
    "reduced_datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad98a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22acf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#proj=f\"l135/-25/-45/-10/2c\"\n",
    "region_gmt= [afr_lon_min, afr_lon_max,afr_lat_min, afr_lat_max]\n",
    "projection = 'M5.4i'\n",
    "\n",
    "\n",
    "frames = ['wNes', 'wNes','wNEs','Wnes', 'wnes', 'wnes','wnEs','WneS', 'wneS', 'wneS','wnES','wnES']\n",
    "\n",
    "fig = pygmt.Figure()\n",
    "\n",
    "\n",
    "\n",
    "pygmt.config(\n",
    "FONT='30p',\n",
    "MAP_TICK_PEN='1p',\n",
    "FONT_ANNOT='26p',\n",
    "#MAP_GRID_PEN  ,\n",
    "#MAP_TICK_LENGTH\n",
    "#MAP_TICK_PEN\n",
    ")\n",
    "\n",
    "#pygmt.config(FONT='30p')\n",
    "\n",
    "with fig.subplot(\n",
    "    nrows=6,\n",
    "    ncols=4,\n",
    "    #figsize=(\"30c\", \"15c\"),  # width of 15 cm, height of 6 cm\n",
    "    subsize = (\"13.5c\", \"13.5c\"),\n",
    "    autolabel=['A)+o0.3/-1.5'],\n",
    "    margins=[\"0.3c\", \"3.5c\"],  # horizontal 0.3 cm and vertical 0.2 cm margins\n",
    "    #title=model[3:],\n",
    "    #sharex=\"bt\",  # shared x-axis on the bottom side\n",
    "    sharey=True,  # shared y-axis on the left side\n",
    "    #frame=\"WSrt\",\n",
    "):\n",
    "\n",
    "    with fig.set_panel(panel=0):\n",
    "\n",
    "        cmap = pygmt.makecpt(\n",
    "                cmap=DIR/'GMT'/'temperature.cpt', #temp 19lev\n",
    "            #cmap='lajolla',\n",
    "                series='30/130',\n",
    "                #truncate = '40/140',\n",
    "                #continuous=True,\n",
    "                reverse=False,\n",
    "            )\n",
    "\n",
    "        fig.basemap(region=region_gmt, projection=projection, frame='WNes', panel=0)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        fig.coast(\n",
    "            projection=projection,\n",
    "            #shorelines=0.5,\n",
    "            water=\"lightblue\", \n",
    "            shorelines=\"0.1p,black\",\n",
    "            borders=[\"1/0.001p,black\"],\n",
    "            lakes=\"lightblue\",\n",
    "            rivers=\"lightblue\" ,\n",
    "            land='darkgrey'\n",
    "            #borders=[\"1/0.5p,black\"],\n",
    "            #water='white',\n",
    "            \n",
    "            )\n",
    "\n",
    "        fig.plot(x=Afr_OD_rab.lon, y=Afr_OD_rab.lat,  cmap=True, projection=projection,\n",
    "             color=Afr_OD_rab[target], #label=f\"'A Rating'\",\n",
    "                      pen=\"0.01p,darkgrey\", style=\"c0.21c\")\n",
    "        fig.colorbar(frame=[\"af\", f\"x+lGHF\\t\\t[mW/m@+2@+]\"], \n",
    "                     position=f\"g{str(afr_lon_min-1)}/{str(afr_lat_min-10)}+w12.5c/0.5c+h+e\")\n",
    "    \n",
    "\n",
    "    for  cmap_i, grid_label_i, v_range_i, label_i ,unit_i, importance, panel,  in zip(\n",
    "         reduced_datsets['CMAPS'],  reduced_datsets['OBS_AFR'],\n",
    "                    reduced_datsets['V_RANGE_AFR'], reduced_datsets['LABELS_gmt'],\n",
    "        reduced_datsets['UNITS_gmt'], feature_importance['RI'],\n",
    "        list(range(len(reduced_datsets)))):\n",
    "\n",
    "        \n",
    "        with fig.set_panel(panel=panel+1):\n",
    "            if cmap_i =='bilbao':\n",
    "                pygmt.makecpt(\n",
    "                cmap=cmap_i, #temp 19lev\n",
    "                series=f'{v_range_i[0]}/{v_range_i[1]}',\n",
    "                #continuous=True,\n",
    "                reverse=True,\n",
    "                )\n",
    "            elif grid_label_i in ['REG', 'GLIM']:\n",
    "                pygmt.makecpt(\n",
    "                cmap=cmap_i, #temp 19lev\n",
    "                series=f'{v_range_i[0]}/{v_range_i[1]}/1',\n",
    "                #categorical=True,\n",
    "                #continuous=True,\n",
    "                )\n",
    "            elif grid_label_i in ['SV_Velocity', 'PV_Velocity']:\n",
    "                pygmt.makecpt(\n",
    "                cmap=cmap_i, #temp 19lev\n",
    "                series=f'{ds_afr[grid_label_i].min().values}/{ds_afr[grid_label_i].max().values}',\n",
    "                #continuous=True,\n",
    "                reverse=False,\n",
    "                )\n",
    "            elif grid_label_i == 'EMAG2':\n",
    "                pygmt.makecpt(\n",
    "                cmap=cmap_i, #temp 19lev\n",
    "                series=f'{v_range_i[0]}/{v_range_i[1]}',\n",
    "                #continuous=True,\n",
    "                reverse=False,\n",
    "                )\n",
    "            else:\n",
    "                pygmt.makecpt(\n",
    "                cmap=cmap_i, #temp 19lev\n",
    "                series=f'{v_range_i[0]}/{v_range_i[1]}',\n",
    "                #continuous=True,\n",
    "                reverse=False,\n",
    "                )\n",
    "                \n",
    "\n",
    "            fig.basemap(region=region_gmt, projection=projection, frame=frames[panel], panel=panel+1)\n",
    "\n",
    "\n",
    "\n",
    "            fig.grdimage(\n",
    "                 grid=ds_afr[grid_label_i], # xarray.DataArray containing VSV values\n",
    "                 region=region_gmt,\n",
    "                 projection=projection,\n",
    "                     cmap=cmap,\n",
    "\n",
    "                #shading='+a45+nt0.5'\n",
    "                #shading=dgrid\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            fig.coast(\n",
    "                projection=projection,\n",
    "                #shorelines=0.5,\n",
    "                water=\"lightblue\", \n",
    "                shorelines=\"0.1p,black\",\n",
    "                borders=[\"1/0.001p,black\"],\n",
    "                lakes=\"lightblue\",\n",
    "                rivers=\"lightblue\" ,\n",
    "                #borders=[\"1/0.5p,black\"],\n",
    "                #water='white',\n",
    "                )\n",
    "\n",
    "            #print(f'{importance} {label_i}')\n",
    "            #fig.colorbar(frame=[\"af\", f'x+l\"{label_i}\"\\t\\t\"[{unit_i}]\"\\t\\t({str(round(importance*100,1))}\\%)',\n",
    "            fig.colorbar(frame=[\"afg+el\",f'x+l\"{label_i}\"\\t\\t\"[{unit_i}]\"',],\n",
    "                    position=f\"g{str(afr_lon_min-1)}/{str(afr_lat_min-10)}+w12.5c/0.5c+h+e\")\n",
    "            \n",
    "\n",
    " \n",
    "\n",
    "fig.show(width=800)\n",
    "\n",
    "\n",
    "fig.savefig(DIR/'fig'/\"fig_3.pdf\", dpi=300 )\n",
    "fig.savefig(DIR/'fig'/\"fig_3.jpg\", dpi=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49659f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433427b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
