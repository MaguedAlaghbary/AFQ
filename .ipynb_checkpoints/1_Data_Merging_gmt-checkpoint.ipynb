{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439b1091",
   "metadata": {},
   "source": [
    "# 1-Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536a619",
   "metadata": {},
   "source": [
    "import necesaary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0841861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "import os, sys, pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray, save, load\n",
    "\n",
    "\n",
    "from scipy import stats, interpolate, spatial, io\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Arc \n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import pyproj as proj\n",
    "from pyproj import Transformer\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "# or: requests.get(url).content\n",
    "\n",
    "import pygmt \n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "import pooch\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from rasterio.warp import Resampling\n",
    "\n",
    "\n",
    "import AFQ_utils as utils\n",
    "\n",
    "from AFQ_utils import Invdisttree\n",
    "\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "crs_to='epsg:4326'\n",
    "crs_from='epsg:4326'\n",
    "projection = 'M5.4i'\n",
    "#parent directory\n",
    "\n",
    "DIR = Path().resolve() \n",
    "\n",
    "#constants\n",
    "km = 1000\n",
    "milli = 0.001\n",
    "micro = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "# create grid for each region\n",
    "# crs Coordinate reference system\n",
    "\n",
    "#EPSG is projection\n",
    "# 0.2 degrees equal roughly 20 km\n",
    "\n",
    "region_afr = [afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max]\n",
    "region_world = [world_lon_min, world_lon_max, world_lat_min, world_lat_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# create necessray folders\n",
    "\n",
    "path = os.path.join(DATA, 'Fig')\n",
    "os.mkdir(path)\n",
    "\n",
    "path = os.path.join(DATA, 'Grids')\n",
    "os.mkdir(path)\n",
    "\n",
    "path = os.path.join(DATA, 'Grids', 'Inputs')\n",
    "os.mkdir(path)\n",
    "\n",
    "path = os.path.join(DATA, 'Grids', 'Outputs')\n",
    "os.mkdir(path)\n",
    "\n",
    "path = os.path.join(DATA, 'Hyperparameters')\n",
    "os.mkdir(path)\n",
    "\n",
    "\n",
    "path = os.path.join(DATA, 'KPI')\n",
    "os.mkdir(path)\n",
    "\n",
    "\n",
    "path = os.path.join(DATA, 'Datasets')\n",
    "os.mkdir(path)\n",
    "\n",
    "path = os.path.join(DATA, 'Datasets', 'Preprocessed')\n",
    "os.mkdir(path)\n",
    "\n",
    "path = os.path.join(DATA, 'Datasets', 'References')\n",
    "os.mkdir(path)\n",
    "\n",
    "\n",
    "references = ['a_Moho_Depth', 'b_LAB', 'c_Distance_to_Nearest_Volcano', 'd_SV', 'e_PV',\n",
    "              'f_CTD', 'g_Magnetic', 'h_DEM', 'i_Crustal_Rho', 'i_Lithospheric_Rho', 'k_Free_Air',\n",
    "              'l_Geoid', 'm_Bouguer','n_Shape_Index', 'o_Tectonic_Regionalization', \n",
    "              'p_GLiM', 'q_Heat_Flow']\n",
    "\n",
    "for reference in references :\n",
    "    path = os.path.join(DATA, 'Datasets', 'References', reference)\n",
    "    os.mkdir(path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4182eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "obs[\"OBS_REF\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2_CLASS\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST_W\", \"REG\", \"GLIM\"]\n",
    "\n",
    "obs[\"OBS_AFR\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV_SPEED\",\"PV_SPEED\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST\", \"REG\", \"GLIM\"]\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "\n",
    "\n",
    "# Labels for plots etc\n",
    "obs[\"LABELS_gmt\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"S@_v@ 150km\", \"P@_v@ 150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. rho\", \"Crust rho\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]  \n",
    "\n",
    "\n",
    "obs[\"LABELS\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"$S_v$ @150km\", \"$P_v$ @150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. ρ\", \"Crust ρ\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]\n",
    "    \n",
    "# \"vp/vs\"\n",
    "# Units to display in plots etc\n",
    "obs[\"UNITS\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"$\\delta$$S_v$ %\",\"$\\delta$$P_v$ %\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m$^3$\", \"kg/m$^3$\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "\n",
    "\n",
    "\n",
    "obs[\"UNITS_gmt\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"km/s\",\"km/s\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m@+3@+\", \"kg/m@+3@+\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs[\"V_RANGE\"] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,16),]\n",
    "\n",
    "\n",
    "    \n",
    "obs[\"V_RANGE_AFR\"] = [(0,50), (-1,1),(50,250),(20,50),\n",
    "          (-0.075,0.075), (-0.02,0.02), \n",
    "          (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-200, 200), \n",
    "               (3260, 3360), (2650, 2950),\n",
    "              (0,100), (1,6),(1,15),]\n",
    "\n",
    "\n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"categorical\", ]\n",
    "\n",
    "obs[\"CMAPS\"] = [\"SCM/bamako\",  \"SCM/broc\", \"SCM/bamako\", \"SCM/bamako\", \n",
    "             \"SCM/roma\",\"SCM/roma\", \n",
    "             \"SCM/bamako\", \"SCM/broc\", \"SCM/oleron\", \"SCM/broc\", \"SCM/bilbao\",            \n",
    "                \"SCM/batlow\", \"SCM/batlow\",\n",
    "               \"SCM/broc\",  \"gmt/categorical\",\"gmt/categorical\", ]\n",
    "\n",
    "new_index = [0,1,2,3,4,5,6,8,7,9,10,11,12,13,14,15]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "obs = obs.reindex(new_index)\n",
    "\n",
    "#obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n",
    "obs.set_index(['OBS_REF'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'GHF'\n",
    "coord = ['lon', 'lat']\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "\n",
    "features = obs.index.tolist()\n",
    "\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.append(target)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e97ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing_world = 0.2 # 12 arcmin\n",
    "\n",
    "\n",
    "# create coordinates\n",
    "cols_world =np.linspace(world_lon_min, world_lon_max, 5* (world_lon_max - world_lon_min)-1 )\n",
    "rows_world = np.linspace(world_lat_min, world_lat_max, 5* (world_lat_max - world_lat_min) -1 )\n",
    "row_meshgrid_world ,col_meshgrid_world = np.meshgrid( rows_world, cols_world, indexing='ij')\n",
    "\n",
    "\n",
    "# put data into a dataset\n",
    "ds_world = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "         #variables=([\"Y\", \"X\"])\n",
    "    ),\n",
    "    coords=dict(\n",
    "        \n",
    "        YV=([\"Y\", \"X\"], row_meshgrid_world),\n",
    "        XV=([\"Y\", \"X\"], col_meshgrid_world),\n",
    "        lat=([\"Y\", \"X\"], row_meshgrid_world),\n",
    "        lon=([\"Y\", \"X\"], col_meshgrid_world),\n",
    "        Y=([\"Y\"], rows_world),\n",
    "        X=([\"X\"], cols_world),\n",
    "        \n",
    "    ),\n",
    "    attrs=dict(description=\"coords with matrices\"),\n",
    ")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "\n",
    "spacing_afr_lr = 0.5\n",
    "\n",
    "\n",
    "# create coordinates\n",
    "cols_afr_lr =np.linspace(afr_lon_min, afr_lon_max, 2* (afr_lon_max - afr_lon_min)-1 )\n",
    "rows_afr_lr = np.linspace(afr_lat_min, afr_lat_max, 2* (afr_lat_max - afr_lat_min) -1 )\n",
    "row_meshgrid_afr_lr ,col_meshgrid_afr_lr = np.meshgrid( rows_afr_lr, cols_afr_lr, indexing='ij')\n",
    "\n",
    "\n",
    "# put data into a dataset\n",
    "ds_afr_lr = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "         #variables=([\"Y\", \"X\"])\n",
    "    ),\n",
    "    coords=dict(\n",
    "        \n",
    "        YV=([\"Y\", \"X\"], row_meshgrid_afr_lr),\n",
    "        XV=([\"Y\", \"X\"], col_meshgrid_afr_lr),\n",
    "        lat=([\"Y\", \"X\"], row_meshgrid_afr_lr),\n",
    "        lon=([\"Y\", \"X\"], col_meshgrid_afr_lr),\n",
    "        Y=([\"Y\"], rows_afr_lr),\n",
    "        X=([\"X\"], cols_afr_lr),\n",
    "        \n",
    "    ),\n",
    "    attrs=dict(description=\"coords with matrices\"),\n",
    ")\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "\n",
    "spacing_afr_hr = 0.2\n",
    "\n",
    "\n",
    "# create coordinates\n",
    "cols_afr_hr =np.linspace(afr_lon_min, afr_lon_max, 5* (afr_lon_max - afr_lon_min)-1 )\n",
    "rows_afr_hr = np.linspace(afr_lat_min, afr_lat_max, 5* (afr_lat_max - afr_lat_min) -1 )\n",
    "row_meshgrid_afr_hr ,col_meshgrid_afr_hr = np.meshgrid( rows_afr_hr, cols_afr_hr, indexing='ij')\n",
    "\n",
    "\n",
    "# put data into a dataset\n",
    "ds_afr_hr = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "         #variables=([\"Y\", \"X\"])\n",
    "    ),\n",
    "    coords=dict(\n",
    "        \n",
    "        YV=([\"Y\", \"X\"], row_meshgrid_afr_hr),\n",
    "        XV=([\"Y\", \"X\"], col_meshgrid_afr_hr),\n",
    "        lat=([\"Y\", \"X\"], row_meshgrid_afr_hr),\n",
    "        lon=([\"Y\", \"X\"], col_meshgrid_afr_hr),\n",
    "        Y=([\"Y\"], rows_afr_hr),\n",
    "        X=([\"X\"], cols_afr_hr),\n",
    "        \n",
    "    ),\n",
    "    attrs=dict(description=\"coords with matrices\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452ef2c",
   "metadata": {},
   "source": [
    "# Global datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d62686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped files to be downaloded and extracted automatically in their respective directories \n",
    "ctd_url = 'https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/bvz2jz99xh-2.zip'\n",
    "emag2v3_url =  'https://www.ngdc.noaa.gov/geomag/data/EMAG2/EMAG2_V3_20170530/EMAG2_V3_20170530_UpCont.tif'\n",
    "etopo_url =  'https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/bedrock/grid_registered/georeferenced_tiff/ETOPO1_Bed_g_geotiff.zip'\n",
    "litho_rho_url = 'https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/gji/217/3/10.1093_gji_ggz094/1/ggz094_supplemental_files.zip?Expires=1661251444&Signature=b7PyU~ibwgQaxXDldilEyaUoxfLvyzHwkEVK6klpeXwFI3C7QDbr0Cky8bfhTm83hUr5ZziVhmogMp2Rr~2K74mE-H8BVM61T9BpT1Jnx9WEiO~9iZ2J5h76CN-87cUiF~SKLUQzqANAo-HCAcRyKVuOL6w6UELbeLgiAvgwuZcx5pK6IJeSBac47bP5RiPzFwtpAabpZTkjTc3uYFVblxpvkzyWWk4F~Vc8bdpMeD-iYgTuoE73JaGSN-q9Yvcy-wwRrMnqZBQ~O8xMvoyVOIGL6-6Hj-Ww4LbNaf8e8Li1W55N8g67WStG3e6~GFBSlGJWYYmw~NoCwBOiZornNw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA'\n",
    "glim_url = 'https://doi.pangaea.de/10013/epic.39939.d001'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f29de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# correct values to limit them [-1,1]\n",
    "def mag_log(data, C=400, clip_min=-1, clip_max = 1):\n",
    "     return np.clip(np.sign(data)*np.log(1+np.abs(data)/C), clip_min, clip_max)\n",
    "\n",
    "\n",
    "emag2v3_path = pooch.retrieve(\n",
    "    url=emag2v3_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#The emag dataset needs to be adjusted to place to 0-meridain in the middle,\n",
    "\n",
    "kwargs = {'dstSRS':'EPSG:4326', \n",
    "          'srcSRS':'EPSG:4326',\n",
    "          'format' : 'GTiff', \n",
    "          'outputBounds': (0, -90, 360, 90) }\n",
    "\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "emag2v3_dir = DIR/ 'Dataset'/ 'References'/'g_Magnetic'\n",
    "emag2v3_f = 'EMAG2_V3_20170530_UpCont_m0.tif'\n",
    "gdal.Warp(emag2v3_f, emag2v3_path,  **kwargs)\n",
    "\n",
    "\n",
    "try:\n",
    "    #Path(DIR/emag2v3_f).rename(emag2v3_dir /emag2v3_f)\n",
    "    shutil.move(DIR/emag2v3_f, emag2v3_dir /emag2v3_f)\n",
    "except OSError as e:\n",
    "    print(f\"An error has occurred. Continuing anyways: {e}\", end='\\n')\n",
    "\n",
    "      \n",
    "print('Interpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'EMAG2'\n",
    "    ds_world[observable] = (('Y', 'X'),  \n",
    "                         utils.read_raster(f_name = emag2v3_dir /emag2v3_f, \n",
    "                                     ds=ds_world,\n",
    "                                   crs_from=crs_from, crs_to=crs_to, no_data = -99999))\n",
    "    \n",
    "    observable = 'EMAG2_CLASS'\n",
    "    ds_world[observable] = (('Y', 'X'),  mag_log(ds_world['EMAG2'].values)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "glim_path = pooch.retrieve(\n",
    "    url=glim_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "print('Unzipping.....', end='\\n\\n')\n",
    "\n",
    "archive = ZipFile(glim_path, 'r', )\n",
    "\n",
    "\n",
    "\n",
    "glim_dir = DIR/ 'Dataset'/ 'References'/'p_Global_Lithological_Map'\n",
    "glim_f = archive.extract(archive.filelist[1], path= glim_dir)\n",
    "\n",
    "print('Interpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'GLIM'\n",
    "    ds_world[observable] = (('Y', 'X'), np.around(utils.read_raster(\n",
    "    f_name=glim_f, skiprows=6,  ds =ds_world,\n",
    "        interpol='nearest', crs_from=crs_from, crs_to=crs_to),0))\n",
    "\n",
    "\n",
    "    ds_world[observable] = ds_world[observable].where(ds_world[observable].data>0, 16)\n",
    "    ds_world[observable] = ds_world[observable].where(ds_world[observable].data<16,16)\n",
    "\n",
    "\n",
    "glim_classes = [\"su\", \"vb\" ,\"ss\", \"pb\" , \"sm\" ,\"sc\" ,\"va\", \"mt\" ,\"pa\",\n",
    "\"vi\", \"wb\" ,\"py\", \"pi\" ,\"ev\" ,\"nd\" ,\"ig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "litho_rho_path = pooch.retrieve(\n",
    "    url=litho_rho_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "\n",
    "print('Unzipping.....', end='\\n\\n')\n",
    "archive = ZipFile(litho_rho_path, 'r', )\n",
    "\n",
    "\n",
    "litho_rho_dir = DIR/ 'Dataset'/ 'References'/'i_Lithosphere_Average_Density'\n",
    "lith_roh_f = archive.extract(archive.filelist[0], path=litho_rho_dir)\n",
    "\n",
    "lith_df = pd.read_csv(lith_roh_f ,header=None,  engine='python',skiprows=9, \n",
    "                      encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "print(f'min lon : {lith_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {lith_df.iloc[:,1].min()}')\n",
    "print(f'min rho_c: {lith_df.iloc[:,5].min()}')\n",
    "\n",
    "print(f'max lon : {lith_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {lith_df.iloc[:,1].max()}')\n",
    "print(f'max rho_c: {lith_df.iloc[:,5].max()}')\n",
    "\n",
    "\n",
    "litho_c_df = lith_df.iloc[:,[0,1,5]]\n",
    "\n",
    "litho_c_df.columns = [0,1,2]\n",
    "\n",
    "litho_c_df[2] = litho_c_df[2].round(4)\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "print('#################')\n",
    "\n",
    "crust_rho_dir = DIR/ 'Dataset'/ 'References'/'i_Crustal_Average_Density'\n",
    "archive.extract(archive.filelist[0], path=crust_rho_dir)\n",
    "\n",
    "print(f'min lon : {lith_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {lith_df.iloc[:,1].min()}')\n",
    "print(f'min rho_l: {lith_df.iloc[:,6].min()}')\n",
    "\n",
    "print(f'max lon : {lith_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {lith_df.iloc[:,1].max()}')\n",
    "print(f'max rho_l: {lith_df.iloc[:,6].max()}')\n",
    "\n",
    "litho_l_df = lith_df.iloc[:,[0,1,6]]\n",
    "litho_l_df.columns = [0,1,2]\n",
    "\n",
    "litho_l_df[2] = litho_l_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for observable, df in tqdm_notebook(\n",
    "    zip(['RHO_L', 'RHO_C'], [litho_l_df, litho_c_df]), \n",
    "    total = 2, desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=df.values, ds=ds_world, crs_from=crs_from, \n",
    "        crs_to=crs_to, interpol='IDW'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402fc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ctd_path = pooch.retrieve(\n",
    "    url=ctd_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('\\nUnzipping.....', end='\\n\\n')\n",
    "archive = ZipFile(ctd_path, 'r', )\n",
    "#glim_f = archive.read('glim_wgs84_0point5deg.txt.asc')\n",
    "\n",
    "ctd_dir = DIR/ 'Dataset'/ 'References'/'f_CTD'\n",
    "ctd_f = archive.extract(archive.filelist[0], path=ctd_dir)\n",
    "CTD_df = pd.read_csv(ctd_f ,header=None,  delim_whitespace=True)\n",
    "\n",
    "print(f'min lon : {CTD_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {CTD_df.iloc[:,1].min()}')\n",
    "print(f'min z: {CTD_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {CTD_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {CTD_df.iloc[:,1].max()}')\n",
    "print(f'max z: {CTD_df.iloc[:,2].max()}')\n",
    "\n",
    "CTD_df[2] = CTD_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'CTD'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=CTD_df.values, ds=ds_world, crs_from=crs_from, \n",
    "        crs_to=crs_to, interpol='IDW'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "etopo_path = pooch.retrieve(\n",
    "    url=etopo_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('Unzipping.....', end='\\n\\n')\n",
    "archive = ZipFile(etopo_path, 'r', )\n",
    "\n",
    "\n",
    "dem_dir = DIR/ 'Dataset'/ 'References'/'h_Elevation'\n",
    "dem_f = archive.extract(archive.filelist[0], path=dem_dir)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "\n",
    "    observable = 'DEM'\n",
    "    ds_world[observable] = (('Y', 'X'), \n",
    "                            utils.read_raster(dem_f,   ds = ds_world, crs_from=crs_from, crs_to=crs_to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic downloads\n",
    "moho_url = 'https://zenodo.org/record/5730195/files/Global_Moho_WINTERC-G.xyz?download=1'\n",
    "lab_url = 'https://zenodo.org/record/5771863/files/WINTERC-G_LAB.lis?download=1'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_path = pooch.retrieve(\n",
    "    url=lab_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "lab_df = pd.read_csv(lab_path ,header=None,  engine='python',skiprows=1,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "lab_df.set_index([0], inplace=True)\n",
    "\n",
    "\n",
    "print(f'min lon : {lab_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {lab_df.iloc[:,1].min()}')\n",
    "print(f'min lab: {lab_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {lab_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {lab_df.iloc[:,1].max()}')\n",
    "print(f'max lab : {lab_df.iloc[:,2].max()}')\n",
    "\n",
    "lab_df[2]  = lab_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'LAB'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=lab_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, set_center=True, interpol='IDW'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c876b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "moho_path = pooch.retrieve(\n",
    "    url=moho_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "moho_df = pd.read_csv(moho_path ,header=None,  engine='python',skiprows=1,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'min lon : {moho_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {moho_df.iloc[:,1].min()}')\n",
    "print(f'min si: {moho_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {moho_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {moho_df.iloc[:,1].max()}')\n",
    "print(f'max si : {moho_df.iloc[:,2].max()}')\n",
    "\n",
    "moho_df[2]  = moho_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'MOHO'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=moho_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW',\n",
    "        set_center=True, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35384f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets to be downloaded directly from their respective sources\n",
    "# then to be placed in their respective directories\n",
    "# sources of datasets\n",
    "# pv wave : https://www.earth.ox.ac.uk/~smachine/cgi/index.php?page=tomo_depth\n",
    "# Sv wave : http://www.dias.ie/~aschaeff/models/SL2013sv_0.5d-grd_v2.tar.bz2\n",
    "# Geoid, Free-air, Bouguer : http://icgem.gfz-potsdam.de/calcgrid\n",
    "# shape index : https://www.3dearth.uni-kiel.de/en/public-data-products/copy_of_depth-to-moho-boundary\n",
    "# Tectonic regions : https://schaeffer.ca/models/sl2013sv-tectonic-regionalization/\n",
    "# Holocenes volcanoes https://volcano.si.edu/volcanolist_holocene.cfm\n",
    "# Pleistocene volcanoes https://volcano.si.edu/volcanolist_pleistocene.cfm\n",
    "#NGHF : 'https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1029%2F2019GC008389&file=2019GC008389-sup-0004-Data_Set_SI-S02.zip'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pv_g_f = DIR / 'Dataset'/'References'/'e_PV'/'DETOX-P1.txt'\n",
    "sv_g_f = DIR / 'Dataset'/'References'/'d_SV'/'SL2013sv_dVs_25km-0.5d_50+.xyz'\n",
    "geoid_f = DIR / 'Dataset'/ 'References'/'l_Geoid' / 'EIGEN-6C4_geoid.gdf'\n",
    "fa_f = DIR / 'Dataset'/ 'References'/'k_Free_Air' / 'EIGEN-6C4_gravity_anomaly_cl.gdf'\n",
    "bg_f = DIR / 'Dataset'/ 'References'/'m_Bouguer' / 'EIGEN-6C4_gravity_anomaly_bg.gdf'\n",
    "si_f = DIR / 'Dataset'/ 'References'/'n_Shape_Index' / 'GOCE_Curvature_Topo_Iso_Corr.txt'\n",
    "reg_f = DIR / 'Dataset'/'References'/'o_Tectonic_Regionalization'/'SL2013sv_Cluster_2d'\n",
    "volcs_hol_f =   DIR / 'Dataset'/ 'References'/'c_Distance_to_nearest_volcano'/'GVP_Volcano_List_Holocene.xlsx'\n",
    "volcs_plei_f =  DIR / 'Dataset'/ 'References'/'c_Distance_to_nearest_volcano'/'GVP_Volcano_List_Pleistocene.xlsx'\n",
    "hq_f = DIR / 'Dataset'/ 'References'/ 'q_Heat_Flow'/'NGHF.csv'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f012d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg_df = pd.read_csv(reg_f ,header=None,   delim_whitespace=True)\n",
    "print(f'min lon : {reg_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {reg_df.iloc[:,1].min()}')\n",
    "print(f'min si: {reg_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {reg_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {reg_df.iloc[:,1].max()}')\n",
    "print(f'max si : {reg_df.iloc[:,2].max()}')\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'REG'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=reg_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='nearest'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....')\n",
    "si_df = pd.read_csv(si_f ,header=None,  engine='python',skiprows=1, \n",
    "                    encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f'min lon : {si_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {si_df.iloc[:,1].min()}')\n",
    "print(f'min si: {si_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {si_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {si_df.iloc[:,1].max()}')\n",
    "print(f'max si : {si_df.iloc[:,2].max()}')\n",
    "\n",
    "si_f_df = si_df[[0,1,2]]\n",
    "si_f_df[2]  = si_f_df[2].round(4)\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'SI'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=si_f_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "geoid_df = pd.read_csv(geoid_f ,header=None,  engine='python',skiprows=36,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "print(f'min lon : {geoid_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {geoid_df.iloc[:,1].min()}')\n",
    "print(f'min geoid: {geoid_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {geoid_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {geoid_df.iloc[:,1].max()}')\n",
    "print(f'max z: {geoid_df.iloc[:,2].max()}')\n",
    "\n",
    "\n",
    "geoid_df = geoid_df.round(4)\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "print('###############')\n",
    "fa_df = pd.read_csv(fa_f ,header=None,  engine='python',skiprows=36,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'min lon : {fa_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {fa_df.iloc[:,1].min()}')\n",
    "print(f'min geoid: {fa_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {fa_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {fa_df.iloc[:,1].max()}')\n",
    "print(f'max z: {fa_df.iloc[:,2].max()}')\n",
    "\n",
    "fa_df = fa_df.round(4)\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "print('###############')\n",
    "bg_df = pd.read_csv(bg_f ,header=None,  engine='python',skiprows=37,  encoding=\"ISO-8859-1\", delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'min lon : {bg_df.iloc[:,0].min()}')\n",
    "print(f'min lat : {bg_df.iloc[:,1].min()}')\n",
    "print(f'min geoid: {bg_df.iloc[:,2].min()}')\n",
    "\n",
    "print(f'max lon : {bg_df.iloc[:,0].max()}')\n",
    "print(f'max lat : {bg_df.iloc[:,1].max()}')\n",
    "print(f'max z: {bg_df.iloc[:,2].max()}')\n",
    "\n",
    "bg_df = bg_df.round(4)\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for observable, df in tqdm_notebook(zip(['GEOID', 'FA' , 'BG'], [geoid_df, fa_df, bg_df]), \n",
    "                                    total=3, desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ea373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "pv_g_df = pd.read_csv(pv_g_f, skiprows=3,\n",
    "                              header=None, engine='python', skip_blank_lines=True,   na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'lon min : {pv_g_df.iloc[:,0].min()}')\n",
    "print(f'lat min : {pv_g_df.iloc[:,1].min()}')\n",
    "print(f'lon max : {pv_g_df.iloc[:,0].max()}')\n",
    "print(f'lat max : {pv_g_df.iloc[:,1].max()}')\n",
    "print(f'pv min before: {pv_g_df[2] .min()}')\n",
    "print(f'pv max before: {pv_g_df[2] .max()}')\n",
    "\n",
    "\n",
    "# remove % by * 0.01 'dVp(%)'\n",
    "\n",
    "pv_g_df[2] = pv_g_df.iloc[:,2].multiply(0.01).round(4)\n",
    "print(f'pv min after: {pv_g_df[2] .min()}')\n",
    "print(f'pv max after: {pv_g_df[2] .max()}')\n",
    "\n",
    "\n",
    "print('#####')\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'PV'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=pv_g_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        set_center=True, crs_to=crs_to, interpol='IDW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "sv_init_df = pd.read_csv(sv_g_f,header=None, skiprows=1, skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "print(f'lon min : {sv_init_df.iloc[:,1].min()}')\n",
    "print(f'lat min : {sv_init_df.iloc[:,2].min()}')\n",
    "print(f'lon max : {sv_init_df.iloc[:,1].max()}')\n",
    "print(f'lat max : {sv_init_df.iloc[:,2].max()}')\n",
    "print(f'sv min before: {sv_init_df[5] .min()}')\n",
    "print(f'sv max before: {sv_init_df[5] .max()}')\n",
    "# remove % by * 0.01 'dVs(%)'\n",
    "\n",
    "sv_init_df[5] = sv_init_df.iloc[:,5].multiply(0.01).round(4)\n",
    "print(f'sv min after: {sv_init_df[5] .min()}')\n",
    "print(f'sv max after: {sv_init_df[5] .max()}')\n",
    "\n",
    "print('#####')\n",
    "\n",
    "sv_g_150_df = sv_init_df[sv_init_df[0]==150][[1,2,5]]\n",
    "#seismic_w_sv_df = copy.deepcopy(seismic_w_sv_df_150_df.iloc[:,[1,2,5]]\n",
    "\n",
    "\n",
    "sv_g_150_df.columns = [0,1,2]\n",
    "\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    observable = 'SV'\n",
    "    ds_world[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=sv_g_150_df.values, ds=ds_world, crs_from=crs_from,\n",
    "        crs_to=crs_to, interpol='IDW'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_max_dist = 100\n",
    "a_max_dist = 250\n",
    "\n",
    "\n",
    "volcs_hol_df = pd.read_excel(volcs_hol_f, header=1)\n",
    "volcs_plei_df = pd.read_excel(volcs_plei_f, header=1)\n",
    "volc_df = pd.concat([volcs_hol_df, volcs_plei_df])\n",
    "\n",
    "print(volc_df[['Latitude','Longitude']])\n",
    "cols = volc_df.columns.to_list()\n",
    "cols[1], cols[0] = cols[0], cols[1]\n",
    "volc_df.loc[:,cols]\n",
    "\n",
    "print(f' Hol: {len(volcs_hol_df)}')\n",
    "print(f' Plei: {len(volcs_plei_df)}')\n",
    "print(f' Total: {len(volc_df)}')\n",
    "\n",
    "volc_df = volc_df[['Latitude','Longitude']]\n",
    "volc_df.columns = [ 'lon', 'lat']\n",
    "\n",
    "\n",
    "volc_df = volc_df.round(2)\n",
    "\n",
    "print(f\" lon: {volc_df['lon'].min()} {volc_df['lon'].max()}\")\n",
    "print(f\" lat: {volc_df['lat'].min()} {volc_df['lat'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca30cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual inspection of imported datasets\n",
    "observables = ['RHO_L', 'RHO_C', 'CTD', 'DEM', 'GLIM', 'EMAG2_CLASS', 'MOHO', 'LAB',\n",
    "              'SV', 'PV', 'REG', 'BG', 'FA', 'GEOID', 'SI']\n",
    "data_types = [True,True, True, True, False , True , True,True, \n",
    "              True,True, False, True,True, True,True, ]\n",
    "scales = [False,False,False,False, False, True, False,False,\n",
    "          \n",
    "          False,False, False, False, False, False, False,]\n",
    "\n",
    "\n",
    "\n",
    "for observable, data_type, scale in tqdm_notebook(zip(observables, data_types, scales), \n",
    "                                          total=15, desc = 'Processing: '):\n",
    "    print('Plotting.....', end='\\n\\n')\n",
    "    sleep(0.01)\n",
    "#for observable, data_type, scale in zip(observables, data_types, scales):\n",
    "    label = obs.loc[observable, 'LABELS_gmt']\n",
    "    unit = obs.loc[observable, 'UNITS_gmt']\n",
    "    if (observable == 'GLIM') or (observable == 'REG') :\n",
    "        series=(obs.loc[observable, 'V_RANGE'][0], obs.loc[observable, 'V_RANGE'][1]+1, 1)\n",
    "    else:\n",
    "        min_range = obs.loc[observable, 'V_RANGE'][0]\n",
    "        max_range = obs.loc[observable, 'V_RANGE'][1]\n",
    "        series=(min_range, max_range,(max_range - min_range)/100 )\n",
    "    \n",
    "    fig = pygmt.Figure()\n",
    "    pygmt.makecpt(\n",
    "        cmap=obs.loc[observable, 'CMAPS'], #temp 19lev\n",
    "\n",
    "        #cmap='lajolla',\n",
    "        #series= \"1/17/1\",\n",
    "        #truncate = '-2000/2000',\n",
    "       # categorical=list(range(1,17)),\n",
    "        continuous=data_type,\n",
    "        reverse=scale,\n",
    "        \n",
    "       series = series,\n",
    "        # convert ['Adelie', 'Chinstrap', 'Gentoo'] to 'Adelie,Chinstrap,Gentoo'\n",
    "        #color_model=\"+c\" + \",\".join(glim_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    fig.basemap(frame=True, region=region_world, projection=projection,)\n",
    "    #avoid interpolation\n",
    "    fig.grdimage(ds_world[observable],\n",
    "                 interpolation='n+a' \n",
    "                )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    fig.coast(\n",
    "        #region = 'JP',\n",
    "        #shorelines=0.5,\n",
    "        #water=\"lightblue\", \n",
    "        shorelines=\"0.1p,black\",\n",
    "        #borders=[\"1/0.001p,black\"],\n",
    "        lakes=\"lightblue\",\n",
    "        rivers=\"lightblue\" ,\n",
    "        #borders=[\"1/0.5p,black\"],\n",
    "        #water='white',\n",
    "        )\n",
    "    fig.colorbar(frame=[\"af\", f'x+l\"{label}\"\\t\\t{unit}'])\n",
    "    fig.show(width=1000)\n",
    "    #fig.savefig(\"north-america-pygmt-map.png\",crop=True, dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed31da6",
   "metadata": {},
   "source": [
    "# Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start with shared observables with global models\n",
    "\n",
    "\n",
    "for ds in tqdm_notebook([ds_afr_lr, ds_afr_hr], desc = 'Grid: '):\n",
    "    for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "        sleep(0.01)\n",
    "\n",
    "        ds['EMAG2'] = (('Y', 'X'),  utils.read_raster(f_name = emag2v3_dir /emag2v3_f, \n",
    "                                                ds=ds, crs_to=crs_to, crs_from=crs_from, \n",
    "                                                interpol='nearest', no_data = -99999))\n",
    "        ds['EMAG2_CLASS'] = (('Y', 'X'),  mag_log(ds['EMAG2'].values)) \n",
    "\n",
    "\n",
    "    for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "        sleep(0.01)\n",
    "        observable = 'DEM'\n",
    "        ds[observable] = (('Y', 'X'), utils.read_raster(\n",
    "                f_name=dem_f, ds=ds, interpol='nearest', crs_from=crs_from, crs_to=crs_to))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####\n",
    "\n",
    "    for i in tqdm_notebook(range(1), desc = 'Processing: '):\n",
    "        sleep(0.01)\n",
    "        observable = 'GLIM'\n",
    "        ds[observable] = (('Y', 'X'), np.around(utils.read_raster(\n",
    "        f_name=glim_f, skiprows=6, interpol='nearest', ds=ds, crs_from=crs_from, crs_to=crs_to),0))\n",
    "\n",
    "\n",
    "        ds[observable] = ds[observable].where(ds[observable].data>0, 16)\n",
    "        ds[observable] = ds[observable].where(ds[observable].data<16,16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    observables = ['BG', 'FA', 'GEOID', 'SI', 'REG', 'MOHO', 'LAB', 'CTD', 'RHO_L', 'RHO_C' ]\n",
    "    dfs = [bg_df, fa_df, geoid_df, si_df, reg_df, moho_df, lab_df, CTD_df, litho_l_df, litho_c_df]\n",
    "    interpols = ['IDW', 'IDW', 'IDW', 'IDW', 'nearest', 'IDW', 'IDW', 'IDW', 'IDW', 'IDW']\n",
    "    centerings = [False, False, False, False, False, True, True, False, False, False]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for observable, df, interpol, centering in tqdm_notebook(\n",
    "        zip(observables, dfs, interpols, centerings), \n",
    "                                              total=10, desc = 'Processing: '):\n",
    "\n",
    "        sleep(0.01)\n",
    "        ds[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "        data=df.values, ds=ds, crs_from=crs_from, crs_to=crs_to, interpol=interpol,\n",
    "        set_center=centering))\n",
    "\n",
    "    print('Terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the position of volcanoes\n",
    "\n",
    "\n",
    "\n",
    "geod = proj.Geod(ellps='WGS84' )\n",
    "vol_int = volc_df[volc_df['lon'].between(afr_lon_min, afr_lon_max) \n",
    "                     & volc_df['lat'].between(afr_lat_min, afr_lat_max)]\n",
    "v_lon = vol_int['lon'].values\n",
    "v_lat = vol_int['lat'].values\n",
    "\n",
    "v_n = len(v_lon)\n",
    "\n",
    "v_k = 1\n",
    "\n",
    "for ds in tqdm_notebook([ds_afr_lr, ds_afr_hr], desc = 'Grid: '):\n",
    "    \n",
    "    ny = len(ds.Y)\n",
    "    nx = len(ds.X)\n",
    "    nn = (ny, nx)\n",
    "    volcs = np.zeros(nn)\n",
    "    \n",
    "    lats = ds['lat'].values\n",
    "    lons = ds['lon'].values\n",
    "\n",
    "    # this process is slow to map all grid\n",
    "\n",
    "    for x in tqdm_notebook(range(nx), desc='lon: ' ):\n",
    "        for y in tqdm_notebook(range(ny), desc='lat: ', leave=False ):\n",
    "            sleep(0.01)\n",
    "            lat = lats[y,x]\n",
    "            lon = lons[y,x]\n",
    "            _, _, ds_ = geod.inv(v_n*[lon], v_n*[lat], v_lon, v_lat)\n",
    "            idx = np.argpartition(ds_, v_k)[:v_k]\n",
    "            volcs[y,x] = np.sum(np.take(ds_, idx))/v_k/km\n",
    "\n",
    "    ds['VOLC_DIST'] = (('Y', 'X'), volcs)\n",
    "    ds['VOLC_DIST_W'] = (('Y', 'X'), np.clip(1 - volcs/volc_max_dist, 0, 1)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1225116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the modle from https://nlscelli.wixsite.com/ncseismology/af2019\n",
    "\n",
    "sv_a_f = DIR / 'Dataset'/'References'/'d_SV'/'AF2019_vsvp.mod'\n",
    "\n",
    "pv_afr_url = 'http://ds.iris.edu/files/products/emc/emc-files/AFRP20-RF-CR1-MOD-3D.r0.0.nc'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_afr_path = pooch.retrieve(\n",
    "    url=pv_afr_url,\n",
    "    known_hash=None,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "depth_150_km = 2\n",
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "### dVp\n",
    "dvp = xr.load_dataset(pv_afr_path)['dvp'][depth_150_km,:,:].values.reshape(-1,1)\n",
    "dvp = np.round(dvp * 0.01, 4)\n",
    "pv_lon = xr.load_dataset(pv_afr_path)['longitude'].values\n",
    "pv_lat = xr.load_dataset(pv_afr_path)['latitude'].values\n",
    "lat_mesh ,lon_mesh = np.meshgrid( pv_lat, pv_lon, indexing='ij')\n",
    "\n",
    "lons = lon_mesh.reshape(-1,1)\n",
    "lats = lat_mesh.reshape(-1,1)\n",
    "dvp_arr = np.append(lons, lats, axis=1)\n",
    "dvp_arr = np.append(dvp_arr, dvp, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "### VP velocity\n",
    "\n",
    "\n",
    "pv = xr.load_dataset(pv_afr_path)['vp'][depth_150_km,:,:].values.reshape(-1,1)\n",
    "\n",
    "vp = np.round(pv, 4)\n",
    "vp_arr = np.append(lons, lats, axis=1)\n",
    "vp_arr = np.append(vp_arr, vp, axis=1)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for ds in tqdm_notebook([ds_afr_lr, ds_afr_hr], desc = 'Grid: '):\n",
    "    for observable, arr in tqdm_notebook(\n",
    "        zip(['PV', 'PV_Velocity' ], [dvp_arr, vp_arr]), \n",
    "                                              total=2, desc = 'Processing: '):\n",
    "\n",
    "        ds[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "            data=arr, ds=ds, crs_from=crs_from, crs_to=crs_to, interpol='IDW'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60aaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Importing.....', end='\\n\\n')\n",
    "\n",
    "sv_init_df = pd.read_csv(sv_a_f   , skip_blank_lines=True, \n",
    "                      na_filter=True, delim_whitespace=True)\n",
    "\n",
    "\n",
    "print(f'lon min : {sv_init_df[\"#lon\"].min()}')\n",
    "print(f'lat min : {sv_init_df[\"lat\"].min()}')\n",
    "print(f'lon max : {sv_init_df[\"#lon\"].max()}')\n",
    "print(f'lat max : {sv_init_df[\"lat\"].max()}')\n",
    "print(f'dVs min before: {sv_init_df[\"dVs\"] .min()}')\n",
    "print(f'dVs max before: {sv_init_df[\"dVs\"] .max()}')\n",
    "\n",
    "#calculate percentile dvs\n",
    "\n",
    "sv_init_df[\"dVs%\"] = sv_init_df[\"dVs\"] / sv_init_df[\"Vs_ref\"]\n",
    "print(f'dVs % min after: {sv_init_df[\"dVs%\"].min()}')\n",
    "print(f'dVs % max after: {sv_init_df[\"dVs%\"].max()}')\n",
    "\n",
    "\n",
    "dvs_df = sv_init_df[sv_init_df['depth']==150][['#lon','lat' , \"dVs%\"]]\n",
    "\n",
    "dvs_df[\"dVs%\"]  = dvs_df[\"dVs%\"].round(4)\n",
    "    \n",
    "dvs_df.columns = [0,1,2]\n",
    "\n",
    "#######\n",
    "# SV africa speed\n",
    "\n",
    "\n",
    "vs_df = sv_init_df[sv_init_df['depth']==150][['#lon','lat' , 'Vs_abs']]\n",
    "\n",
    "\n",
    "    \n",
    "vs_df['Vs_abs'] = vs_df['Vs_abs'].round(4)/1000   \n",
    "\n",
    "\n",
    "vs_df.columns = [0,1,2]\n",
    "\n",
    "\n",
    "print('\\nInterpolating.....', end='\\n\\n')\n",
    "\n",
    "for ds in tqdm_notebook([ds_afr_lr, ds_afr_hr], desc = 'Grid: '):\n",
    "    for observable, df in tqdm_notebook(\n",
    "        zip(['SV', 'SV_Velocity' ], [dvs_df, vs_df]), \n",
    "                                              total=2, desc = 'Processing: '):\n",
    "\n",
    "        ds[observable] =  (('Y', 'X'), utils.read_numpy(\n",
    "            data=df.values, ds=ds, crs_from=crs_from, crs_to=crs_to, interpol='IDW'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fab79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visual inspection of imported datasets\n",
    "\n",
    "\n",
    "data_types = [True,True, True, True, True , True , True,True, \n",
    "              True, True,True, True,True, True, False, False ]\n",
    "scales = [False,False,False,False, False, False, False,False,\n",
    "          \n",
    "          False,False, False, False, False, False, False, False]\n",
    "\n",
    "for ds in tqdm_notebook([ds_afr_lr, ds_afr_hr], desc = 'Grid: '):\n",
    "    for observable, data_type, scale in tqdm_notebook(zip(obs.index, data_types, scales), \n",
    "                                              total=16, desc = 'Processing: '):\n",
    "        print('Plotting.....', end='\\n\\n')\n",
    "        sleep(0.01)\n",
    "        label = obs.loc[observable, 'LABELS_gmt']\n",
    "        unit = obs.loc[observable, 'UNITS_gmt']\n",
    "        if (observable == 'GLIM') or (observable == 'REG') :\n",
    "            series=(obs.loc[observable, 'V_RANGE'][0], obs.loc[observable, 'V_RANGE'][1]+1, 1)\n",
    "        else:\n",
    "            min_range = obs.loc[observable, 'V_RANGE'][0]\n",
    "            max_range = obs.loc[observable, 'V_RANGE'][1]\n",
    "            series=(min_range, max_range,(max_range - min_range)/100 )\n",
    "\n",
    "        fig = pygmt.Figure()\n",
    "        cmap = pygmt.makecpt(\n",
    "            cmap=obs.loc[observable, 'CMAPS'], #temp 19lev\n",
    "            #cmap='lajolla',\n",
    "            #series= \"1/17/1\",\n",
    "            #truncate = '-2000/2000',\n",
    "           # categorical=list(range(1,17)),\n",
    "            continuous=data_type,\n",
    "            reverse=scale,\n",
    "\n",
    "           series = series,\n",
    "            # convert ['Adelie', 'Chinstrap', 'Gentoo'] to 'Adelie,Chinstrap,Gentoo'\n",
    "            #color_model=\"+c\" + \",\".join(glim_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        fig.basemap(frame=True, region=region_afr, projection=projection,)\n",
    "        #avoid interpolation\n",
    "        if (observable == 'GLIM') or (observable == 'REG') :\n",
    "            fig.grdimage(ds[observable],\n",
    "                         interpolation='n+a' \n",
    "                        )\n",
    "        else:\n",
    "\n",
    "            fig.grdimage(ds[observable]\n",
    "                        )\n",
    "        fig.coast(\n",
    "            #shorelines=0.5,\n",
    "            #water=\"lightblue\", \n",
    "            shorelines=\"0.1p,black\",\n",
    "            #borders=[\"1/0.001p,black\"],\n",
    "            lakes=\"lightblue\",\n",
    "            rivers=\"lightblue\" ,\n",
    "            #dcw=\"=AS,=EU+gdarkgrey\",\n",
    "            resolution = 'i',\n",
    "            )\n",
    "        fig.colorbar(frame=[\"af\", f'x+l\"{label}\"\\t\\t{unit}'])\n",
    "        fig.show(width=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908bf5a",
   "metadata": {},
   "source": [
    "function to make inverse distance weight interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3646808",
   "metadata": {},
   "source": [
    "# read in dataseys from xyz folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f076e",
   "metadata": {},
   "source": [
    "Interpolation for wolrd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88c05d",
   "metadata": {},
   "source": [
    "interpolatation for Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64610d7f",
   "metadata": {},
   "source": [
    "# 2 - Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract hq ratings\n",
    "\n",
    "# extracted from agrid Tobias Stal\n",
    "rabcd = ['A', 'B', 'C', 'D']\n",
    "rabc = ['A', 'B', 'C']\n",
    "rab = ['A', 'B']\n",
    "ra = ['A']\n",
    "exclude = ['Indian Ocean', \n",
    "           'Arctic Ocean', \n",
    "           'North Atlantic Ocean', \n",
    "           'South Atlantic Ocean', \n",
    "           'South Pacific Ocean', \n",
    "           'North Pacific Ocean']\n",
    "\n",
    "\n",
    "\n",
    "hq_in = pd.read_csv(hq_f)\n",
    "# convert hear to mili\n",
    "\n",
    "\n",
    "elev_range = (-5000,5000)\n",
    "elev_cut = -1000\n",
    "elev_high_cut = -250\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16, 5))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "\n",
    "hq_in = hq_in.rename(columns={'heat-flow (mW/m2)' :target, 'longitude': 'lon', 'latitude': 'lat'})\n",
    "    \n",
    "ax[1].set_xlim(elev_range)\n",
    "\n",
    "\n",
    "utils.print_latex_tab_line(d = hq_in[target], c = 'All records')\n",
    "\n",
    "utils.hq_hist(hq_in, target, ax=ax[0], label = f'All records (N={len(hq_in)})')\n",
    "\n",
    "\n",
    "# Remove records with missing position or heat flow value in long lat heat\n",
    "hq_clean = hq_in.dropna(subset = ['lon', 'lat', target])\n",
    "utils.print_latex_tab_line(d = hq_clean[target], c = 'Excluded incomplete records')\n",
    "utils.hq_hist(hq_clean, target,ax=ax[0], label = f'Excluded incomplete records (N={len(hq_clean)})')\n",
    "\n",
    "# Remove measurements at low elevation\n",
    "hq_elev = hq_clean[hq_clean['elevation (m)']>elev_cut]\n",
    "\n",
    "\n",
    "\n",
    "hq_clean['elevation (m)'].hist(bins=151, ax=ax[1], \n",
    "                               range = elev_range, \n",
    "                               label = 'Excluded',\n",
    "                              color = 'brown')\n",
    "\n",
    "hq_elev['elevation (m)'].hist(bins=151, ax=ax[1], \n",
    "                              range = elev_range, \n",
    "                              label = 'Used records',\n",
    "                             color = 'green')\n",
    "\n",
    "\n",
    "ax[1].axvline(x=elev_cut, color='gray', label=f'Cut-off elevation {elev_cut} m', lw=1)\n",
    "\n",
    "utils.print_latex_tab_line(d = hq_elev[target], c = f'Excluded deeper than {abs(elev_cut)} m bsl')\n",
    "utils.hq_hist(hq_elev, target,ax=ax[0],  label = f'Excluded deeper than {abs(elev_cut)} m (N={len(hq_elev)})') \n",
    "\n",
    "#hq_land = hq_clean.loc[~hq_clean['country'].isin(exclude)] # Exclude oceans \n",
    "\n",
    "# Remove any heat flow measurements from high lats for poles\n",
    "hq_no_pole = hq_elev[hq_elev['lat'].between(world_lat_min, world_lat_max, inclusive='both')]\n",
    "utils.print_latex_tab_line(d = hq_no_pole[target], c = 'Excluded high lats')\n",
    "utils.hq_hist(hq_no_pole, target, ax=ax[0], label = f'Excluded high lats (N={len(hq_no_pole)})')\n",
    "\n",
    "\n",
    "# Select rabd rathings\n",
    "hq_rabcd = hq_no_pole.loc[hq_no_pole['code6'].isin(rabcd)] # Only rab measurements\n",
    "utils.hq_hist(hq_rabcd, target, ax=ax[0], label = f'Rating A-D (N={len(hq_rabcd)})')\n",
    "utils.print_latex_tab_line(d = hq_rabcd[target], c = 'Rating A$^c$ , B$^d$ , C$^e$')  \n",
    "\n",
    "#hq_rabcd = hq_rabcd.rename(columns={target :target})\n",
    "\n",
    "# Select rab rathings\n",
    "hq_rabc = hq_no_pole.loc[hq_no_pole['code6'].isin(rabc)] # Only rab measurements\n",
    "utils.hq_hist(hq_rabc, target, ax=ax[0], label = f'Rating A-C (N={len(hq_rabc)})')\n",
    "utils.print_latex_tab_line(d = hq_rabc[target], c = 'Rating A$^c$ , B$^d$ , C$^e$')  \n",
    "\n",
    "#hq_rabc = hq_rabc.rename(columns={target :target})\n",
    "\n",
    "\n",
    "# Select better ratings\n",
    "hq_rab = hq_no_pole.loc[hq_no_pole['code6'].isin(rab)] # Only very rab measurements\n",
    "utils.hq_hist(hq_rab, target,ax=ax[0],  label = f'Rating A-B (N={len(hq_rab)})')\n",
    "utils.print_latex_tab_line(d = hq_rab[target], c = 'Rating A$^c$ , B$^d$')    \n",
    "\n",
    "#hq_rab = hq_rab.rename(columns={target :target})\n",
    "\n",
    "# Select ra ratings\n",
    "hq_ra = hq_no_pole.loc[hq_no_pole['code6'].isin(ra)] # Only the ra measurements\n",
    "utils.hq_hist(hq_ra, target, ax=ax[0], label = f'Rating A (N={len(hq_ra)})')\n",
    "utils.print_latex_tab_line(d = hq_ra[target], c = 'Rating A$^c$')\n",
    "\n",
    "#hq_ra = hq_ra.rename(columns={target :target})\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.set_ylabel('N')\n",
    "\n",
    "ax[1].set_xlabel('Elevation (m)')\n",
    "ax[0].set_xlabel('Heat flow (mW/m2)')\n",
    "    \n",
    "ax[1].set_title('Cut-off elevation')\n",
    "ax[0].set_title('Distribution of heat flow values after removal of records')\n",
    "   \n",
    "plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22127d",
   "metadata": {},
   "source": [
    "get index from grid and distance and transfer it to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer obs values from grid to hq integrate grid index\n",
    "\n",
    "\n",
    "training_f = DIR /'Dataset'/'Preprocessed'\n",
    "\n",
    "hq_ra_cp = hq_ra[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "hq_rab_cp = hq_rab[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "\n",
    "\n",
    "# volcanoes\n",
    "geod = proj.Geod(ellps='WGS84' )\n",
    "vol_int = volc_df[volc_df['lon'].between(world_lon_min, world_lon_max) \n",
    "                     & volc_df['lat'].between(world_lat_min, world_lat_max)]\n",
    "\n",
    "#poistion of volvanoes inside targeted area\n",
    "v_lons = vol_int['lon'].values.ravel()\n",
    "v_lats = vol_int['lat'].values.ravel()\n",
    "\n",
    "for hq,label in tqdm_notebook(\n",
    "    zip([hq_ra_cp, hq_rab_cp], ['int_ra.csv','int_rab.csv']), \n",
    "                                          total=2, desc = 'Processing: '):\n",
    "\n",
    "    sleep(0.01)\n",
    "\n",
    "    model_coords = np.stack([ds_world.lon.values.ravel(), ds_world.lat.values.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum([spacing_world , spacing_world])/1.5 # equivalent to 15 km\n",
    "\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hq_coords = np.stack([hq['lon'], hq['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hq_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    hq.loc[:,'grid_index_world'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "    hq.loc[:,'grid_index_world'] = hq.loc[:,'grid_index_world'].astype('int')\n",
    "\n",
    "    dists_km = utils.distance(hq['lat'].values, hq['lon'].values, \n",
    "                    ds_world.lat.values.ravel()[indexs], ds_world.lon.values.ravel()[indexs])\n",
    "\n",
    "    hq.loc[:,'dist_from_grid_world'] = dists_km*km\n",
    "\n",
    "\n",
    "    for observable in list(ds_world):\n",
    "            hq.loc[:,observable] = ds_world[observable].values.ravel()[hq.loc[:,'grid_index_world'].values] \n",
    "            \n",
    "     \n",
    "    ###volcanoes\n",
    "\n",
    "\n",
    "\n",
    "    # this process is slow to map all grid\n",
    "\n",
    "    volc_dist = []\n",
    "    volc_dist_w = []\n",
    "    # this process is slow to map all grid\n",
    "    hq_lons, hq_lats = hq['lon'].values.ravel().reshape(-1,1) , hq['lat'].values.ravel().reshape(-1,1)\n",
    "    for i in tqdm_notebook(range(len(hq)), desc='Volcs: ' ):\n",
    "        sleep(0.01)\n",
    "        hq_lat = np.repeat(hq_lats[i], len(v_lats) , axis=0).reshape(-1,1)\n",
    "        hq_lon =np.repeat(hq_lons[i], len(v_lons) , axis=0).reshape(-1,1)\n",
    "        _, _, ds = geod.inv(v_lons, v_lats, hq_lon, hq_lat)\n",
    "        #idx = np.argpartition(ds, v_k)[:v_k]\n",
    "        #ds_volcs[i] = (('Y', 'X'), ds.reshape(nn_world)/v_k/km)\n",
    "        #volcs[i] = min(ds/v_k/km)\n",
    "        volc_dist.append(min(ds)[0]/1/km)\n",
    "        volc_dist_w.append(np.clip(1 - (min(ds)[0]/1/km)/volc_max_dist, 0, 1))\n",
    "        #hq_volc.loc[i, 'VOLC_DIST_W'] = np.clip(1 - hq_volc.loc[i, 'VOLC_DIST']/volc_max_dist, 0, 1)\n",
    "    hq['VOLC_DIST'] = volc_dist\n",
    "    hq['VOLC_DIST_W'] = volc_dist_w\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    hq.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    hq.reset_index( inplace=True,drop=True)\n",
    "    hq.to_csv(training_f/f'W_{label}' , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "for hq,label in tqdm_notebook(\n",
    "    zip([hq_ra_copy, hq_rab_copy], ['W_merged_ra_int.csv','W_merged_rab_int.csv']), \n",
    "                                          total=2, desc = 'Processing: '):\n",
    "    sleep(0.01)\n",
    "    \n",
    "    hq_Afr = hq[(hq['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hq['lat'].between(afr_lat_min, afr_lat_max))]\n",
    "    \n",
    "\n",
    "    \n",
    "    for observable in list(ds_world):\n",
    "            hq.loc[:,observable] = ds_world[observable].values.ravel()[hq.loc[:,'grid_index_world'].values] \n",
    "            \n",
    "    #######\n",
    "\n",
    "\n",
    "     # from grid afr take values into world\n",
    "    model_coords = np.stack([ds_afr.lon.values.ravel(), ds_afr.lat.values.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "    max_dist = np.sum([spacing_afr, spacing_afr])/1.5 \n",
    "\n",
    "\n",
    "    # stack coordin of best the good to compare to0 world grids[region]\n",
    "    hq_coords = np.stack([hq_Afr['lon'], hq_Afr['lat']], axis=-1)\n",
    "    # what is the index and distance of the neasrest points in world grids[region] to df\n",
    "    grid_dists, indexs = spatial.KDTree(model_coords).query(hq_coords, \n",
    "                             distance_upper_bound=max_dist)\n",
    "    \n",
    "    #modify grid index of world extracted africa with Africas\n",
    "    hq_Afr.loc[:,'grid_index_afr'] = indexs # This is the index to the cell in the ravel grids[region]\n",
    "\n",
    "    hq_Afr.loc[:,'grid_index_afr'] = hq_Afr.loc[:,'grid_index_afr'].astype('int')\n",
    "    \n",
    "    dists_km = utils.distance( hq_Afr['lat'].values, hq_Afr['lon'].values,\n",
    "                     ds_afr.lat.values.ravel()[indexs], ds_afr.lon.values.ravel()[indexs])\n",
    "\n",
    "    #hq_Afr.loc[:,'dist_from_grid'] = dists_km*km\n",
    "    \n",
    "    #transfer value of africa into world\n",
    "    for observable in list(ds_afr):\n",
    "            hq_Afr.loc[:,observable] = ds_afr[observable].values.ravel()[hq_Afr.loc[:,'grid_index_afr'].values] \n",
    "    #hq_Afr.drop(['grid_index_afr'], inplace=True, axis=1)\n",
    "    \n",
    "    \n",
    "     #######\n",
    "\n",
    "\n",
    "    print(len(hq))\n",
    "    hq_copy = hq[~(hq['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                    & hq['lat'].between(afr_lat_min, afr_lat_max))]\n",
    "    \n",
    "\n",
    "    print(len(hq_copy))\n",
    "\n",
    "\n",
    "    frames = [hq_copy,hq_Afr ]\n",
    "    hq_copy = pd.concat(frames).reset_index(drop=True)\n",
    "\n",
    "    print(len(hq_copy))\n",
    "    hq_copy = hq_copy.rename({target:'GHF'})\n",
    "\n",
    "    hq_copy.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "    hq_copy.reset_index( inplace=True,drop=True)\n",
    "    hq_copy.to_csv(training_f/f'Training_{label}' , index=False, header=True, sep='\\t')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('terminated')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa28b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hq_ra_cp = hq_ra[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "hq_rab_cp = hq_rab[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "hq_rabc_cp = hq_rabc[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "hq_rabcd_cp = hq_rabcd[['lon', 'lat', 'GHF']].copy(deep=True)\n",
    "\n",
    "for ds, spacing, res in tqdm_notebook(zip([ds_afr_lr, ds_afr_hr], [spacing_afr_lr, spacing_afr_hr], \n",
    "                                               ['lr','hr']), total= 2, desc = 'Grid: '):\n",
    "\n",
    "    for hq,label in tqdm_notebook(\n",
    "        zip([hq_ra_cp, hq_rab_cp], [f'int_ra_{res}.csv',f'int_rab_{res}.csv']), \n",
    "                                              total=2, desc = 'Processing: '):\n",
    "        sleep(0.01)\n",
    "        gt_afr_df = hq[(hq['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                        & hq['lat'].between(afr_lat_min, afr_lat_max))].reset_index(drop=True)\n",
    "\n",
    "        model_coords = np.stack([ds.lon.values.ravel(), ds.lat.values.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "        max_dist = np.sum([spacing, spacing])/1.5  # equivalent to 15 km\n",
    "\n",
    "\n",
    "        # stack coordin of best the good to compare to0 world ds\n",
    "        hq_coords = np.stack([gt_afr_df['lon'], gt_afr_df['lat']], axis=-1)\n",
    "        # what is the index and distance of the neasrest points in world ds to df\n",
    "        grid_dists, indexs = spatial.KDTree(model_coords).query(hq_coords, \n",
    "                                 distance_upper_bound=max_dist)\n",
    "        gt_afr_df.loc[:,'grid_index_afr'] = indexs # This is the index to the cell in the ravel ds\n",
    "\n",
    "        gt_afr_df.loc[:,'grid_index_afr'] = gt_afr_df.loc[:,'grid_index_afr'].astype('int')\n",
    "\n",
    "        dists_km = utils.distance( gt_afr_df['lat'].values, gt_afr_df['lon'].values,\n",
    "                          ds.lat.values.ravel()[indexs], ds.lon.values.ravel()[indexs])\n",
    "\n",
    "        gt_afr_df.loc[:,'dist_from_grid'] = dists_km*km\n",
    "\n",
    "        for observable in list(ds):\n",
    "                gt_afr_df.loc[:,observable] = ds[observable].values.ravel()[gt_afr_df.loc[:,'grid_index_afr'].values] \n",
    "\n",
    "        gt_afr_df = gt_afr_df.rename({target:'GHF'})\n",
    "        gt_afr_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "        gt_afr_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        gt_afr_df.to_csv(training_f/f'Afr_{label}' ,  index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11670daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "\n",
    "for ds, spacing, res in tqdm_notebook(zip([ds_afr_lr, ds_afr_hr], [spacing_afr_lr, spacing_afr_hr], \n",
    "                                               ['lr','hr']), total=2, desc = 'Grid: '):\n",
    "    for hq,label in tqdm_notebook(\n",
    "            zip([hq_rabc_cp, hq_rabcd_cp], [f'int_rabc_{res}.csv',f'int_rabcd_{res}.csv']), \n",
    "                                                  total=2, desc = 'Processing: '):\n",
    "            sleep(0.01)\n",
    "            gt_afr_df = hq[(hq['lon'].between(afr_lon_min, afr_lon_max)  \n",
    "                            & hq['lat'].between(afr_lat_min, afr_lat_max))].reset_index(drop=True)\n",
    "\n",
    "            model_coords = np.stack([ds.lon.values.ravel(), ds.lat.values.ravel()], axis=-1) # lat and lon for all cells ravelled\n",
    "            max_dist = np.sum([spacing, spacing])/1.5 \n",
    "\n",
    "\n",
    "            # stack coordin of best the good to compare to0 world ds\n",
    "            hq_coords = np.stack([gt_afr_df['lon'], gt_afr_df['lat']], axis=-1)\n",
    "            # what is the index and distance of the neasrest points in world ds to df\n",
    "            grid_dists, indexs = spatial.KDTree(model_coords).query(hq_coords, \n",
    "                                     distance_upper_bound=max_dist)\n",
    "            gt_afr_df.loc[:,'grid_index_afr'] = indexs # This is the index to the cell in the ravel ds\n",
    "\n",
    "            gt_afr_df.loc[:,'grid_index_afr'] = gt_afr_df.loc[:,'grid_index_afr'].astype('int')\n",
    "\n",
    "\n",
    "            dists_km = utils.distance( gt_afr_df['lat'].values, gt_afr_df['lon'].values,\n",
    "                              ds.lat.values.ravel()[indexs], ds.lon.values.ravel()[indexs])\n",
    "\n",
    "            gt_afr_df.loc[:,'dist_from_grid_afr'] = dists_km*km\n",
    "\n",
    "            for observable in list(ds):\n",
    "                    gt_afr_df.loc[:,observable] = ds[observable].values.ravel()[gt_afr_df.loc[:,'grid_index_afr'].values] \n",
    "\n",
    "            gt_afr_df = gt_afr_df.rename({target:'GHF'})\n",
    "            gt_afr_df.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "            gt_afr_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            gt_afr_df.to_csv(training_f/f'Afr_{label}',  index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59616c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_world.to_netcdf(DIR/'Grids'/'Inputs'/\"ds_world.nc\", mode='w', \n",
    "                    engine='netcdf4')\n",
    "\n",
    "ds_afr_lr.to_netcdf(DIR/'Grids'/'Inputs'/\"ds_afr_lr.nc\", mode='w', \n",
    "                    engine='netcdf4')\n",
    "\n",
    "ds_afr_hr.to_netcdf(DIR/'Grids'/'Inputs'/\"ds_afr_hr.nc\", mode='w', \n",
    "                    engine='netcdf4')\n",
    "\n",
    "#ds_world = xr.load_dataset(DIR/'Grids'/'Inputs'/\"ds_world.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91097dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
