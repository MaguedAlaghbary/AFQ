{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557b37cd",
   "metadata": {},
   "source": [
    "# Key performance indicators and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebe301",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "#dir_p = Path().resolve() \n",
    "#dir_path = os.path.dirname(os.path.abspath(os.curdir))\n",
    "#os.chdir(dir_path)\n",
    "\n",
    "from agrid.grid import Grid\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "from scipy import stats, interpolate, spatial, io\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Arc \n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import pyproj as proj\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, max_error, \\\n",
    "median_absolute_error, mean_absolute_percentage_error, mean_poisson_deviance, mean_gamma_deviance\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer , r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(16, 9))\n",
    "\n",
    "from sklearn.feature_selection import RFECV,RFE\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbd194",
   "metadata": {},
   "source": [
    "define constants and dcitionaries to easy looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "\n",
    "#parent directory\n",
    "\n",
    "dir_p = Path().resolve() \n",
    "\n",
    "#constants\n",
    "km = 1000\n",
    "milli = 0.001\n",
    "micro = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "# create grid for each region\n",
    "# crs Coordinate reference system\n",
    "\n",
    "#EPSG is projection\n",
    "# 0.2 degrees equal roughly 20 km\n",
    "\n",
    "World = Grid(res=[0.2, 0.2], up=world_lat_max, down=world_lat_min)\n",
    "\n",
    "\n",
    "# africa grid low resolution 50 x 50 km\n",
    "\n",
    "Africa =    Grid(res=[0.5, 0.5],  left = afr_lon_min, right= afr_lon_max, up=afr_lat_max , down=afr_lat_min)\n",
    "\n",
    "\n",
    "#dictionary of all grids\n",
    "\n",
    "grids = {}\n",
    "\n",
    "grids['Afr'] = Africa\n",
    "grids['World'] = World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5077a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ease looping with dictionaries\n",
    "\n",
    "regions_a = [ 'Afr' ]\n",
    "\n",
    "\n",
    "regions_Total = ['World' ,'Afr']\n",
    "\n",
    "\n",
    "# raster exenets to adjust map\n",
    "raster_extent_Afr = [grids['Afr'].extent[0], grids['Afr'].extent[1], grids['Afr'].extent[3], grids['Afr'].extent[2]]\n",
    "raster_extent_World = [grids['World'].extent[0], grids['World'].extent[1], grids['World'].extent[3], grids['World'].extent[2]]\n",
    "\n",
    "# to correct plot maps\n",
    "raster_extents = {}\n",
    "\n",
    "raster_extents['Afr'] = raster_extent_Afr\n",
    "raster_extents['World'] = raster_extent_World\n",
    "\n",
    "\n",
    "# list of latitudes and longitudes\n",
    "lon_dict = {}\n",
    "lat_dict = {}\n",
    "\n",
    "lon_dict['Afr'] = [afr_lon_min, afr_lon_max]\n",
    "lon_dict['World'] = [world_lon_min, world_lon_max]\n",
    "\n",
    "lat_dict['Afr'] = [afr_lat_min, afr_lat_max]\n",
    "lat_dict['World'] = [world_lat_min, world_lat_max]\n",
    "\n",
    "\n",
    "\n",
    "print('terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeabornFig2Grid():\n",
    "\n",
    "    def __init__(self, seaborngrid, fig,  subplot_spec):\n",
    "        self.fig = fig\n",
    "        self.sg = seaborngrid\n",
    "        self.subplot = subplot_spec\n",
    "        if isinstance(self.sg, sns.axisgrid.FacetGrid) or \\\n",
    "            isinstance(self.sg, sns.axisgrid.PairGrid):\n",
    "            self._movegrid()\n",
    "        elif isinstance(self.sg, sns.axisgrid.JointGrid):\n",
    "            self._movejointgrid()\n",
    "        self._finalize()\n",
    "\n",
    "    def _movegrid(self):\n",
    "        \"\"\" Move PairGrid or Facetgrid \"\"\"\n",
    "        self._resize()\n",
    "        n = self.sg.axes.shape[0]\n",
    "        m = self.sg.axes.shape[1]\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(n,m, subplot_spec=self.subplot)\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                self._moveaxes(self.sg.axes[i,j], self.subgrid[i,j])\n",
    "\n",
    "    def _movejointgrid(self):\n",
    "        \"\"\" Move Jointgrid \"\"\"\n",
    "        h= self.sg.ax_joint.get_position().height\n",
    "        h2= self.sg.ax_marg_x.get_position().height\n",
    "        r = int(np.round(h/h2))\n",
    "        self._resize()\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(r+1,r+1, subplot_spec=self.subplot)\n",
    "\n",
    "        self._moveaxes(self.sg.ax_joint, self.subgrid[1:, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_x, self.subgrid[0, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_y, self.subgrid[1:, -1])\n",
    "\n",
    "    def _moveaxes(self, ax, gs):\n",
    "        #https://stackoverflow.com/a/46906599/4124317\n",
    "        ax.remove()\n",
    "        ax.figure=self.fig\n",
    "        self.fig.axes.append(ax)\n",
    "        self.fig.add_axes(ax)\n",
    "        ax._subplotspec = gs\n",
    "        ax.set_position(gs.get_position(self.fig))\n",
    "        ax.set_subplotspec(gs)\n",
    "\n",
    "    def _finalize(self):\n",
    "        plt.close(self.sg.fig)\n",
    "        self.fig.canvas.mpl_connect(\"resize_event\", self._resize)\n",
    "        self.fig.canvas.draw()\n",
    "\n",
    "    def _resize(self, evt=None):\n",
    "        self.sg.fig.set_size_inches(self.fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "''' \n",
    "obs['REF_n'] = [ 'MOHO','LAB', 'RHO_C', 'SV', 'PV', 'CTD',\n",
    "             'RHO_L', 'DEM', \n",
    "                'VOLC_DIST_W', 'A_MEDIAN_W', 'FA', 'SI','LITH_MANTLE', \n",
    "                'EMAG2_CLASS', 'GEOID', 'BG',\n",
    "              'GLIM']'''\n",
    "\n",
    "\n",
    "\n",
    "obs['OBS_REF'] = ['CTD' ,  'SI',\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            'GEOID','FA','DEM','BG', 'EMAG2_CLASS',\n",
    "                   'RHO_L', 'RHO_C', \n",
    "                  'VOLC_DIST_W', 'REG', 'GLIM']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "obs['LABELS'] = ['CTD',  'Shape index', 'LAB depth', 'Moho depth', \n",
    "                'S$_V$ 150km', 'P$_V$ 150km', \n",
    "                'Geoid', 'Free air', 'DEM', 'Bouguer', 'Mag.', \n",
    "                'Lith. ρ', 'Crust ρ',  \n",
    "                 'Volcano d.', 'GliM', 'REG', ]  \n",
    "    \n",
    "    \n",
    "# 'vp/vs'\n",
    "# Units to display in plots etc\n",
    "obs['UNITS'] = ['km',  'si', 'km', 'km',\n",
    "             '$\\delta$ v_s %','$\\delta$ v_p %', \n",
    "             'm', 'mGal', 'm', 'mGal',  'f(nT)', \n",
    "                 'kg/m$^3$', 'kg/m$^3$',\n",
    "                'km',  'class', 'class']\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs['V_RANGE'] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-250,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,15),]\n",
    "    \n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"topo\", ]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0,14,10,16,17,9, 2,1,5,13,12, 8,11,]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "#obs = obs.reindex(new_index)\n",
    "\n",
    "obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 12\n",
    "print(obs)\n",
    "\n",
    "n_obs = len(obs)\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66693080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"Concordance correlation coefficient.\"\"\"\n",
    "    # Remove NaNs\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    df = df.dropna()\n",
    "    y_true = df['y_true']\n",
    "    y_pred = df['y_pred']\n",
    "    # Pearson product-moment correlation coefficients\n",
    "    cor = np.corrcoef(y_true, y_pred)[0][1]\n",
    "    # Mean\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    # Variance\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    # Standard deviation\n",
    "    sd_true = np.std(y_true)\n",
    "    sd_pred = np.std(y_pred)\n",
    "    # Calculate CCC\n",
    "    numerator = 2 * cor * sd_true * sd_pred\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82870e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'heat-flow (mW/m2)'\n",
    "coord = ['lon', 'lat']\n",
    "grid_index = ['grid_index']\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex = []\n",
    "features_ghf = []\n",
    "\n",
    "\n",
    "\n",
    "features = obs['OBS_REF'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "in_features = set(features)\n",
    "\n",
    "features_ex = copy.deepcopy(features)\n",
    "features_ex.extend(coord)\n",
    "features_ex.extend(grid_index)\n",
    "\n",
    "features_ex.append(target)\n",
    "\n",
    "features_ghf = copy.deepcopy(features)\n",
    "features_ghf.append(target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34e259",
   "metadata": {},
   "source": [
    "# comaprison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "files = [\n",
    "         'OD_ra',\n",
    "         'NOD_ra', \n",
    "\n",
    "        ]\n",
    "\n",
    "observables = [    \n",
    "            features,\n",
    "            features,\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file_label, observable in zip(files, observables):\n",
    "    \n",
    "     \n",
    "    KPI_df = pd.DataFrame()\n",
    "\n",
    "    #### training\n",
    "    tarin_w_f =  dir_p/ 'data'/'dataset'/'Preprocessed'/f'Training_W_{file_label}.csv'\n",
    "\n",
    "    train_W = pd.read_csv(tarin_w_f, sep='\\t')\n",
    "\n",
    "    \n",
    "    X = train_W[observable] \n",
    "    y = train_W[target]\n",
    "    X['GLIM']  = X['GLIM'].astype('int').astype('category')\n",
    "    X['REG']  = X['REG'].astype('int').astype('category')\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # hyperparameter\n",
    "    bs_rfr_hyp =  dir_p/'Hyperparameters'/f'RFR_{file_label}.csv'\n",
    "\n",
    "\n",
    "    bs_rfr_hyp_df = pd.read_csv(bs_rfr_hyp, sep='\\t')\n",
    "\n",
    "\n",
    "    best_params = bs_rfr_hyp_df.to_dict('list')\n",
    "\n",
    "\n",
    "    # Load hyper parameter \n",
    "\n",
    "    regressor = RandomForestRegressor()\n",
    "\n",
    "    tuned_params = {item[11:]: best_params[item][0] for item in best_params}\n",
    "    regressor.set_params(**tuned_params)\n",
    "\n",
    "    \n",
    "    scaler = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "\n",
    "\n",
    "    numeric_transformer = scaler\n",
    "\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "       (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", regressor)]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline= Pipeline(steps = steps)\n",
    "\n",
    "    model_pipeline = TransformedTargetRegressor(regressor=pipeline, transformer=scaler)\n",
    "\n",
    "\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = model_pipeline.predict(X_test)\n",
    "\n",
    "    print('optimized')\n",
    "\n",
    "    errors      = abs(y_test - y_predict)\n",
    "    mape        = round( mean_absolute_percentage_error(y_test, y_predict), 2)  \n",
    "    accuracy    = round(100 - mape*100, 2)\n",
    "    mae         = round(mean_absolute_error(y_test, y_predict ), 2)\n",
    "    rmse        = round(mean_squared_error(y_test, y_predict , squared=False), 2)\n",
    "    nrmse       = round( rmse/(y_test.mean()) , 2)\n",
    "    r2          = round(r2_score(y_test, y_predict ), 2)\n",
    "    ev          = round( explained_variance_score(y_test, y_predict), 2)\n",
    "    max_e       = round( max_error(y_test, y_predict), 2)\n",
    "    min_e       = round( errors.min(), 2)\n",
    "    mnae        = round( median_absolute_error(y_test, y_predict), 2) \n",
    "    #mpd         = round( mean_poisson_deviance(y_test, y_predict), 2) \n",
    "    #mgd         = round( mean_gamma_deviance(y_test, y_predict), 2) \n",
    "    # # Mean Absolute Error (MAE)\n",
    "    mpe         =  round(np.mean((y_test -y_predict)/y_test) , 2)\n",
    "    ccc         =   round( concordance_correlation_coefficient(y_test, y_predict), 2) \n",
    "\n",
    "\n",
    "\n",
    "    KPI_df.loc['NRMSE', f'{file_label}'] = nrmse\n",
    "    KPI_df.loc['RMSE', f'{file_label}'] = rmse\n",
    "    KPI_df.loc['MAE', f'{file_label}'] = mae\n",
    "    KPI_df.loc['MAPE', f'{file_label}'] = mape\n",
    "    KPI_df.loc['CD', f'{file_label}'] = r2\n",
    "    KPI_df.loc['EV', f'{file_label}'] = ev\n",
    "    KPI_df.loc['MAX_E', f'{file_label}'] = max_e\n",
    "    KPI_df.loc['MIN_E', f'{file_label}'] = min_e\n",
    "    KPI_df.loc['MedAE', f'{file_label}'] = mnae\n",
    "    KPI_df.loc['MPE', f'{file_label}'] = mpe\n",
    "    KPI_df.loc['MAX', f'{file_label}'] = round(y_predict.max(),1)\n",
    "    KPI_df.loc['MIN', f'{file_label}'] = y_predict.min()\n",
    "    KPI_df.loc['ACC', f'{file_label}'] = accuracy\n",
    "    KPI_df.loc['Mean', f'{file_label}'] = round(np.mean(y_predict),2)\n",
    "    KPI_df.loc['Median', f'{file_label}'] = round(np.median(y_predict),2)\n",
    "    KPI_df.loc['Stdev', f'{file_label}'] = round(np.std(y_predict),2)\n",
    "    KPI_df.loc['RSD', f'{file_label}'] = round(np.std(y_predict) / np.mean(y_predict) ,2)\n",
    "    KPI_df.loc['CCC', f'{file_label}'] = ccc\n",
    "    #save results\n",
    "    kpi_f =  dir_p/'KPI'/f'KPI_{file_label}.csv'\n",
    "\n",
    "    KPI_df.to_csv(kpi_f , sep='\\t')\n",
    "    \n",
    "    \n",
    "    print(f'terminated {file_label}')\n",
    "\n",
    "print('terminated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0cc4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "KPI_df = pd.DataFrame()\n",
    "files = [filename for filename in os.listdir(dir_p/'KPI') if 'ra' in filename]\n",
    "for file in zip(files):\n",
    "    path = dir_p/'KPI'/f'{file[0]}'\n",
    "    tmp_df = pd.read_csv(path, index_col=0, sep='\\t')\n",
    "    KPI_df = pd.concat([tmp_df, KPI_df], axis=1)\n",
    "    \n",
    "#KPI_df.columns = ['Without Anomalies Ln', 'Without Anomalies IDW', 'With Anomalies Ln', 'With Anomalies IDW']\n",
    "\n",
    "# Initialize auc_score dictionary\n",
    "\n",
    "# Set figure size and create barplot\n",
    "\n",
    "fig, axs = plt.subplots(8,2,figsize=(16, 50))\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\n",
    "#fig.set_size_inches(30,30)\n",
    "\n",
    "#scores = ['RMSE', 'NRMSE','MAE', 'MAPE', 'CD','EV', 'MAX_E','MIN_E' ,'MedAE', 'MPE']\n",
    "scores = ['RMSE', 'MAE', 'NRMSE','MAPE', 'CD','EV', 'MAX_E', 'MAX', 'Stdev', \n",
    "          'RSD', 'Mean','Median','MedAE', 'MPE', 'ACC']\n",
    "\n",
    "#subfile = 'HOD'\n",
    "#files_rg = [ x for x in files_rg if subfile in x ]\n",
    "for score, ax in zip(scores, axs.flatten()):\n",
    "\n",
    "\n",
    "\n",
    "    if score in ['CD','EV' ,'MAX', 'MPE', 'ACC','CCC']:\n",
    "        x_data = KPI_df.loc[score,:].sort_values(ascending=False).index\n",
    "        y_data = KPI_df.loc[score,:].sort_values(ascending=False)\n",
    "        bar = sns.barplot(y_data[:], x_data[:] ,     ax=ax)\n",
    "        bar.bar_label(ax.containers[0])\n",
    "\n",
    "\n",
    "\n",
    "        # Turn frame off\n",
    "        ax.set_frame_on(False)\n",
    "\n",
    "        #ax.legend(loc='upper left')\n",
    "\n",
    "        # Tight layout\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "        x_data = KPI_df.loc[score,:].sort_values().index\n",
    "        y_data = KPI_df.loc[score,:].sort_values()\n",
    "        bar = sns.barplot(y_data[:], x_data[:] ,     ax=ax)\n",
    "        bar.bar_label(ax.containers[0], )\n",
    "\n",
    "\n",
    "\n",
    "        # Turn frame off\n",
    "        ax.set_frame_on(False)\n",
    "\n",
    "        #ax.legend(loc='upper left')\n",
    "\n",
    "        # Tight layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save Figure\n",
    "#plt.savefig(f\"{score.__name__} Scores.png\", dpi = 1080)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Figure\n",
    "#plt.savefig(f\"{score.__name__} Scores.png\", dpi = 1080)\n",
    "\n",
    "# get data\n",
    "\n",
    "rating = 'ra'\n",
    "outlier = 'OD'\n",
    "\n",
    "file_label = f'{outlier}_{rating}'\n",
    "\n",
    "tarin_w_f =  dir_p/ 'data'/'dataset'/'Preprocessed'/f'Training_W_{file_label}.csv'\n",
    "train_W = pd.read_csv(tarin_w_f, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "train_W_data = train_W[features_ex]\n",
    "\n",
    "print(train_W_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "train_W_data = train_W[features_ex]\n",
    "\n",
    "print(train_W_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "X = train_W_data[features]\n",
    "y = train_W_data[target].values.reshape(-1,1) \n",
    "X['GLIM']  = X['GLIM'].astype('int').astype('category')\n",
    "X['REG']  = X['REG'].astype('int').astype('category')\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train.describe(include='all')\n",
    "\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0f08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
