{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f0ea51",
   "metadata": {},
   "source": [
    "# 3. Expolartory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb95990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28797ccf",
   "metadata": {},
   "source": [
    "naming convetion\n",
    "\n",
    "NOD : No Outlier Detection\n",
    "\n",
    "OD : Outlier Detection\n",
    "\n",
    "ra : rating best code A\n",
    "\n",
    "rab : rating good code A +B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os, sys, pickle, copy, pygmt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# load the dataset\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer , r2_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder, PowerTransformer\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8a8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constanst\n",
    "crs_to='epsg:4326'\n",
    "crs_from='epsg:4326'\n",
    "projection = 'M5.4i'\n",
    "#parent directory\n",
    "\n",
    "DIR = Path().resolve() \n",
    "\n",
    "\n",
    "# We can exclude Arctic ocean and Antarctica, as there are no HF measurements to use\n",
    "world_lon_min, world_lon_max, world_lat_min, world_lat_max  = -180, 180, -60, 80\n",
    "\n",
    "# map extents of Africa and Australia\n",
    "afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max =  -20, 52, -37 , 38  \n",
    "\n",
    "\n",
    "# create grid for each region\n",
    "# crs Coordinate reference system\n",
    "\n",
    "#EPSG is projection\n",
    "# 0.2 degrees equal roughly 20 km\n",
    "\n",
    "region_afr = [afr_lon_min, afr_lon_max, afr_lat_min, afr_lat_max]\n",
    "region_world = [world_lon_min, world_lon_max, world_lat_min, world_lat_max]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1569405b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBS_AFR</th>\n",
       "      <th>LABELS_gmt</th>\n",
       "      <th>LABELS</th>\n",
       "      <th>UNITS</th>\n",
       "      <th>UNITS_gmt</th>\n",
       "      <th>V_RANGE</th>\n",
       "      <th>V_RANGE_AFR</th>\n",
       "      <th>CMAPS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBS_REF</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CTD</th>\n",
       "      <td>CTD</td>\n",
       "      <td>CTD</td>\n",
       "      <td>CTD</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(0, 50)</td>\n",
       "      <td>(0, 50)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAB</th>\n",
       "      <td>LAB</td>\n",
       "      <td>LAB</td>\n",
       "      <td>LAB</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(0, 300)</td>\n",
       "      <td>(50, 250)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI</th>\n",
       "      <td>SI</td>\n",
       "      <td>Shape index</td>\n",
       "      <td>Shape index</td>\n",
       "      <td>si</td>\n",
       "      <td>si</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>(-1, 1)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOHO</th>\n",
       "      <td>MOHO</td>\n",
       "      <td>Moho</td>\n",
       "      <td>Moho</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(15, 60)</td>\n",
       "      <td>(20, 50)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SV</th>\n",
       "      <td>SV_SPEED</td>\n",
       "      <td>S@_v@ 150km</td>\n",
       "      <td>$S_v$ @150km</td>\n",
       "      <td>$\\delta$$S_v$ %</td>\n",
       "      <td>km/s</td>\n",
       "      <td>(-0.075, 0.075)</td>\n",
       "      <td>(-0.075, 0.075)</td>\n",
       "      <td>SCM/roma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PV</th>\n",
       "      <td>PV_SPEED</td>\n",
       "      <td>P@_v@ 150km</td>\n",
       "      <td>$P_v$ @150km</td>\n",
       "      <td>$\\delta$$P_v$ %</td>\n",
       "      <td>km/s</td>\n",
       "      <td>(-0.02, 0.02)</td>\n",
       "      <td>(-0.02, 0.02)</td>\n",
       "      <td>SCM/roma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEM</th>\n",
       "      <td>DEM</td>\n",
       "      <td>DEM</td>\n",
       "      <td>DEM</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>(-2200, 2200)</td>\n",
       "      <td>(-2200, 2200)</td>\n",
       "      <td>SCM/oleron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOID</th>\n",
       "      <td>GEOID</td>\n",
       "      <td>Geoid</td>\n",
       "      <td>Geoid</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>(-45, 45)</td>\n",
       "      <td>(-45, 45)</td>\n",
       "      <td>SCM/bamako</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMAG2_CLASS</th>\n",
       "      <td>EMAG2</td>\n",
       "      <td>Mag.</td>\n",
       "      <td>Mag.</td>\n",
       "      <td>f(nT)</td>\n",
       "      <td>f(nT)</td>\n",
       "      <td>(-0.4, 0.4)</td>\n",
       "      <td>(-200, 200)</td>\n",
       "      <td>SCM/bilbao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FA</th>\n",
       "      <td>FA</td>\n",
       "      <td>Free air</td>\n",
       "      <td>Free air</td>\n",
       "      <td>mGal</td>\n",
       "      <td>mGal</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BG</th>\n",
       "      <td>BG</td>\n",
       "      <td>Bouguer</td>\n",
       "      <td>Bouguer</td>\n",
       "      <td>mGal</td>\n",
       "      <td>mGal</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>(-100, 100)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RHO_L</th>\n",
       "      <td>RHO_L</td>\n",
       "      <td>Lith. rho</td>\n",
       "      <td>Lith. ρ</td>\n",
       "      <td>kg/m$^3$</td>\n",
       "      <td>kg/m@+3@+</td>\n",
       "      <td>(3260, 3360)</td>\n",
       "      <td>(3260, 3360)</td>\n",
       "      <td>SCM/batlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RHO_C</th>\n",
       "      <td>RHO_C</td>\n",
       "      <td>Crust rho</td>\n",
       "      <td>Crust ρ</td>\n",
       "      <td>kg/m$^3$</td>\n",
       "      <td>kg/m@+3@+</td>\n",
       "      <td>(2650, 2950)</td>\n",
       "      <td>(2650, 2950)</td>\n",
       "      <td>SCM/batlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLC_DIST_W</th>\n",
       "      <td>VOLC_DIST</td>\n",
       "      <td>Volcano</td>\n",
       "      <td>Volcano</td>\n",
       "      <td>km</td>\n",
       "      <td>km</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 100)</td>\n",
       "      <td>SCM/broc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REG</th>\n",
       "      <td>REG</td>\n",
       "      <td>REG</td>\n",
       "      <td>REG</td>\n",
       "      <td>class</td>\n",
       "      <td>class</td>\n",
       "      <td>(1, 6)</td>\n",
       "      <td>(1, 6)</td>\n",
       "      <td>gmt/categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLIM</th>\n",
       "      <td>GLIM</td>\n",
       "      <td>GliM</td>\n",
       "      <td>GliM</td>\n",
       "      <td>class</td>\n",
       "      <td>class</td>\n",
       "      <td>(1, 16)</td>\n",
       "      <td>(1, 15)</td>\n",
       "      <td>gmt/categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               OBS_AFR   LABELS_gmt        LABELS            UNITS  UNITS_gmt          V_RANGE      V_RANGE_AFR            CMAPS\n",
       "OBS_REF                                                                                                                         \n",
       "CTD                CTD          CTD           CTD               km         km          (0, 50)          (0, 50)       SCM/bamako\n",
       "LAB                LAB          LAB           LAB               km         km         (0, 300)        (50, 250)       SCM/bamako\n",
       "SI                  SI  Shape index   Shape index               si         si          (-1, 1)          (-1, 1)         SCM/broc\n",
       "MOHO              MOHO         Moho          Moho               km         km         (15, 60)         (20, 50)       SCM/bamako\n",
       "SV            SV_SPEED  S@_v@ 150km  $S_v$ @150km  $\\delta$$S_v$ %       km/s  (-0.075, 0.075)  (-0.075, 0.075)         SCM/roma\n",
       "PV            PV_SPEED  P@_v@ 150km  $P_v$ @150km  $\\delta$$P_v$ %       km/s    (-0.02, 0.02)    (-0.02, 0.02)         SCM/roma\n",
       "DEM                DEM          DEM           DEM                m          m    (-2200, 2200)    (-2200, 2200)       SCM/oleron\n",
       "GEOID            GEOID        Geoid         Geoid                m          m        (-45, 45)        (-45, 45)       SCM/bamako\n",
       "EMAG2_CLASS      EMAG2         Mag.          Mag.            f(nT)      f(nT)      (-0.4, 0.4)      (-200, 200)       SCM/bilbao\n",
       "FA                  FA     Free air      Free air             mGal       mGal      (-100, 100)      (-100, 100)         SCM/broc\n",
       "BG                  BG      Bouguer       Bouguer             mGal       mGal      (-100, 100)      (-100, 100)         SCM/broc\n",
       "RHO_L            RHO_L    Lith. rho       Lith. ρ         kg/m$^3$  kg/m@+3@+     (3260, 3360)     (3260, 3360)       SCM/batlow\n",
       "RHO_C            RHO_C    Crust rho       Crust ρ         kg/m$^3$  kg/m@+3@+     (2650, 2950)     (2650, 2950)       SCM/batlow\n",
       "VOLC_DIST_W  VOLC_DIST      Volcano       Volcano               km         km           (0, 1)         (0, 100)         SCM/broc\n",
       "REG                REG          REG           REG            class      class           (1, 6)           (1, 6)  gmt/categorical\n",
       "GLIM              GLIM         GliM          GliM            class      class          (1, 16)          (1, 15)  gmt/categorical"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = pd.DataFrame()\n",
    "\n",
    "\n",
    "obs[\"OBS_REF\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV\",\"PV\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2_CLASS\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST_W\", \"REG\", \"GLIM\"]\n",
    "\n",
    "obs[\"OBS_AFR\"] = [\"CTD\" ,  \"SI\",\"LAB\", \"MOHO\",\n",
    "            \"SV_SPEED\",\"PV_SPEED\", \n",
    "            \"GEOID\",\"FA\",\"DEM\",\"BG\", \"EMAG2\",\n",
    "                   \"RHO_L\", \"RHO_C\", \n",
    "                  \"VOLC_DIST\", \"REG\", \"GLIM\"]\n",
    "  \n",
    "     \n",
    "# Labels for plots etc\n",
    "\n",
    "\n",
    "# Labels for plots etc\n",
    "obs[\"LABELS_gmt\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"S@_v@ 150km\", \"P@_v@ 150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. rho\", \"Crust rho\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]  \n",
    "\n",
    "\n",
    "obs[\"LABELS\"] = [\"CTD\",  \"Shape index\", \"LAB\", \"Moho\", \n",
    "                \"$S_v$ @150km\", \"$P_v$ @150km\", \n",
    "                \"Geoid\", \"Free air\", \"DEM\", \"Bouguer\", \"Mag.\", \n",
    "                \"Lith. ρ\", \"Crust ρ\",  \n",
    "                 \"Volcano\", \"REG\", \"GliM\", ]\n",
    "    \n",
    "# \"vp/vs\"\n",
    "# Units to display in plots etc\n",
    "obs[\"UNITS\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"$\\delta$$S_v$ %\",\"$\\delta$$P_v$ %\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m$^3$\", \"kg/m$^3$\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "\n",
    "\n",
    "\n",
    "obs[\"UNITS_gmt\"] = [\"km\",  \"si\", \"km\", \"km\",\n",
    "             \"km/s\",\"km/s\", \n",
    "             \"m\", \"mGal\", \"m\", \"mGal\",  \"f(nT)\", \n",
    "                 \"kg/m@+3@+\", \"kg/m@+3@+\",\n",
    "                \"km\",  \"class\", \"class\"]\n",
    "        \n",
    "# Range of colormap for plots. Similar data are placed in same ranges for consistancy\n",
    "obs[\"V_RANGE\"] = [(0,50), (-1,1),(0,300),(15,60),\n",
    "              (-0.075,0.075), (-0.02,0.02), \n",
    "              (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-0.4, 0.4), \n",
    "                   (3260, 3360), (2650, 2950),\n",
    "                  (0,1), (1,6),(1,16),]\n",
    "\n",
    "\n",
    "    \n",
    "obs[\"V_RANGE_AFR\"] = [(0,50), (-1,1),(50,250),(20,50),\n",
    "          (-0.075,0.075), (-0.02,0.02), \n",
    "          (-45,45), (-100,100) , (-2200, 2200),(-100,100),  (-200, 200), \n",
    "               (3260, 3360), (2650, 2950),\n",
    "              (0,100), (1,6),(1,15),]\n",
    "\n",
    "\n",
    "obs[\"CMAPS\"] = [\"batlow\",  \"broc\", \"bamako\", \"batlow\", \n",
    "             \"roma\",\"roma\", \n",
    "             \"bamako\", \"broc\", \"bukavu\", \"broc\", \"batlow\",            \n",
    "                \"batlow\", \"batlow\",\n",
    "               \"bamako\",  \"batlowS\",\"categorical\", ]\n",
    "\n",
    "obs[\"CMAPS\"] = [\"SCM/bamako\",  \"SCM/broc\", \"SCM/bamako\", \"SCM/bamako\", \n",
    "             \"SCM/roma\",\"SCM/roma\", \n",
    "             \"SCM/bamako\", \"SCM/broc\", \"SCM/oleron\", \"SCM/broc\", \"SCM/bilbao\",            \n",
    "                \"SCM/batlow\", \"SCM/batlow\",\n",
    "               \"SCM/broc\",  \"gmt/categorical\",\"gmt/categorical\", ]\n",
    "\n",
    "new_index = [0,2,1, 3,4,5,8,6,10, 7,9,11,12,13,14,15]\n",
    "\n",
    "#new_index = [4,3,15,6,7,0, 14, 10,16, 8, 9,2, 13, 12, 8, 11, ]\n",
    "\n",
    "obs = obs.reindex(new_index)\n",
    "\n",
    "#obs.index = np.arange(0,len(obs))\n",
    "\n",
    "pd.options.display.width = 370\n",
    "pd.options.display.max_colwidth = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs_dict = obs.to_dict(orient='records')\n",
    "\n",
    "obs.set_index(['OBS_REF'], inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0963ff",
   "metadata": {},
   "source": [
    "import gridded data to the grid not necessary step just grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610aa1d",
   "metadata": {},
   "source": [
    "# 3 - Convert to correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f6d33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTD',\n",
       " 'LAB',\n",
       " 'SI',\n",
       " 'MOHO',\n",
       " 'SV',\n",
       " 'PV',\n",
       " 'DEM',\n",
       " 'GEOID',\n",
       " 'EMAG2_CLASS',\n",
       " 'FA',\n",
       " 'BG',\n",
       " 'RHO_L',\n",
       " 'RHO_C',\n",
       " 'VOLC_DIST_W',\n",
       " 'REG',\n",
       " 'GLIM',\n",
       " 'lon',\n",
       " 'lat',\n",
       " 'grid_index_afr',\n",
       " 'GHF']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target = 'GHF'\n",
    "coord = ['lon', 'lat']\n",
    "grid_index_world = 'grid_index_world'\n",
    "grid_index_afr = 'grid_index_afr'\n",
    "\n",
    "\n",
    "ghf_lower_bound = 25\n",
    "ghf_upper_bound =100\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "features_ex_world = []\n",
    "features_ex_afr = []\n",
    "features_ghf = []\n",
    "\n",
    "\n",
    "\n",
    "features = obs.index.to_list()\n",
    "\n",
    "\n",
    "\n",
    "in_features = set(features)\n",
    "\n",
    "features_ex_world = copy.deepcopy(features)\n",
    "features_ex_world.extend(coord)\n",
    "features_ex_world.append(grid_index_world)\n",
    "features_ex_world.append(target)\n",
    "\n",
    "features_ex_afr = copy.deepcopy(features)\n",
    "features_ex_afr.extend(coord)\n",
    "features_ex_afr.append(grid_index_afr)\n",
    "features_ex_afr.append(target)\n",
    "\n",
    "features_ghf = copy.deepcopy(features)\n",
    "features_ghf.append(target)\n",
    "\n",
    "features_ex_afr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c555b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "### input####\n",
    "\n",
    "w_ra_i =    DIR /'Dataset'/'Preprocessed'/'W_int_ra.csv' \n",
    "\n",
    "w_rab_i =   DIR /'Dataset'/'Preprocessed'/'W_int_rab.csv'\n",
    "\n",
    "Afr_ra_lr_i =   DIR /'Dataset'/'Preprocessed'/'Afr_int_ra_lr.csv'\n",
    "\n",
    "Afr_rab_lr_i =   DIR /'Dataset'/'Preprocessed'/'Afr_int_rab_lr.csv'\n",
    "\n",
    "Afr_ra_hr_i =   DIR /'Dataset'/'Preprocessed'/'Afr_int_ra_hr.csv'\n",
    "\n",
    "Afr_rab_hr_i =   DIR /'Dataset'/'Preprocessed'/'Afr_int_rab_hr.csv'\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "hq_W_ra = pd.read_csv(w_ra_i, sep='\\t')\n",
    "\n",
    "hq_W_rab = pd.read_csv(w_rab_i, sep='\\t')\n",
    "\n",
    "\n",
    "hq_Afr_ra_lr = pd.read_csv(Afr_ra_lr_i, sep='\\t')\n",
    "\n",
    "hq_Afr_rab_lr = pd.read_csv(Afr_rab_lr_i, sep='\\t')\n",
    "\n",
    "hq_Afr_ra_hr = pd.read_csv(Afr_ra_hr_i, sep='\\t')\n",
    "\n",
    "hq_Afr_rab_hr = pd.read_csv(Afr_rab_hr_i, sep='\\t')\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "hq_W_ra['GLIM']  = hq_W_ra['GLIM'].astype('int').astype('category')\n",
    "hq_W_rab['GLIM']  = hq_W_rab['GLIM'].astype('int').astype('category')\n",
    "\n",
    "hq_W_ra['REG']  = hq_W_ra['REG'].astype('int').astype('category')\n",
    "hq_W_rab['REG']  = hq_W_rab['REG'].astype('int').astype('category')\n",
    "\n",
    "\n",
    "hq_Afr_ra_lr['GLIM']  = hq_Afr_ra_lr['GLIM'].astype('int').astype('category')\n",
    "hq_Afr_rab_lr['GLIM']  = hq_Afr_rab_lr['GLIM'].astype('int').astype('category')\n",
    "\n",
    "hq_Afr_ra_lr['REG']  = hq_Afr_ra_lr['REG'].astype('int').astype('category')\n",
    "hq_Afr_rab_lr['REG']  = hq_Afr_rab_lr['REG'].astype('int').astype('category')\n",
    "\n",
    "hq_Afr_ra_hr['GLIM']  = hq_Afr_ra_hr['GLIM'].astype('int').astype('category')\n",
    "hq_Afr_rab_hr['GLIM']  = hq_Afr_rab_hr['GLIM'].astype('int').astype('category')\n",
    "\n",
    "hq_Afr_ra_hr['REG']  = hq_Afr_ra_hr['REG'].astype('int').astype('category')\n",
    "hq_Afr_rab_hr['REG']  = hq_Afr_rab_hr['REG'].astype('int').astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c938f31",
   "metadata": {},
   "source": [
    "# 4- Replace missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82d675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hq_W_cp_ra = hq_W_ra.loc[hq_W_ra[features_ghf].dropna().index, features_ex_world].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "hq_W_cp_rab = hq_W_rab.loc[hq_W_rab[features_ghf].dropna().index, features_ex_world].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "hq_Afr_cp_ra_lr = hq_Afr_ra_lr.loc[hq_Afr_ra_lr[features_ghf].dropna().index, features_ex_afr].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "hq_Afr_cp_rab_lr = hq_Afr_rab_lr.loc[hq_Afr_rab_lr[features_ghf].dropna().index, features_ex_afr].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "\n",
    "hq_Afr_cp_ra_hr = hq_Afr_ra_hr.loc[hq_Afr_ra_hr[features_ghf].dropna().index, features_ex_afr].reset_index(drop=True).copy(deep=True)\n",
    "\n",
    "hq_Afr_cp_rab_hr = hq_Afr_rab_hr.loc[hq_Afr_rab_hr[features_ghf].dropna().index, features_ex_afr].reset_index(drop=True).copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a9a67",
   "metadata": {},
   "source": [
    "# 5- outliers detection for traget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc27d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hq_W_cp_ra           : len: 5792; max 787.5; min 0.8\n",
      "hq_W_cp_ra_nod       : len: 5729; max 197.0; min 0.8\n",
      "hq_W_cp_rab          : len: 12707; max 5146.0; min -3.0\n",
      "hq_W_cp_rab_nod      : len: 12591; max 199.0; min 0.8\n",
      "hq_Afr_cp_ra_lr      : len: 925; max 787.5; min 6.0\n",
      "hq_Afr_cp_ra_lr_nod  : len: 879; max 197.0; min 6.0\n",
      "hq_Afr_cp_rab_lr     : len: 1814; max 787.5; min 1.0\n",
      "hq_Afr_cp_rab_lr_nod : len: 1766; max 197.7; min 1.0\n",
      "hq_Afr_cp_ra_hr      : len: 925; max 787.5; min 6.0\n",
      "hq_Afr_cp_ra_hr_nod  : len: 879; max 197.0; min 6.0\n",
      "hq_Afr_cp_rab_hr: len: 1814; max 787.5; min 1.0\n",
      "hq_Afr_cp_rab_hr_nod : len: 1766; max 197.7; min 1.0\n"
     ]
    }
   ],
   "source": [
    "# drop values greater than 200 labeled _nod\n",
    "\n",
    "\n",
    "\n",
    "hq_W_cp_ra_nod  = hq_W_cp_ra.drop(hq_W_cp_ra[hq_W_cp_ra[target] >= ghf_upper_bound].index)\n",
    "hq_W_cp_ra_nod  = hq_W_cp_ra_nod.drop(hq_W_cp_ra_nod[hq_W_cp_ra_nod[target] <= ghf_lower_bound].index)\n",
    "\n",
    "hq_W_cp_rab_nod  = hq_W_cp_rab.drop(hq_W_cp_rab[hq_W_cp_rab[target] >= ghf_upper_bound].index)\n",
    "hq_W_cp_rab_nod  = hq_W_cp_rab_nod.drop(hq_W_cp_rab_nod[hq_W_cp_rab_nod[target] <= ghf_lower_bound].index)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "hq_Afr_cp_ra_lr_nod = hq_Afr_cp_ra_lr.drop(hq_Afr_cp_ra_lr[hq_Afr_cp_ra_lr[target] >= ghf_upper_bound].index)\n",
    "hq_Afr_cp_ra_lr_nod = hq_Afr_cp_ra_lr_nod.drop(hq_Afr_cp_ra_lr_nod[hq_Afr_cp_ra_lr_nod[target] <= ghf_lower_bound].index)\n",
    "\n",
    "hq_Afr_cp_rab_lr_nod = hq_Afr_cp_rab_lr.drop(hq_Afr_cp_rab_lr[hq_Afr_cp_rab_lr[target] >= ghf_upper_bound].index)\n",
    "hq_Afr_cp_rab_lr_nod = hq_Afr_cp_rab_lr_nod.drop(hq_Afr_cp_rab_lr_nod[hq_Afr_cp_rab_lr_nod[target] <= ghf_lower_bound].index)\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "hq_Afr_cp_ra_hr_nod = hq_Afr_cp_ra_hr.drop(hq_Afr_cp_ra_hr[hq_Afr_cp_ra_hr[target] >= ghf_upper_bound].index)\n",
    "hq_Afr_cp_ra_hr_nod = hq_Afr_cp_ra_hr_nod.drop(hq_Afr_cp_ra_hr_nod[hq_Afr_cp_ra_hr_nod[target] <= ghf_lower_bound].index)\n",
    "\n",
    "hq_Afr_cp_rab_hr_nod = hq_Afr_cp_rab_hr.drop(hq_Afr_cp_rab_hr[hq_Afr_cp_rab_hr[target] >= ghf_upper_bound].index)\n",
    "hq_Afr_cp_rab_hr_nod = hq_Afr_cp_rab_hr_nod.drop(hq_Afr_cp_rab_hr_nod[hq_Afr_cp_rab_hr_nod[target] <= ghf_lower_bound].index)\n",
    "\n",
    "\n",
    "\n",
    "print(f'hq_W_cp_ra           : len: {len(hq_W_cp_ra)}; max {hq_W_cp_ra[target].max()}; min {hq_W_cp_ra[target].min()}')\n",
    "print(f'hq_W_cp_ra_nod       : len: {len(hq_W_cp_ra_nod)}; max {hq_W_cp_ra_nod[target].max()}; min {hq_W_cp_ra_nod[target].min()}')\n",
    "print(f'hq_W_cp_rab          : len: {len(hq_W_cp_rab)}; max {hq_W_cp_rab[target].max()}; min {hq_W_cp_rab[target].min()}')\n",
    "print(f'hq_W_cp_rab_nod      : len: {len(hq_W_cp_rab_nod)}; max {hq_W_cp_rab_nod[target].max()}; min {hq_W_cp_rab_nod[target].min()}')\n",
    "print(f'hq_Afr_cp_ra_lr      : len: {len(hq_Afr_cp_ra_lr)}; max {hq_Afr_cp_ra_lr[target].max()}; min {hq_Afr_cp_ra_lr[target].min()}')\n",
    "print(f'hq_Afr_cp_ra_lr_nod  : len: {len(hq_Afr_cp_ra_lr_nod)}; max {hq_Afr_cp_ra_lr_nod[target].max()}; min {hq_Afr_cp_ra_lr_nod[target].min()}')\n",
    "print(f'hq_Afr_cp_rab_lr     : len: {len(hq_Afr_cp_rab_lr)}; max {hq_Afr_cp_rab_lr[target].max()}; min {hq_Afr_cp_rab_lr[target].min()}')\n",
    "print(f'hq_Afr_cp_rab_lr_nod : len: {len(hq_Afr_cp_rab_lr_nod)}; max {hq_Afr_cp_rab_lr_nod[target].max()}; min {hq_Afr_cp_rab_lr_nod[target].min()}')\n",
    "print(f'hq_Afr_cp_ra_hr      : len: {len(hq_Afr_cp_ra_hr)}; max {hq_Afr_cp_ra_hr[target].max()}; min {hq_Afr_cp_ra_hr[target].min()}')\n",
    "print(f'hq_Afr_cp_ra_hr_nod  : len: {len(hq_Afr_cp_ra_hr_nod)}; max {hq_Afr_cp_ra_hr_nod[target].max()}; min {hq_Afr_cp_ra_hr_nod[target].min()}')\n",
    "print(f'hq_Afr_cp_rab_hr: len: {len(hq_Afr_cp_rab_hr)}; max {hq_Afr_cp_rab_hr[target].max()}; min {hq_Afr_cp_rab_hr[target].min()}')\n",
    "print(f'hq_Afr_cp_rab_hr_nod : len: {len(hq_Afr_cp_rab_hr_nod)}; max {hq_Afr_cp_rab_hr_nod[target].max()}; min {hq_Afr_cp_rab_hr_nod[target].min()}')\n",
    "\n",
    "\n",
    "\n",
    "# save datasets for training\n",
    "\n",
    "\n",
    "### ra\n",
    "training_w_NOD_ra_nod  =    DIR /'Dataset'/'Preprocessed'/'W_NOD_ra.csv'\n",
    "\n",
    "hq_Afr_NOD_ra_lr_nod  =   DIR /'Dataset'/'Preprocessed'/'Afr_NOD_ra_lr.csv'\n",
    "hq_Afr_NOD_ra_hr_nod  =   DIR /'Dataset'/'Preprocessed'/'Afr_NOD_ra_hr.csv'\n",
    "\n",
    "\n",
    "hq_W_cp_ra_nod.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hq_W_cp_ra_nod.reset_index( inplace=True,drop=True)\n",
    "hq_W_cp_ra_nod.to_csv(training_w_NOD_ra_nod , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "hq_Afr_cp_ra_lr_nod.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hq_Afr_cp_ra_lr_nod.reset_index( inplace=True,drop=True)\n",
    "hq_Afr_cp_ra_lr_nod.to_csv(hq_Afr_NOD_ra_lr_nod , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "hq_Afr_cp_ra_hr_nod.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hq_Afr_cp_ra_hr_nod.reset_index( inplace=True,drop=True)\n",
    "hq_Afr_cp_ra_hr_nod.to_csv(hq_Afr_NOD_ra_hr_nod , index=False, header=True, sep='\\t')\n",
    "\n",
    "####\n",
    "\n",
    "##### rab\n",
    "\n",
    "# save datasets for training\n",
    "\n",
    "\n",
    "training_w_NOD_rab_nod  =    DIR /'Dataset'/'Preprocessed'/'W_NOD_rab.csv'\n",
    "\n",
    "hq_Afr_NOD_rab_lr_nod  =   DIR /'Dataset'/'Preprocessed'/'Afr_NOD_rab_lr.csv'\n",
    "\n",
    "hq_Afr_NOD_rab_hr_nod  =   DIR /'Dataset'/'Preprocessed'/'Afr_NOD_rab_hr.csv'\n",
    "\n",
    "\n",
    "hq_W_cp_rab_nod.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hq_W_cp_rab_nod.reset_index( inplace=True,drop=True)\n",
    "hq_W_cp_rab_nod.to_csv(training_w_NOD_rab_nod , index=False, header=True, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "hq_Afr_cp_rab_lr_nod.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hq_Afr_cp_rab_lr_nod.reset_index( inplace=True,drop=True)\n",
    "hq_Afr_cp_rab_lr_nod.to_csv(hq_Afr_NOD_rab_lr_nod , index=False, header=True , sep='\\t')\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "hq_Afr_cp_rab_hr_nod.sort_values(by=['lon', 'lat'], ascending=True, inplace=True)\n",
    "hq_Afr_cp_rab_hr_nod.reset_index( inplace=True,drop=True)\n",
    "hq_Afr_cp_rab_hr_nod.to_csv(hq_Afr_NOD_rab_hr_nod , index=False, header=True , sep='\\t')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bf66c",
   "metadata": {},
   "source": [
    "Biggest insight here is that all variables have a different scale, there are many outliers for most \n",
    "of them and no one seem really close to normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616fb37b",
   "metadata": {},
   "source": [
    "# 5-Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf1fbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    " def IForest(df):\n",
    "\n",
    "        idx = copy.deepcopy(features_ghf)\n",
    "\n",
    "        X = df[features]\n",
    "        y =  df[target]\n",
    "\n",
    "        X_if = df[idx]\n",
    "\n",
    "\n",
    "        detector = IsolationForest(random_state=random_state)\n",
    "\n",
    "        numeric_transformer = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "\n",
    "        categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "           (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "                (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        # Append classifier to preprocessing pipeline.\n",
    "        # Now we have a full prediction pipeline.\n",
    "        steps=[(\"preprocessor\", preprocessor), (\"detector\", detector)]\n",
    "\n",
    "        IF = Pipeline(steps = steps)\n",
    "        \n",
    "        \n",
    "        ## ranked 1\n",
    "\n",
    "        IF.set_params(**best_params )\n",
    "        IF.fit(X.values)\n",
    "        anomaly_decison_rank_1 =  IF.decision_function(X.values)\n",
    "        anomalies_rank_1  = IF.predict(X.values) \n",
    "        scores_rank_1  = IF.score_samples(X.values)\n",
    "        parameters_rank_1 =IF.get_params()\n",
    "\n",
    "\n",
    "        # manual and automatic\n",
    "        tuned_params = {\n",
    "            'detector__bootstrap': bs_df.loc[0:5, 'param_detector__bootstrap'].mode()[0],\n",
    "            'detector__contamination': bs_df.loc[0:5, 'param_detector__contamination'].min(),\n",
    "            'detector__max_features': bs_df.loc[0:5, 'param_detector__max_features'].min(),\n",
    "            'detector__max_samples': bs_df.loc[0:5, 'param_detector__max_samples'].min(),\n",
    "            'detector__n_estimators': bs_df.loc[0:5, 'param_detector__n_estimators'].min()\n",
    "        }\n",
    "        IF.set_params(**tuned_params)\n",
    "        IF.fit(X.values)\n",
    "        anomaly_decison =  IF.decision_function(X.values)\n",
    "        anomalies = IF.predict(X.values) \n",
    "        scores = IF.score_samples(X.values)\n",
    "        \n",
    "        #print(IF.get_params())\n",
    "\n",
    "        #mask = anomalies == -1\n",
    "        mask = anomalies_rank_1 == -1\n",
    "        \n",
    "        #X_train_iforest, y_train_iforest = X.values[~mask, :], y.values[~mask]\n",
    "        #X.values_if.columns = \n",
    "        #print(anomalies.shape)\n",
    "\n",
    "\n",
    "\n",
    "        X_if['scores'] = scores\n",
    "        X_if['anomaly'] = anomalies\n",
    "\n",
    "        #q1 = X_if['scores'].quantile(0.25)\n",
    "        #q3 = X_if['scores'].quantile(0.75)\n",
    "        #iqr = q3 - q1\n",
    "       # lower_bound=(q1 - k * iqr)\n",
    "       # upper_bound=(q3 + k * iqr)\n",
    "        \n",
    "        print('for optimization')\n",
    "        #print(\"Lower bound:{} \\nUpper bound:{}\".format(lower_bound,upper_bound))\n",
    "\n",
    "        #anomolus_dp = X.values[np.where(anomalies == -1, True, False)]\n",
    "        anomolus_dp = X.values[np.where(anomalies_rank_1 == -1, True, False)]\n",
    "        print(\"\\nThe data has {} anomolus points and the data points are the following for  : \".format(len(anomolus_dp)))\n",
    "        print(\"Percentage of anomalies in data: {:.2f}\".format((len(anomolus_dp)/len(X.values))*100))\n",
    "        #print(anomolus_dp)\n",
    "\n",
    "      \n",
    "       #####\n",
    "        \n",
    "        \n",
    "        hf_tmp = X_if[X_if['anomaly'] ==1][idx]\n",
    "        corr_tuned = hf_tmp.corr().abs()\n",
    "        corr_tuned.sort_values(by = target, ascending=False, inplace=True)\n",
    "\n",
    "        corr_tuned.drop(target, inplace=True)\n",
    "\n",
    "        corr_tuned.index.to_list().append(target)\n",
    "\n",
    "        columns_titles = corr_tuned.index.to_list()\n",
    "        columns_titles.append(target)\n",
    "\n",
    "        corr_tuned=corr_tuned.reindex(columns=columns_titles)\n",
    "\n",
    "        obs_tmp = obs\n",
    "        y_axis_labels = [obs_tmp.loc[feature, 'LABELS'] for feature in corr_tuned.index]\n",
    "\n",
    "        x_axis_labels = copy.deepcopy(y_axis_labels)\n",
    "\n",
    "        x_axis_labels.append('GHF')\n",
    "        # Increase the size of the heatmap.\n",
    "        fig = plt.figure(num=None, figsize=(30, 10), facecolor='w', edgecolor='k')\n",
    "       \n",
    "        heatmap = sns.heatmap(corr_tuned, vmin=0, vmax=1, annot=True,\n",
    "                              xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "        # Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
    "        heatmap.set_title('Pearson Correlation Heatmap After Removal of Anomalous Values', \n",
    "                          fontdict={'fontsize':30}, pad=12);\n",
    "        heatmap.tick_params(labelsize=30)\n",
    "\n",
    "        fig.savefig(DIR/'Fig'/f\"fig_s4_OD_{region}_{rating}{resolution}_{tuned_params['contamination']}.pdf\", bbox_inches='tight', dpi=300 , pad_inches=0.1)        \n",
    "        fig.savefig(DIR/'Fig'/f\"fig_s4_OD_{region}_{rating}{resolution}_{tuned_params['contamination']}.jpg\", bbox_inches='tight', dpi=300 , pad_inches=0.1)\n",
    "         ######\n",
    "            \n",
    "        #plt.close()\n",
    "\n",
    "        hf_tmp = X_if[idx]\n",
    "        corr = hf_tmp.corr().abs()\n",
    "        corr.sort_values(by = target, ascending=False, inplace=True)\n",
    "\n",
    "        corr.drop(target, inplace=True)\n",
    "\n",
    "        corr.index.to_list().append(target)\n",
    "\n",
    "        columns_titles = corr.index.to_list()\n",
    "        columns_titles.append(target)\n",
    "\n",
    "        corr=corr.reindex(columns=columns_titles)\n",
    "\n",
    "\n",
    "        y_axis_labels = [obs_tmp.loc[feature, 'LABELS'] for feature in corr.index]\n",
    "\n",
    "        x_axis_labels = copy.deepcopy(y_axis_labels)\n",
    "\n",
    "        x_axis_labels.append('GHF')\n",
    "        # Increase the size of the heatmap.\n",
    "        fig = plt.figure(num=None, figsize=(30, 10), facecolor='w', edgecolor='k')\n",
    "\n",
    "        \n",
    "\n",
    "        heatmap = sns.heatmap(corr, vmin=0, vmax=1, annot=True,\n",
    "                              xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "        # Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
    "        heatmap.set_title('Pearson Correlation Heatmap Before Removal of Anomalous Values', \n",
    "                          fontdict={'fontsize':30}, pad=12);\n",
    "        heatmap.tick_params(labelsize=30)\n",
    "        \n",
    "        fig.savefig(DIR/'Fig'/f\"fig_s3_NOD_{region}_{rating}{resolution}.pdf\", bbox_inches='tight', dpi=300 , pad_inches=0.1)        \n",
    "        fig.savefig(DIR/'Fig'/f\"fig_s3_NOD_{region}_{rating}{resolution}.jpg\", bbox_inches='tight', dpi=300 , pad_inches=0.1)\n",
    "     \n",
    "        #plt.close()\n",
    "        ######\n",
    "        \n",
    "        hf_tmp = X_if[anomalies_rank_1 ==1][idx]\n",
    "        corr_rank_1 = hf_tmp.corr().abs()\n",
    "        corr_rank_1.sort_values(by = target, ascending=False, inplace=True)\n",
    "\n",
    "        corr_rank_1.drop(target, inplace=True)\n",
    "\n",
    "        corr_rank_1.index.to_list().append(target)\n",
    "\n",
    "        columns_titles = corr_rank_1.index.to_list()\n",
    "        columns_titles.append(target)\n",
    "\n",
    "        corr_rank_1=corr_rank_1.reindex(columns=columns_titles)\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        print('Without outlier detection')\n",
    "        print(f'r2 before : {round(corr[target].sum(),3)}, mean : {round(corr[target].sum()/len(X.columns),3)}\\n')\n",
    "        #print(f' < 0.1 : {len(corr[corr[target] < 0.15])}')\n",
    "        #print(f' < 0.5 : {len(corr[corr[target] > 0.5])}')\n",
    "        print('#'*20)\n",
    "        print('Ranked 1 best paramter and score')\n",
    "        print(f'r2 : {round(corr_rank_1[target].sum(),3)}, mean : {round(corr_rank_1[target].sum()/len(X.columns),3)}')\n",
    "        print(f'score :  {round(np.mean(scores_rank_1),5)*-1}')\n",
    "        print(f'parameter : {parameters_rank_1}\\n')\n",
    "        print('#'*20)\n",
    "        print('Automatic tuned paramter and score')\n",
    "        print(f'r2 : {round(corr_tuned[target].sum(),3)}, mean : {round(corr_tuned[target].sum()/len(X.columns),3)}')\n",
    "        print(f'score :  {round(np.mean(scores),5)*-1}')\n",
    "        #print(f' < 0.1 : {len(corr_tuned[corr_tuned[target] < 0.15])}')\n",
    "        #print(f' < 0.5 : {len(corr_tuned[corr_tuned[target] > 0.5])}')\n",
    "        print(f'parameter : {tuned_params}\\n')\n",
    "\n",
    "        # saving best parameters\n",
    "        bs_if_hyp =  DIR/'Hyperparameters'/f'IF_{region}_{file_label}{resolution}.csv'\n",
    "\n",
    "\n",
    "        bs_if_hyp_df = pd.DataFrame.from_dict([tuned_params])\n",
    "\n",
    "\n",
    "        bs_if_hyp_df.to_csv(bs_if_hyp , index=False, sep='\\t')\n",
    "        \n",
    "        print(f'final df %: {round(len(df[~mask])/len(df),3)*100}')\n",
    "        \n",
    "        #saving reduced dataframe\n",
    "        hf_tmp_csv =  df[~mask].sort_values(by=['lon', 'lat'], ascending=True).reset_index().rename(columns={'index':'index_OD'})\n",
    "        hf_tmp_csv.to_csv(file_out ,index=False, header=True, sep='\\t')\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f44d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rating = 'rab'\n",
    "outlier = 'OD'\n",
    "\n",
    "\n",
    "n_iter = 100\n",
    "kfold = 10\n",
    "\n",
    "\n",
    "def scorer_f(estimator, X):   #your own scorer\n",
    "      return np.mean(estimator.score_samples(X)) *-1\n",
    "\n",
    "\n",
    "#scoring = make_scorer(nrmse , greater_is_better=False )\n",
    "#scoring = make_scorer(r2_normalized, greater_is_better=True)\n",
    "#scoring = make_scorer(scorer_f, greater_is_better=True)\n",
    "scoring = make_scorer(mean_squared_error,squared=False, greater_is_better=False)\n",
    "\n",
    "file_label = f'{outlier}_{rating}'\n",
    "\n",
    "\n",
    "\n",
    "training_w_od  =   DIR /'Dataset'/'Preprocessed'/f'W_{file_label}.csv'\n",
    "training_afr_lr_od = DIR /'Dataset'/'Preprocessed'/f'Afr_{file_label}_lr.csv'\n",
    "training_afr_hr_od = DIR /'Dataset'/'Preprocessed'/f'Afr_{file_label}_hr.csv'\n",
    "\n",
    "\n",
    "\n",
    "# Initiate parameter grid\n",
    "\n",
    "# param distribution\n",
    "n_estimators      = np.arange(100, 800,50)\n",
    "max_features = np.arange(1,17,1) \n",
    "bootstrap    = [True, False] # deafult is false for extra trees True for randomforest\n",
    "contamination = np.arange(0.0001, 0.1, 0.0001)\n",
    "max_samples =  np.arange(100, 4000, 5)\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "hyperparameters = {}\n",
    "\n",
    "\n",
    "# Update dict with Extra Trees\n",
    "search_space =  { \n",
    "                'detector__n_estimators' : n_estimators,  # # inc up to a point then overfit train reduce test\n",
    "                'detector__bootstrap'    : bootstrap,\n",
    "                'detector__contamination': contamination, # inc up to a point then overfit train reduce test\n",
    "                'detector__max_features' : max_features,  # # inc up to a point then overfit train reduce test\n",
    "                'detector__max_samples'  : max_samples, \n",
    "}\n",
    "\n",
    "\n",
    "hqs = [hq_W_cp_rab_nod, hq_Afr_cp_rab_lr_nod, hq_Afr_cp_rab_hr_nod,  ]\n",
    "regions = ['W', 'Afr', 'Afr']\n",
    "resolutions = ['', '_lr', '_hr']\n",
    "file_outs = [training_w_od , training_afr_lr_od, training_afr_hr_od, ]\n",
    "\n",
    "\n",
    "'''hqs = [  hq_Afr_cp_rab_hr_nod,  ]\n",
    "regions = [ 'Afr']\n",
    "resolutions = [  '_hr']\n",
    "file_outs = [ training_afr_hr_od, ]\n",
    "\n",
    "'''\n",
    "\n",
    "for hq, region, file_out, resolution  in tqdm_notebook(\n",
    "    zip(hqs, regions, file_outs, resolutions), \n",
    "                                          total=3, desc = f'Processing: '):\n",
    "\n",
    "    sleep(0.01)\n",
    "    \n",
    "    X_train = hq[features]\n",
    "    y_train = hq[target]\n",
    "\n",
    "    # Initialize dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    \n",
    "\n",
    "    cv = KFold(n_splits=kfold, random_state=random_state,  shuffle=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Tune and evaluate detectors\n",
    "    detector_label = 'IF'\n",
    "\n",
    "    # Print message to user\n",
    "    print('#'*60)\n",
    "    print(f\"Now tuning {detector_label} for {region}.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Scale features via Z-score normalization\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler= RobustScaler()\n",
    "\n",
    "    detector = IsolationForest(random_state=random_state)\n",
    "    \n",
    "\n",
    "    numeric_transformer = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "\n",
    "    categorical_transformer = OrdinalEncoder(handle_unknown='use_encoded_value' , unknown_value =-1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "       (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Append classifier to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"detector\", detector)]\n",
    "\n",
    "    pipeline_bscv = Pipeline(steps = steps)\n",
    "    \n",
    "\n",
    "\n",
    "    # Initialize BaysSearch object                  \n",
    "    bscv = BayesSearchCV(\n",
    "        pipeline_bscv,\n",
    "        # (parameter space, # of evaluations)f\n",
    "        search_space,\n",
    "        n_iter = n_iter, \n",
    "        cv = cv, \n",
    "        #verbose = 1, \n",
    "        n_jobs= -1, \n",
    "        scoring= scorer_f ,\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Fit gscv\n",
    "    bscv.fit(X_train)  \n",
    "\n",
    "    # Get best parameters and score\n",
    "    best_params = bscv.best_params_\n",
    "    best_score = bscv.best_score_\n",
    "\n",
    "\n",
    "    print(f'Optimization has terminated for {region}.')\n",
    "   \n",
    "\n",
    "    #save results\n",
    "    bs_if_hyp  =  DIR/'Hyperparameters'/f'IF_hyper_{region}_{file_label}{resolution}.csv'\n",
    "\n",
    "    dropped_columns = ['mean_fit_time', 'std_fit_time',  'params', 'mean_score_time', 'std_score_time']\n",
    "\n",
    "    # all hyp results\n",
    "    # gives traing and validation results\n",
    "    bs_df = pd.DataFrame(bscv.cv_results_)\n",
    "\n",
    "    bs_df = bs_df.sort_values(f'rank_test_score', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    bs_df['mean_test_score'] = bs_df['mean_test_score'] \n",
    "    bs_df['mean_train_score'] = bs_df['mean_train_score'] \n",
    "\n",
    "    split_test = [f'split{x}_test_score' for x in range(kfold)]\n",
    "    split_train = [f'split{x}_train_score' for x in range(kfold)]\n",
    "\n",
    "    dropped_columns.extend(split_test)\n",
    "    dropped_columns.extend(split_train)\n",
    "\n",
    "    bs_df = bs_df.drop(dropped_columns, axis=1)\n",
    "\n",
    "\n",
    "    bs_df.to_csv(bs_if_hyp , index=False, sep='\\t')\n",
    "\n",
    " \n",
    "    IForest(hq)\n",
    "\n",
    "    ## save files \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4285a",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b7e43",
   "metadata": {},
   "source": [
    "rating good clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58797f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4,figsize=(30,30),sharey=True, gridspec_kw = {'wspace':0.03},)\n",
    "\n",
    "axs = ax.flatten()\n",
    "\n",
    "sns.set(style=\"white\",  font_scale = 3)\n",
    "\n",
    "line_kws={'lw': 4, 'color': 'orange'}\n",
    "scatter_kws= {'s':100, 'color':'grey', 'edgecolor':'w'}\n",
    "\n",
    "for i, feature in tqdm_notebook(\n",
    "    enumerate(obs.index.to_list()), total=16, desc = 'Feature: '):\n",
    "\n",
    "    unit_i = obs['UNITS'][i]\n",
    "    label_i = obs['LABELS'][i]\n",
    "    range_i = obs['V_RANGE'][i]\n",
    "    \n",
    "    train_od_df = pd.read_csv(training_afr_hr_od, sep='\\t')\n",
    "    removed_indexes = list(set(hq_Afr_cp_rab_hr_nod.index.tolist()) -  set(train_od_df['index_OD'].tolist()))\n",
    "\n",
    "    # An lmplot\n",
    "    if feature in ['GLIM','REG']:\n",
    "\n",
    "        g0 = sns.boxplot(x=hq_Afr_cp_rab_hr_nod[feature], y=hq_Afr_cp_rab_hr_nod[target], \n",
    "                         ax=axs[i], palette='bright')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        g0.set_xlabel(unit_i)\n",
    "        axs[i].locator_params(axis='x', nbins=len(hq_Afr_cp_rab_hr_nod[feature].unique()))  # set divisor \n",
    "    \n",
    "    elif feature =='VOLC_DIST_W':\n",
    "\n",
    "        \n",
    "        g0 = sns.regplot(x=hq_Afr_cp_rab_hr_nod[feature]*100, \n",
    "                         y=hq_Afr_cp_rab_hr_nod[target], ax=axs[i],\n",
    "                         ci=0,\n",
    "                         line_kws=line_kws,                         \n",
    "                         scatter_kws=scatter_kws ,scatter=True)\n",
    "\n",
    "\n",
    "        g0.set_xlabel(unit_i)\n",
    "        if label_i == 'DEM' or label_i =='RHO_L':\n",
    "            axs[i].set_xlim([range_i[0], range_i[1]])\n",
    "         \n",
    "        axs[i].locator_params(axis='x', nbins=8)  # set divisor \n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        g0 = sns.regplot(x=hq_Afr_cp_rab_hr_nod[feature], \n",
    "                         y=hq_Afr_cp_rab_hr_nod[target], ax=axs[i],\n",
    "                         ci=0,\n",
    "                         line_kws=line_kws,                         \n",
    "                         scatter_kws=scatter_kws ,scatter=True)\n",
    "\n",
    "\n",
    "        g0.set_xlabel(unit_i)\n",
    "        if label_i == 'DEM' or label_i =='RHO_L':\n",
    "            axs[i].set_xlim([range_i[0], range_i[1]])\n",
    "         \n",
    "        axs[i].locator_params(axis='x', nbins=4)  # set divisor \n",
    "\n",
    "    if (i%4) ==0:\n",
    "        g0.set_ylabel('GHF [mW/m2]')\n",
    "        g0.xaxis.labelpad = 0\n",
    "    else:\n",
    "        g0.set_ylabel('')\n",
    "        g0.get_yaxis().set_visible(False)\n",
    "        g0.xaxis.labelpad = 0\n",
    "\n",
    "        \n",
    "    if i%2 != 0:\n",
    "        g0.xaxis.labelpad = 0\n",
    "        g0.tick_params(axis=\"x\",pad=12)\n",
    "    else:\n",
    "        g0.xaxis.labelpad = 0\n",
    "        g0.tick_params(axis=\"x\",pad=0)\n",
    "        \n",
    "    \n",
    "    g0.set_title('(%s) %s'%(sub_figs[i],label_i), loc ='left', pad=30,  y=1)\n",
    "    #axs[i].spines['right'].set_visible(False)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(DIR/'Fig'/\"fig_2_before.pdf\", bbox_inches='tight', dpi=300 , pad_inches=0.1)\n",
    "fig.savefig(DIR/'Fig'/\"fig_2_before.jpg\", bbox_inches='tight', dpi=300 , pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18507a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4,figsize=(30,30),sharey=True, gridspec_kw = {'wspace':0.03},)\n",
    "\n",
    "axs = ax.flatten()\n",
    "\n",
    "sns.set(style=\"white\",  font_scale = 3)\n",
    "\n",
    "line_kws    = {'lw': 4, 'color':'orange'}\n",
    "\n",
    "kwargs_r    = { 's':100, 'color':'r'}\n",
    "\n",
    "kwargs_g    =  {'s':100, 'color':'grey'}\n",
    "\n",
    "\n",
    "for i, feature in tqdm_notebook(\n",
    "    enumerate(obs.index.to_list()), total=16, desc = 'Feature: '):\n",
    "\n",
    "    unit_i = obs['UNITS'][i]\n",
    "    label_i = obs['LABELS'][i]\n",
    "    range_i = obs['V_RANGE'][i]\n",
    "    \n",
    "    train_od = pd.read_csv(training_afr_hr_od, sep='\\t')\n",
    "    removed_indexes = list(set(hq_Afr_cp_rab_hr_nod.index.tolist()) -  set(train_od['index_OD'].tolist()))\n",
    "\n",
    "    # An lmplot\n",
    "    if feature in ['GLIM','REG']:\n",
    "\n",
    "        g0 = sns.boxplot(x=hq_Afr_cp_rab_hr_nod[feature][train_od['index_OD'].tolist()],\n",
    "                         y=hq_Afr_cp_rab_hr_nod[target][train_od['index_OD'].tolist()], \n",
    "                         ax=axs[i], palette='bright')\n",
    "        \n",
    "\n",
    "        \n",
    "        sns.scatterplot(hq_Afr_cp_rab_hr_nod[feature][removed_indexes].cat.codes , \n",
    "                        hq_Afr_cp_rab_hr_nod[target][removed_indexes], \n",
    "                        ax= axs[i], **kwargs_r)\n",
    "\n",
    "\n",
    "\n",
    "        g0.set_xlabel(unit_i)\n",
    "        axs[i].locator_params(axis='x', nbins=len(hq_Afr_cp_rab_hr_nod[feature].unique()))  # set divisor \n",
    "\n",
    "    elif feature =='VOLC_DIST_W':\n",
    "        #g0 = sns.scatterplot(x=hq_tmp[feature], y=hq_tmp[target],  ax=axs[i], size=1)\n",
    "        #g0 = sns.regplot(x=hq_Afr_cp_rab_hr_nod[feature], y=hq_Afr_cp_rab_hr_nod[target], ax=axs[i], \n",
    "         #                line_kws={'lw': 4, 'color': 'orange'},\n",
    "         #                scatter_kws= {'s':60, 'color':'grey'} ,scatter=True)\n",
    "        \n",
    "\n",
    "        g0 = sns.regplot(x=hq_Afr_cp_rab_hr_nod[feature] [train_od['index_OD'].tolist()]*100, \n",
    "                         y=hq_Afr_cp_rab_hr_nod[target][train_od['index_OD'].tolist()], ax=axs[i], \n",
    "                         ci=0,\n",
    "                         line_kws=line_kws, scatter=False)\n",
    "        sns.scatterplot(hq_Afr_cp_rab_hr_nod[feature]*100, \n",
    "                        hq_Afr_cp_rab_hr_nod[target], ax= axs[i],\n",
    "                        **kwargs_g)\n",
    "        \n",
    "        sns.scatterplot(hq_Afr_cp_rab_hr_nod[feature][removed_indexes]*100, \n",
    "                        hq_Afr_cp_rab_hr_nod[target][removed_indexes], ax= axs[i],\n",
    "                        **kwargs_r)\n",
    "\n",
    "\n",
    "        g0.set_xlabel(unit_i)\n",
    "        if label_i == 'DEM' or label_i =='RHO_L':\n",
    "            axs[i].set_xlim([range_i[0], range_i[1]])\n",
    "         \n",
    "        axs[i].locator_params(axis='x', nbins=8)  # set divisor\n",
    "        \n",
    "    else:\n",
    "        #g0 = sns.scatterplot(x=hq_tmp[feature], y=hq_tmp[target],  ax=axs[i], size=1)\n",
    "        #g0 = sns.regplot(x=hq_Afr_cp_rab_hr_nod[feature], y=hq_Afr_cp_rab_hr_nod[target], ax=axs[i], \n",
    "         #                line_kws={'lw': 4, 'color': 'orange'},\n",
    "         #                scatter_kws= {'s':60, 'color':'grey'} ,scatter=True)\n",
    "        \n",
    "\n",
    "        g0 = sns.regplot(x=hq_Afr_cp_rab_hr_nod[feature] [train_od['index_OD'].tolist()], \n",
    "                         y=hq_Afr_cp_rab_hr_nod[target][train_od['index_OD'].tolist()], ax=axs[i], \n",
    "                         ci=0,\n",
    "                         line_kws=line_kws, scatter=False)\n",
    "        \n",
    "        sns.scatterplot(hq_Afr_cp_rab_hr_nod[feature], \n",
    "                        hq_Afr_cp_rab_hr_nod[target], ax= axs[i],\n",
    "                        **kwargs_g)\n",
    "        \n",
    "        sns.scatterplot(hq_Afr_cp_rab_hr_nod[feature][removed_indexes], \n",
    "                        hq_Afr_cp_rab_hr_nod[target][removed_indexes], ax= axs[i],\n",
    "                        **kwargs_r)\n",
    "\n",
    "\n",
    "        g0.set_xlabel(unit_i)\n",
    "        if label_i == 'DEM' or label_i =='RHO_L':\n",
    "            axs[i].set_xlim([range_i[0], range_i[1]])\n",
    "         \n",
    "        axs[i].locator_params(axis='x', nbins=4)  # set divisor \n",
    "\n",
    "    if (i%4) ==0:\n",
    "        g0.set_ylabel('GHF [mW/m2]')\n",
    "        g0.xaxis.labelpad = 0\n",
    "    else:\n",
    "        g0.set_ylabel('')\n",
    "        g0.get_yaxis().set_visible(False)\n",
    "        g0.xaxis.labelpad = 0\n",
    "\n",
    "        \n",
    "    if i%2 != 0:\n",
    "        g0.xaxis.labelpad = 0\n",
    "        g0.tick_params(axis=\"x\",pad=12)\n",
    "    else:\n",
    "        g0.xaxis.labelpad = 0\n",
    "        g0.tick_params(axis=\"x\",pad=0)\n",
    "        \n",
    "    \n",
    "    g0.set_title('(%s) %s'%(sub_figs[i],label_i), loc ='left', pad=30,  y=1)\n",
    "    #axs[i].spines['right'].set_visible(False)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(DIR/'Fig'/\"fig_2_after.pdf\", bbox_inches='tight', dpi=300 , pad_inches=0.1)\n",
    "fig.savefig(DIR/'Fig'/\"fig_2_after.jpg\", bbox_inches='tight', dpi=300 , pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9387d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
